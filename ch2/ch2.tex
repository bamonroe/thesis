\documentclass[../main.tex]{subfiles}

\begin{document}
\doublespacing
\setcounter{chapter}{1}
\singlespacing
\chapter{The Normative Promise of Stochastic Models}
\doublespacing

\lltoc

The recent interest in the applicability of stochastic choice models for explaining choice behavior that seemingly violates Expected Utility Theory (EUT) has led to a variety of calls for, and attempts to seek out, a \enquote{true} stochastic model or combination of  models which best describe observed choice behavior in experiments.
The first call for such a research effort was apparently \textcite{Edwards1954}, who accused economics of having become \enquote{exceedingly elaborate, mathematical, and voluminous.}
\textcite[380]{Edwards1954} criticized economists as \enquote{mak\textins*{ing} assumptions, and from these assumptions \textelp{} deduce\textins{ing} theorems which presumably can be tested, though it seems unlikely that the testing will ever occur.}
\textcite{Edwards1954} proposed that individuals may make choices stochastically as opposed to deterministically, which married some of the extent findings of psychology at the time with economics to help explain the data.

A more recent call to study stochastic models was issued by \textcite[1321]{Hey1994} after they conducted rigorous subject-by-subject tests of some of the popular proposed alternatives to EUT, alongside EUT.
They conclude that \enquote{possibly the overriding feature of our analysis is the importance of error \textelp{} Perhaps we should now spend some time on thinking about the noise, rather than about even more alternatives to EU?}
A wealth of stochastic models has resulted from economists and psychologists taking up the project proposed by \textcite{Hey1994}.
This chapter describes the results of this effort and introduces discussion of the normative promise of some of these models.

\singlespacing
\section{The Specification of Stochastic Models}
\doublespacing

I begin by distinguishing three classes of stochastic models.
\textcite[1301]{Hey1994} proposed a model which incorporates a random error term into the evaluation of lotteries by subjects.
The roots of this type of model date back to \textcite{Fechner1966} and \textcite{Luce1959}, which has subsequently been called a \enquote{Strong Utility} (SU) or \enquote{Fechnerian} model.
There are, however, a large number of models that have been derived from the SU model, and so I will refer generally to this class of models as \enquote{Random Error} (RE) models.
\textcite{Harless1994} had undertaken their own analysis of various alternatives to EUT and suggested a stochastic model which allows for subjects to potentially disregard their underlying preferences and choose between the available options with equal probability.
The models in this class are called the \enquote{Constant Error} or \enquote{Tremble} (TR) models.
\textcite{Loomes1995} reconsidered and generalized a model initially proposed by \textcite{Becker1963} called the \enquote{Random Preference} (RP) model which, in its most popular form, allows for subjects to have some distribution of preference relations from which they randomly choose every time they evaluate a choice situation.
Generally, any model that calls for an agent to have a distribution of preference relations belongs to the RP class.\footnote{
	There are other members of the RP model class that are less commonly utilized, one of which will be discussed later.
	For now, when referring to the RP model, I refer to the formulation specified by \textcite{Loomes1995}, where one preference relation is drawn per decision situation.}

A less popular class of stochastic choice models proposed by \textcite{Machina1985} and \textcite{Chew1991}, suggests that subjects have deterministic preferences for \enquote{stochastic options} and thus deliberately engage in adding randomness to their choices.
That is, subjects have convex indifference curves in the Marshack-Machina Triangle space,\footnote{
	The Marshack-Machina Triangle was developed by \textcite{Machina1987} as a way to represent the relation of lotteries with up to 3 outcomes and preferences for those lotteries.
	Each vertex of the triangle represents an outcome, and any point in the triangle represents a lottery.
	Any point on a vertex of the triangle represents a lottery with a 100\% composition of one outcome.
	Any interior point represents a lottery composed of a mixture of outcomes, with the relative proportion of any outcome in the lottery defined by its geometric distance from its corresponding vertex.
	If the independence axiom holds, a straight line between any two points in the triangle space indicates all the lotteries amongst which the agent is indifferent.
	Parallel lines thus indicate either an increase or decrease in preference.
	A strictly convex curve connecting 2 lottery points represents a violation of the independence axiom.} and therefore a probabilistic (linear) mixture of two lotteries lies on a higher indifference curve than any two lines which lie on the same curve.

\textcite{Hey1995} tested the \enquote{stochastic options} theory of \textcite{Machina1985} using the \enquote{Quadratic Mixture} model of \textcite{Chew1991} and find strong evidence against it.
The model itself has some restrictive aspects for estimation: \enquote{First, the likelihood function, although continuous everywhere, is not smoothly so; there are kinks in the function with resulting discontinuities in the derivatives.
Second, for certain parameter sets, certain observations are \textit{impossible}} [emphasis in the original] \parencite*[164]{Hey1995}.
Out of a sample of 45 subjects, \textcite{Hey1995} find only 4 subjects to whom they can fit the model at all, and of these 4, for only 2 does the \enquote{Quadratic Mixture} model fit better than a RE type model, and of these 2, for only 1 subject are the estimated coefficients plausible \parencite*[167]{Hey1995}.
It appears that continued investigation of this class of models has essentially ceased since these and additional results by \textcite{Camerer1994}.
Keeping with this pattern, when referring to stochastic models for the remainder of this text I do not include this class of models in our definition.

These general classes of stochastic choice constitute the bulk of the research on stochastic models, with the RE models possibly being the most widely employed models when estimating utility parameters from choice data.
I will begin by defining some notation to characterize how these models operate.

For each option $a$ in a set of alternatives $t$, stochastic models generate a probability that an agent will select that option from the set.
These probabilities are referred to as \enquote{choice probabilities} and are generally related to an underlying determinisitic relation of preference.
First, I define the Rank Dependent Utility (RDU) structure as formulated by \textcite{Quiggin1982}, which nests EUT as a special case, as the deterministic structure of preference.
Second I define the manner in which RP, TR, and RE  models relate this deterministic structure to the stochastic specification of probabilities of choice.

RDU is characterized by the following function:
\begin{equation}
	\label{eq2:RDU}
	RDU = \sum_{c=1}^{C} \left[ w_c(p) \times u(x_c) \right]
\end{equation}
\noindent where $c$ indexes the outcomes, $x_c$, from $\{1,\ldots,C\}$ with $c=1$ being the smallest outcome in the lottery and $c=I$ being the largest outcome in the lottery, $u(\cdot)$ returns the utility of its argument, $w_c(\cdot)$ returns the decision weight applied to outcome $c$ given the distribution of probabilities ranked by outcome, $p$.

The utility function $u(\cdot)$ can take many functional forms due to it being unique up to an affine transformation, and can be normalized in various was, as illustrated by \textcite{Hey1994}.
It will sometimes be useful to use a specific functional form to make certain concepts clearer, and in such cases the constant relative risk aversion (CRRA) function will be employed:
\begin{equation}
	\label{eq2:CRRA}
	u(x) = \frac{x^{(1-r)}}{(1-r)}
\end{equation}
\noindent where $r$ is the coefficient of relative risk aversion \parencite{Pratt1964}.
Other popular functions such as the constant absolute risk aversion (CARA) function, or the Expo-Power function due to \textcite{Saha1993} can alternatively be employed without loss of generality.

The decision weight function, $w_c(\cdot)$, takes the form:
\begin{equation}
	\label{eq2:dweight}
	w_c(p) =
	\begin{cases}
		\omega\left(\displaystyle\sum_{a=c}^C p_a\right) - \omega\left(\displaystyle\sum_{k=c+1}^C p_b\right) & \text{for } c<C \\
		\omega(p_c) & \text{for } c = C
	\end{cases}
\end{equation}
\noindent where the probability weighting function, $\omega(\cdot)$, can take a variety of parametric or non-parametric forms.
Many functions have been proposed for $\omega(\cdot)$.
One is the \enquote{Inverse-S} shaped function popularized by \textcite{Tversky1992}:
\begin{equation}
	\label{eq2:pw:kahneman}
	\omega(p_c) = \frac{p_c^\gamma}{\biggl(\sum\limits_{k=1}^{C} p_b^\gamma\biggr)^{ \frac{1}{\gamma} } }
\end{equation}
\noindent Another is the power function used by \textcite{Quiggin1982}:
\begin{equation}
	\label{eq2:pw:quiggin}
	\omega(p_c)=p_c^\gamma
\end{equation}
\noindent The flexible function proposed by \textcite{Prelec1998} is also popular:
\begin{equation}
	\label{eq2:pw:prelec}
	\omega(p_c)=\exp(-\beta(-\ln(p_c))^\alpha)
\end{equation}
\noindent where $\alpha > 0$ and $\beta > 0$.
In all cases there exist values for the shaping parameters which allow $w_c(p) = p_c$, the special case of EUT.
When both a decision weight function applied to probabilities and a utility function applied to outcomes are defined, we have what is called a utility structure.

To make a general point about notation, consider lottery $a$ with $C_a = 2$ possible outcomes, and lottery $b$ with $C_b = 3$ possible outcomes.
Suppose that $x_c^a \neq x_c^b \; \forall i$.
A lottery $a^*$ can be constructed with $C_{a^*}= C_a + C_b = 5$ such that it is equivalent to lottery $a$ by adding the outcomes in lottery $b$ to lottery $a$ and setting the probabilities associated with each of these added outcomes equal to $0$.
Similarly for lottery $b$.
This equivalence property holds for all EUT and RDU functional forms as zero probability outcomes are given no weight in either structure.
The common set of combined outcomes is what \textcite{Wilcox2008} refers to as the choice scenario's \enquote{context.}
Throughout this chapter, the $a^*$ form of lotteries will be assumed whenever notation concerning the composition of individual lotteries is used.
This allows for identical notation to be utilized when comparing the probabilities of ranked outcomes across lotteries.

We now define a single choice scenario or task, $t$, as a discrete set of $A$ mutually exclusive options from which a subject is asked to select one for payment.
The most common form of such a task is a binary choice problem where subjects are presented with 2 alternatives and asked to select one for payment, $t=\left\{X_1,X_2\right\}$ .
Each element of $t$, $X_a$, is a vector of the option's observable characteristics, such as various outcomes and the associated probability of those outcomes in a lottery.
When presented with such a task, should the subject have a deterministic choice process and preference structure, the subject is assumed to choose option $a$ over the alternative $b$ if and only if the utility of option $a$ was at least as great the utility of option $b$:
\begin{equation}
	\label{eq2:ychoice}
	y_t = j \;\Leftrightarrow\; X_{at} \succcurlyeq X_{bt} \;\Leftrightarrow\; G(\beta_n , X_a) \geq G(\beta_n , X_b)
\end{equation}

\noindent where $G(\cdot)$ is some utility structure such as RDU in equation (\ref{eq2:RDU}), $\beta_n$ is the unobservable vector of parameters of the utility structure for subject $n$, such as probability weights and utilities of outcomes, $X$ is as defined above, and $y_t=a$ is a function that records which option $a$ is selected.

In this deterministic case, the preference relation $\succcurlyeq$ provides all the necessary conditions for the creation of a utility function $G(\cdot)$, meaning it is complete, transitive, and continuous.
Consequently, $\succcurlyeq$ provides a complete ranking of all the available alternatives in task $t$.
It will be convenient to denote this ranking explicitly throughout this chapter by setting the $a$ subscript of option $X_{at}$ equal to its ranking in task $t$:
\begin{equation}
	\label{eq2:outcomerank}
	X_{1t} \succcurlyeq^n X_{2t} \succcurlyeq^n \ldots \succcurlyeq^n X_{at} \succcurlyeq^n \ldots \succcurlyeq^n X_{At}\\
\end{equation}
\noindent This allows us to rank the utility of the $A$ options in task $t$ likewise:
\begin{equation}
	\label{eq2:utilityrank}
	G(\beta_n,X_{1t}) \geq G(\beta_n,X_{2t}) \geq \ldots \geq G(\beta_n,X_{at}) \geq \ldots \geq G(\beta_n,X_{At})
\end{equation}
\noindent With $y_t$ defined and the options in task $t$ ranked, we can also define the set of options in task $t$ not selected by subject $n$ as follows:
\begin{equation}
	\label{eq2:emptyset}
	Z = t \,\backslash\, y = \{z \in t \;|\; z \notin y \}.
\end{equation}
\noindent As in equation (\ref{eq2:utilityrank}), we can set the $a$ subscript of the $A-1$ unchosen options in $Z$ equal to their respective rankings in $Z$:
\begin{equation}
	\label{eq2:Zutilityrank}
	G(\beta_n,X_{1t}^Z) \geq G(\beta_n,X_{2t}^Z) \geq \ldots \geq G(\beta_n,X_{at}^Z) \geq \ldots \geq G(\beta_n,X_{(A-1)t}^Z)
\end{equation}

With these base definitions in place, generalized formulations of the classes of stochastic models can be specified.
As stated above, the RP model characterizes each observed choice made by an agent as conforming to a deterministic preference relation which is drawn at random from a set of such relations whenever the agent is confronted with a choice scenario.
While the set preference relations can have a discrete distribution, the RP model is most commonly discussed in terms of utility functions with the relevant parameter vector, $\beta_n$, being continuously distributed according to some density function, $F_n(\beta | \alpha)$.
In this definition, $\alpha$ is a vector containing the necessary parameters to define the shape of the distribution.
Thus the probability of choice in a RP model is simply the probability that a vector $\beta_n^*$ is drawn from the distribution governed by $F_n(\beta | \alpha)$ that would deterministically satisfy that choice:
\begin{equation}
	\label{eq2:RP0}
	{\Prob}(y_t=a) = {\Prob}(a=1) = {\Prob}\left( \beta_n^* \,|\, G(\beta_n^*,X_{at}) \geq G(\beta_n^*,X_{1t}^Z)\right)
\end{equation}
\noindent If we let $ B_t = \left\{ \beta_n^* \,|\, G(\beta_n^*,X_{at}) \geq G(\beta_n^*,X_{1t}^Z)\right\}$, given the density function $F_n(\beta | \alpha)$:
\begin{equation}
	\label{eq2:RP.f}
	{\Prob}(y_t=a) = \int_{\beta \in B_t} dF_n(\beta|\alpha).
\end{equation}

Note the implication concerning first order stochastic dominance (FOSD)\footnote{
	Lottery $a$ is said to FOSD lottery $b$ iff:
	\begin{align*}
		\forall \, x_c , \; \sum_c^C p_c^a \; \geq \; \sum_c^C p_c^b \quad \textit{and} \quad \exists \, x_c , \; \sum_c^C p_c^a \; > \; \sum_c^C p_c^b
	\end{align*}
	where $c$ ranks the outcomes of lotteries $a$ and $b$ as described in equation (\ref{eq2:RDU}).
	All deterministic theories of utility require the dominant option to be chosen over the dominated option.} with the above specification. Should $X_a$ FOSD $X_b$, there is no monotonic preference relation $\beta^*$ that will allow $G(\beta^*,X_{bt}) \geq G(\beta^*,X_{at})$.
Thus, if we observe $y_t = k$ in such a scenario, the RP model collapses econometrically.

RE models are consistent with a model of the latent choice process that assumes that the utility of each option is evaluated with some error term, which is assumed to be homoscedastic with the SU model and generally assumed to be heteroscedastic with the derivative models of SU, but with a mean of $0$ in either case.
A choice is characterized as incorporating this error.
Assuming a binary choice scenario, the error terms and utility functions must satisfy the following given the choice of option $a$:
\begin{align}
	\label{eq2:RE0}
	\begin{split}
		G(\beta_n,X_{at}) + \epsilon_{at} \;&\geq\; G(\beta_n,X_{bt}) + \epsilon_{bt}\\
		\left[G(\beta_n,X_{at}) + \epsilon_{at}\right] \;&-\; \left[G(\beta_n,X_{bt}) + \epsilon_{bt}\right] \geq 0
	\end{split}
\end{align}
\noindent Setting $\epsilon_{at} - \epsilon_{bt} = \epsilon_t\lambda_n$, where $\lambda_n$ is proportional to the standard deviation of $\epsilon_t$,\footnote{
	It is useful to recognize that what is described as \enquote{noise} in the data is determined by the variance (or standard deviation) of the error term, not its mean.
	If the sign and magnitude of the mean of the error were anything but 0, choices would reveal a biased preference, but if the variance is sufficiently small, the choices are unlikely to reveal apparent deviations from a utility theory.} we can rewrite equation (\ref{eq2:RE0}) as:
\begin{align}
	\label{eq2:RE1}
	\begin{split}
		G(\beta_n,X_{at}) &- G(\beta_n,X_{bt}) + \epsilon_t\lambda_n \geq 0\\
		\epsilon_t \geq \frac{1}{\lambda_n} \left[ G(\beta_n,X_{at}) \right.  &- \left. G(\beta_n,X_{bt}) \right]
	\end{split}
\end{align}
\noindent Thus for RE models, the probability option $a$ is chosen is given by:
\begin{align}
	\label{eq2:RE.2}
	\begin{split}
	{\Prob}(y_t = j) &= {\Prob}\left(  \epsilon_t \geq \frac{1}{\lambda_n} \left[ G(\beta_n,X_{bt}) - G(\beta_n,X_{at}) \right] \right)\\
	&= 1 - F\left( \dfrac{G(\beta_n,X_{bt}) - G(\beta_n,X_{at})}{D(\beta_n,X_t)\lambda_n^* }  \right)
	\end{split}
\end{align}
\noindent where $\lambda^*$ is a precision parameter that remains after $\lambda_n$ is adjusted by $D(\beta_n,X)$ for heteroscedastic models.
As $\lambda^*$ approaches $0$, choice probabilities approach $0$ or $1$, while as $\lambda^*$ approaches $\infty$, choice probabilities approach $0.5$.
The asterisk will be dropped from the remaining formulae to save space.
$F(\cdot)$ is some cumulative distribution function (cdf) such that $F(0) = 0.5$ and $F(x) = 1 - F(-x)$.
Usually $F(\cdot)$ is taken to be either the normal or logistic function, but any distribution function satisfying the previous conditions is acceptable.
When discussing the SU model throughout this chapter, what is referred to is the model specified in equation (\ref{eq2:RE.2}) with $D(\beta_n,X_t) = 1$.
This results in the SU model being said to be homoscedastic.

If utilizing the logistic function, equation (\ref{eq2:RE.2}) resembles a latent index model popularly used in a variety of econometric applications, but with a non-linear latent index.
While equation (\ref{eq2:RE.2}) represents the common 2-option case, using the logistic cdf, the RE model can be rewritten to accommodate $A$ alternatives:
\begin{equation}
	\label{eq2:RE.f}
	{\Prob}(y_t=a) =\dfrac{\exp\!\left( \dfrac{ G(\beta_n,X_{at}) }{ D(\beta_n,X_{t})\lambda_n }  \right)}{ \displaystyle\sum_{c=1}^J \left[ \exp\!\left( \dfrac{ G(\beta_n,X_{ct}) }{ D(\beta_n,X_{t})\lambda_n }  \right)  \right]  }
\end{equation}

With the TR model the \enquote{observed} probability of choice, ${\Prob}(y_n=a)$, needs to be distinguished from the choice probability which would be modeled should the tremble not exist, ${\Prob}_0(y_n=a)$.
The agent is said to \enquote{tremble} with probability $\phi_n$ and select among the available options with equal probability, and with probability $(1-\phi_n)$ select an option according to the underlying process:
\begin{equation}
	\label{eq2:TR.f}
	{\Prob}(y_t=a) = (1-\phi) {\Prob}(y_t=a) + \frac{\phi}{A}.
\end{equation}
When \textcite{Harless1994} proposed the TR model, the underlying choice process was made deterministic, assuming that ${\Prob}_0(y_t=1) = 1$.
\textcite{Loomes2002} however, proposed that ${\Prob}_0(y_t=a)$ is generated from the RP model as specified in equation (\ref{eq2:RP.f}).

\singlespacing
\section{The Empirical Support for Stochastic Models}
\doublespacing

The previous section provided the econometric specification of the classes of stochastic models typically utilized in the literature to estimate parameters from choice data.
The choice of model to utilize however, has not been \textit{ad hoc};
because each stochastic model assigns very specific assumptions to the nature of the \enquote{noise} or randomness in choice data, these assumptions \enquote{amount to identifying restrictions which may affect the relative performance of the theories under scrutiny} \parencite[1091]{Ballinger1997}.
In much the same way as the various alternatives to EUT were proposed and then received rigorous testing, stochastic models have also been rigorously tested on the basis of their identifying restrictions.

\textcite{Ballinger1997} engaged in detailed tests of the SU and TR models, along with various assumptions about the heterogeneity of subjects, and find generally mixed results.
The various models must be combined with somewhat unsavory assumptions about the nature of the heterogeneity of the population to make them statistically plausible.
The TR model performs the worst, and requires the most assumptions.
Ultimately \textcite[1104]{Ballinger1997} conclude by continuing to call for development and testing of the stochastic component of choice.

\textcite{Carbone1997} investigates the RP model, in addition to the TR and SU models, by estimating each model for each subject in an experiment, and finds that the SU model performs the best of the three, with RP a close second.
\textcite[307]{Carbone1997} notes that if the set of alternative lotteries contains a lottery that stochastically dominates the others, the RP model requires the subject to always select the stochastically dominant lottery from the set, and that \enquote{this feature is of importance as it seems to capture well the experimental evidence.}
This feature of RP models, however, presents a problem when estimating preferences from choice data because, although violations of FOSD typically constitute a relatively small fraction of choices observed in experiments, this small fraction is reliably replicated in experiments.

\textcite{Loomes1998} (LS) performed a similar investigation of the RP, TR, and SU models and strongly reject the TR model and to a lesser extent the SU model.
The SU model over-predicts violations of FOSD; however, they note, as \textcite{Carbone1997} did, that even one violation of stochastic dominance in a dataset is sufficient to cause the \textit{stand-alone} RP model to collapse econometrically, and thus LS reject the model due to the few observations where stochastic dominance was violated.
LS note, however, that the RP model can potentially accommodate these violations if it is combined with another stochastic choice model, such as the TR model.
This point will be discussed in more detail subsequently.
LS also report systematic deviations from EUT near the edges of the Marschak-Machina triangle,\footnote{
	Recall that the vertices of the Marschak-Machina triangle represent potential outcomes in a lottery, given as a point in the triangle space.
	The probability of any given outcome is proportional to the geometric distance between the point representing the lottery and the vertex representing the outcome.
	Thus, if a lottery point lies on an edge of the triangle, the outcome associated with the opposite vertex has 0 probability.
	If a lottery point is minimally interior of an edge, the outcome associated with vertex opposite of the edge has a small, but positive, probability.} and suggest that these cannot be fully accommodated by any of the stochastic models.
In these instances, they suggest it is EUT that fails, not the stochastic models they consider.

\textcite{Loomes2002} also test the RP, TR, and SU models.
They recognize that \enquote{\textins*{t}here is no obvious reason to assume that only one of these forms of randomness is present} \parencite*[106]{Loomes2002}.\footnote{
	This is a point with which I only partly agree.
	While I agree that there isn't an obvious reason why some models cannot be jointly present, there \textit{is} a restriction on combining stochastic models: normative coherence.
	As discussed later, the RP model fails to satisfy this restriction.}
When faced with a choice situation, an agent may be best characterized as randomly drawing a preference from some set of preferences (RP model), evaluating the choice situation given that randomly drawn preference with some error (SU model), and then, with some positive probability, selecting an option irrespective of the agent's evaluation of the choice probability (TR model).
Practically, this means estimating additional parameters and making clear the identifying restrictions of any combination of these models, but mathematically these models are not mutually exclusive.\footnote{
	To illustrate the combination of RP, RE, and TR models derived in equations (\ref{eq2:RP.f}), (\ref{eq2:RE.f}), and (\ref{eq2:TR.f}):
	\begin{align*}
		{\Prob}(y_t=a) = \frac{\phi}{A} + (1-\phi)\int_{\beta \in B_t}  {\Prob}\left(  \epsilon_t \geq \frac{1}{\lambda} \left[ G(\beta_n,X_{bt}) - G(\beta_n,X_{at}) \right] \,\big|\, \beta \right) dF_n(\beta|\alpha)
	\end{align*}
	This model would require the estimation of $\alpha, \lambda$ and $\phi$.}

\textcite{Loomes2002} report that the best fitting stochastic model was RP plus TR paired with RDU.
In addition, the estimated probability of TR diminished with the number of questions answered by the subjects, as did apparent deviations from EUT.
When pairs with one lottery that stochastically dominates the other are removed from estimation, it is no longer clear that the RP model is superior to the SU model.
This result suggests that \enquote{trembles} can be un-learned.
\textcite{Hey2001} and \textcite{Moffatt2002} report similar results from experiments where it appears that noise is reduced with repetition.
\textcite{Hey2001} also reports diminishing deviations from EUT with repetition.

Other than the prominent TR, SU, and RP models, there are a variety of alternatives that receive less attention, most of which are derivatives of the SU model with heteroscedastic error terms as opposed to the homoscedastic error of SU.\footnote{
	The SU model posits an error term with a variance that is independent of the domain of the utility function it is added to.
	Thus it is said to be homoscedastic.
	If the error term is correlated with some part of the domain of the utility function to which it is added, it is said to be heteroscedastic.
	There are many ways in which this correlation may occur, leading to a large variety of heteroscedastic models.}
While TR and RP models can be manipulated in different ways to possibly explain more of the observed choice behavior,\footnote{
	For example, trembles could apply differently to FOSD pairs and non-FOSD pairs, and RP models could assume flexible distributions of the agent's preferences such as the Logit-Normal or Gamma distributions.
} such attempts often leave underlying, core problems with these models unattended to.
For instance, if TR models predict that with some probability choices will be made irrespective of underlying preferences, why then should this probability vary depending on certain special cases of choice scenarios faced by the subject? Even with flexible distributions of RP models, there is no underlying utility theory which allows violations of FOSD, hence standard, stand-alone RP models can never accommodate such observed violations.\footnote{
	However, non-standard RP models can sometimes accommodate violations of FOSD.
	An example of this is a model which specifies agents as randomly drawing a new preference relation for each option in a set of alternatives instead of randomly drawing one preference relation per set of alternatives.
	I explore the implications of this type of model, which I call the Random Preference Per Option (RPPO) model, later.
	For now, I will note that the RPPO model is very rarely utilized in the literature on stochastic models.
}
In contrast, the ability to manipulate the error term of the SU model in a tractable manner is part of what makes the SU model and its derivatives such popular models of stochastic choice.

To understand the motivations for developing the differing models that are derivatives of SU models, I briefly describe the concept of stochastic transitivity.
Borrowing from \textcite[210]{Wilcox2008}, consider three pairs of lotteries: $\{C,D\}$, $\{D,E\}$, and $\{C,E\}$, designated lotteries $1$, $2$, $3$, respectively.
$P_1$ is the probability of choosing $C$ in lottery $1$, and $P_2$ and $P_3$, are the probabilities of choosing $D$ and $C$ from lotteries $2$ and $3$, respectively.
We can define three forms of stochastic transitivity as follows:
\begin{align*}
	\text{Strong Stochastic Transitivity (SST):} \quad \mathit{min}(P_1,P_2) \geq 0.5 &\Rightarrow P_3 \geq \mathit{max}(P_1,P_2) \\
	\text{Moderate Stochastic Transitivity (MST):} \quad \mathit{min}(P_1,P_2) \geq 0.5 &\Rightarrow P_3 \geq \mathit{min}(P_1,P_2)\\
	\text{Weak Stochastic Transitivity (WST):} \quad \mathit{min}(P_1,P_2) \geq 0.5 &\Rightarrow P_3 \geq 0.5
\end{align*}

Stochastic transitivity (ST) enforces a probabilistic form of transitivity for the same reasons that the non-stochastic transitivity axiom is employed for deterministic theories of choice: it is a mathematically convenient, and normatively useful way to model the choice process of a viable economic agent.
Its consequence is that agents must make choices in a way that are at least stochastically consistent with economic success in an incentivized environment.
Each version of ST makes descriptive and normative predictions which are operationalized by the stochastic model that incorporates them.
For each proposed model described below, a particular version of ST is utilized because of its perceived superiority, either descriptively or normatively.
The SU model, for instance, requires SST, whereas most of the proposed derivatives of SU attempt to relax this requirement in favor of either MST or WST.

\textcite{Hey1995a} proposed three RE models with heteroscedastic error terms and conducted an experiment with 80 subjects to compare the heteroscedastic models to the homoscedastic SU model, denoted (H.1).
In all three models, the Normal distribution was utilized for $F(\cdot)$.
The first heteroscedastic model, (\ref{eq2:Hey2}), modeled the variance of the error as an exponential function of the time taken by the subject to give an answer to the question $m$ and a corresponding coefficient, $\alpha$:
\begin{align*}
	\tag{H.2}
	\label{eq2:Hey2}
	D(\beta_n,X) &= \exp(\alpha \times m)\\
	\lambda_n &= 1
\end{align*}
Thus if $\alpha > 0$, the longer (shorter) it takes a subject to answer the question, the more (less) \enquote{noisy} the subject's responses should be.
This is, however, no reason to expect $\alpha > 0$ \textit{a priori}.

The second heteroscedastic model, (H.3), modeled the error as an exponential function of the absolute value of the difference in utility of the alternatives multiplied by a coefficient, $\alpha$:
\begin{align*}
	\tag{H.3}
	\label{eq2:Hey3}
	D(\beta_n,X) &= \exp\left(\alpha \times \lvert G(\beta_n,X_{bt}) - G(\beta_n,X_{at}) \rvert\right)\\
	\lambda_n &= 1
\end{align*}
Thus, if $\alpha < 0$, the larger the difference in utility of the alternatives, the smaller the noise.

The third heteroscedastic model, (H.4), modeled the error as an exponential function of the \enquote{difficulty} of the question, $d$, multiplied by a coefficient, $\alpha$.
In this specification, $d$ is the average number of outcomes per option in the set of alternatives.
\begin{align*}
	\tag{H.4}
	\label{eq2:Hey4}
	D(\beta_n,X) &= \exp(\alpha \times d)\\
	\lambda_n &= 1
\end{align*}
Thus, if $\alpha > 0$, the greater (smaller) the average number of outcomes per option, the greater (smaller) the noise.
All of the heteroscedastic models nest the homoscedastic SU model as a special case (when $\alpha=0$).
While (\ref{eq2:Hey3}) and (\ref{eq2:Hey4}) did not perform particularly well, (H.1) was rejected in favor of (\ref{eq2:Hey2}) for 27 of 80 subjects at the 1\% level, and 36 subjects at the 5\% level \parencite*[639]{Hey1995a}.

Another derivative called the \enquote{Wandering Vector} (WV) model was proposed initially by \textcite{Carroll1980} and expanded on by \textcite{Carroll1991}.
It makes the standard deviation of the error proportional to the Euclidean distance of the probability vectors of the two alternative lotteries, $a$ and $b$.
\begin{align*}
	D(\beta_n,X) = \left[  \sum_{c=1}^C (p_c^a - p_c^b)^2 \right]^{1/2}
\end{align*}
The original rationale for this model was to incorporate MST into a stochastic model: \enquote{for many realistic multidimensional stimulus domains, SST seems \textit{too} strong.
On the other hand, WST seems too weak} [emphasis in the original] \parencite*[343]{Carroll1991}.
This model was proposed in the psychology literature to accommodate noisy perceptions of multidimensional stimuli, but this can be utilized as an economic model by reinterpreting the noisy perception of stimuli as noisy measurement of utility.

\textcite{Wilcox2011} expands on the \enquote{Contextual Utility} (CU) model initially proposed in \textcite{Wilcox2008}.
It makes the standard deviation of the error proportional to the difference in utility of the greatest non-zero probability outcome and the utility of the least non-zero probability outcome:
\begin{align*}
	\label{eq2:W.cu}
	\begin{split}
		&D(\beta_n,X_t) = \mathit{max}[u(x_{ct})] - \mathit{min}[u(x_{ct})]\\
		&\mathit{st.}\; w_c(x_{ct}) \neq 0
	\end{split}
\end{align*}
This model also satisfies MST and additionally allows for the \enquote{more risk averse than} relation of \textcite{Pratt1964} to be extended to the stronger \enquote{stochastically more risk averse than} relation.
The \enquote{stochastically more risk averse than} relation of the CU model allows for interpersonal comparisons of risk-aversion in a way that is potentially more meaningful than with the SU model \parencite*[221]{Wilcox2008}.

\textcite{Busemeyer1993} propose \enquote{Decision Field Theory} (DFT), which is limited in that it is only applicable to a pair of alternatives where one alternative is a certainty and the other is a lottery of only 2 outcomes.
If we define the set of outcomes that belong to the lottery as $H = \{x \in X_a \,|\, p_x < 1\} = \{h_1,h_2\}$ where $h_2 > h_1$ we have:
\begin{align*}
	D(\beta_n,X) = \left[ u(h_2) - u(h_1) \right] \sqrt{w_{h_1}(p)[1-w_{h_2}(p)]}
\end{align*}
The reasoning behind DFT is ultimately psychological.
\textcite{Busemeyer1993} posit that in cases where objective probabilities of outcomes are unknown, an agent may sample from past experiences with the same decision problem to estimate the objective probabilities.
This kind of decision problem is deemed choice under \enquote{uncertainty} rather than choice under \enquote{risk.}
\enquote{Decision field theory was developed for this more natural type of uncertain decision problem} \parencite*[436]{Busemeyer1993}.

\textcite{Blavatskyy2014} proposes a model (BF) deemed \enquote{Stronger Utility,} which is similar to the \enquote{incremental EU advantage model} initially proposed by \textcite{Fishburn1987}.
It makes the standard deviation of the error proportional to the difference in the utility of two abstract lotteries.
The first abstract lottery is constructed to stochastically dominate all lotteries in the proposed decision scenario, but can itself be stochastically dominated by any other lottery which also scholastically dominates the proposed lotteries.
The second abstract lottery is constructed to be stochastically dominated by both lotteries proposed in the decision scenario, while stochastically dominant any other lottery which is also stochastically dominated by both of the proposed lotteries.
For FOSD pairs, this specification attaches a probability of 1 to the dominating option and 0 to the dominated  option.

The three models proposed by \textcite{Hey1995} don't make any special predictions about the likelihood of choosing options in particular scenarios, such as with FOSD pairs, other than to hypothesize that they improve on the explanatory fit of the SU model.
They also require the estimation of additional parameters.
The (H.2) and (H.4) models of \textcite{Hey1995} are very similar to the method utilized by \textcite[142]{Harrison2009} in which observable characteristics are modeled as linear covariates of the core parameters to be estimated.
However, \textcite{Hey1995} models the standard deviation to be exponential functions of these observable characteristics instead of linear functions, making the heteroscedasticity multiplicative rather than additive.
The linear specification utilized by \textcite{Harrison2009} is used to control for observable heterogeneity of subjects, not aspects of the choice scenario.
The (H.3) model of \textcite{Hey1995}, however, seems to reflect the same line of thinking as the subsequent derivatives of the SU model.

The other RE models mentioned above, however, often add new implications which generally help ease some of the shortcomings of the SU model.
The CU and the DFT models both have the benefit of extending the \enquote{more risk averse than} relation of \textcite{Pratt1964} to the \enquote{stochastically more risk averse than} relation.
DFT additionally requires that as the lottery becomes closer to first order stochastically dominating the {\CE}, the probability of selecting the dominant option approaches 1.
The BF model also requires that the probability of selecting the dominant option is always 1.
Both the WV and the CU models enforce MST as opposed to the more restrictive strong SST,  required by SU models.
The CU, WV, DFT, and BF models don't require the estimation of any additional parameters on top of those required by the SU model, so any improvement of explanatory fit by them is free in terms of degrees of freedom used in classical estimation.

\textcite{Wilcox2008} provides a detailed discussion of the necessary implications of the TR, RP,  and SU models, along with some of the SU model's derivatives.
He also discusses various well known events which can sometimes be attributed to stochasticity in choice: the Common Ratio Effect, low-frequency, but persistent, violations of FOSD, and changes in choice probabilities when lotteries are simply scaled.\footnote{ Borrowing again from \textcite[249]{Wilcox2008}: Consider four pairs of lotteries, $\{C,E\}$, $\{D,E\}$,$\{C,E'\}$, $\{D,E'\}$, and make the probability of selecting the first lottery in pair $\{C,E\}$, $P_{\mathit{ce}}$, and similarly for the remaining pairs.
Simple Scalability requires that $P_{\mathit{ce}} > P_{\mathit{de}} \iff P_{\mathit{ce}'} > P_{\mathit{de}'}$.
This requirement is only met by transitive utility structures, such as EUT and RDU, combined with stochastic models that satisfy SST.}
Treatment is also given to whether models hold to various degrees of stochastic transitivity, whether they are descriptive of the \enquote{more risk averse than} relation, and how well the various models perform at predicting in-sample and out-of-sample choices.\footnote{ This particular topic was given extensive attention in \textcite{Wilcox2007}.}

\textcite{Wilcox2008} estimates WV, CU, RP, SU, and an early variation of SU proposed by \textcite{Luce1958} called \enquote{Strict Utility} on the dataset from \textcite{Hey2001}.
He employs the  method developed by \textcite{Vuong1989} to test if the log-likelihoods of the fitted models are significantly different from each other.
\textcite[273]{Wilcox2008} finds that given an RDU structure, the CU model fits significantly better ($p=0.013,p<0.0001$) than the WV and \enquote{Strict Utility} models in-sample, and significantly better ($p<0.0001, p=0.0005, p<0.0001, p<0.0001$) than the SU, RP, WV, and \enquote{Strict Utility} models out-of-sample.
Given an RDU structure, the RP model fits significantly better ($p=0.0388,p<0.0001$) than the WV and \enquote{Strict Utility} models in-sample, and significantly better ($p=0.049, p<0.0191, p<0.0001$) than the SU, WV, and \enquote{Strict Utility} models out-of-sample.
Neither the CU or RP models fit significantly better or worse than the SU model in-sample with a RDU structure.
In all cases where the CU and RP models both fit better than a third model, the CU model fits better by a greater margin than the RP model.

With an EUT structure, the CU model significantly improves on every model except the SU model in-sample, and on every model out-of-sample, while the RP model fits significantly worse ($p<0.058$) than the SU model and only improves on the \enquote{Strict Utility} significantly ($p<0.0001$) in-sample, but fits significantly better ($p=0.051,p=0.016,p=0.078$) than the SU, WV, and \enquote{Strict Utility} models out-of-sample.
In addition to fitting better than the RP model in a direct comparison, the CU model fits better than every other model that the RP model also improves on, and by a greater margin than the RP model.

From these results it is clear that \enquote{Strict Utility} is a poor model in terms of goodness of fit given the alternatives: it doesn't significantly fit better than any alternative model, regardless of the utility theory, either in-sample or out-of-sample.
The WV model only does marginally better: the only model it significantly improves upon is the \enquote{Strict Utility} model.

The more interesting story in light of the literature up to this point is the comparison of the SU, CU, and RP models.
Considering in-sample fit, the SU model fits significantly better than the RP model with EUT, and with RDU there is no significant difference in goodness of fit.
Out-of-sample the RP model fits  significantly better than the SU model under both EUT and RDU.
This echoes some of the mixed evidence up to this point concerning which of these two models is superior.
New to the competition are the various SU derivatives.
Two of these, the WV and \enquote{Strict Utility} models, perform relatively poorly in goodness of fit compared to the alternatives, but CU is shown to have generally superior performance compared to all the proposed models.
The CU model has a greater log-likelihood than than all of the other models in all of the various test conditions, in-sample or out-of-sample, with EUT or RDU.
This difference is statistically significant for many of the comparisons, as noted above.

This discussion is not meant to be an exhaustive list of every proposed derivative of the SU model and their implications.
Such a list would be very long and many of these derivative models deserve detailed discussion in their own right.
This discussion simply serves to demonstrate that, as put by \textcite[277]{Wilcox2008}, \enquote{we are witnessing a fertile period for stochastic model innovation now.}
Nearly all of this innovation has come from the RE class of models by changing the error term in the SU model from being homoscedastic to heteroscedastic, in very particular ways that have testable implications.
The only apparent non-SU derived innovation was to combine the TR model with the RP model to help explain the low-frequency, but persistent, violations of FOSD observed in economic experiments.\footnote{ This combination of models however is not an innovation to the RP model in itself as such a combination is just as possible with the SU model and any of its derivatives.
As stated earlier, such a combination fails to address a core problem with the RP model: normative coherence.} Given the variety of theoretical implications of the various stochastic models and the repeatedly demonstrated sensitivity of goodness of fit measures to alternative stochastic models, it is no wonder that \textcite[275]{Wilcox2008} concludes: \enquote{It is hard to escape the conclusion that decision research could benefit strongly from more work on stochastic models.}

While such a conclusion is undoubtedly true, there is a question relevant to economics concerning these models which has been sidelined in the continuing effort to find the \enquote{true} or \enquote{best} stochastic model: \enquote{What are the likely welfare implications of an economic agent's choices in an incentivized environment given an assumed stochastic model of choice?} This is the primary question of this chapter.
Answering this question helps to draw the distinction between economics and decision theory or psychology.
I argue that answering this question puts reasonable, restricting conditions on the econometric question \enquote{What is the best stochastic model to employ?}

\singlespacing
\section{Utility and its Relation to Welfare}
\doublespacing

With the RDU structure defined, and the stochastic models specified, we can define the basis of what will become our proposed metrics for individual welfare, the certainty equivalent.
For any salient lottery $X_a$, and any vector $\beta_n$, there exists some certain outcome, $\CE_a$, such that subject $n$ is indifferent between the lottery and the certainty equivalent:
\begin{equation}
	\label{eq2:CE.indiff}
	X_a \sim^n {\CE}_a \;\Leftrightarrow\; G(\beta_n,X_a) = G(\beta_n, {\CE}_a)
\end{equation}

Combining the RDU structure from equation (\ref{eq2:RDU}) with the utility function defined in equation (\ref{eq2:CRRA}), we can define the {\CE} as follows:
\begin{align}
	\label{eq2:CEcalc}
	\begin{split}
		&\sum_{c=1}^{C} w_c(p) \frac{x_{ca}^{(1-r)}}{(1-r)} = \frac{ {\CE}_a^{(1-r)}}{(1-r)}\\
		&{\CE}_a =  \left( (1-r) \times \sum_{c=1}^{C} w_c(p) \frac{x_{ca}^{1-r}}{(1-r)} \right)^{ \displaystyle\nicefrac{1}{(1-r)} }
	\end{split}
\end{align}

Thus if we assume some vector of $\beta_n$, of which $r$ and the parameters governing $w_c(p)$ are elements, we can easily calculate the {\CE} of lottery $X_a$.\footnote{ In general, the {\CE} of any lottery can easily be calculated with numerical methods even if an analytical solution doesn't exist.
This is because the {\CE} must lie in the interval between the lowest outcome and the highest outcome.
Numerically, one can just iterate through this interval until equation (\ref{eq2:CEcalc}) is satisfied, or employ an optimization routine to look for the {\CE} directly.}
If the utility function employed is monotonically increasing in the domain, as the CRRA function is, then this leads to the  logical corollary of equation (\ref{eq2:utilityrank}):
\begin{equation}
	\label{eq2:CErank}
	{\CE}_{n1} \geq {\CE}_{n2} \geq \ldots \geq {\CE}_{na} \geq \ldots \geq {\CE}_{nA}
\end{equation}

With equations (\ref{eq2:CEcalc}) and (\ref{eq2:CErank}), we can also see that the {\CE} can itself be considered a utility function, it is complete, transitive, and continuous, which is all that is required for a utility function to be well defined.
Utilizing the {\CE}s of options in a task is useful because of its potential to be considered a utility function, but also because it allows utility to be normalized to the units of the outcomes.

We can employ the notation used for the set of unchosen alternatives, $Z$, derived in equation (\ref{eq2:emptyset}), to rank the ${\CE}$s of each unchosen alternative just as in equation (\ref{eq2:Zutilityrank}):
\begin{equation}
	\label{eq2:CEZ}
	{\CE}_{n1}^Z \geq {\CE}_{n2}^Z \geq \ldots \geq {\CE}_{na}^Z \geq \ldots \geq {\CE}_{n(A-1)}^Z
\end{equation}

Utilizing these {\CE}s, four metrics are proposed to help measure welfare.
If an agent with deterministic preferences and a deterministic choice process is presented with two lotteries to chose from, $X_1$ and $X_2$, she would choose $X_1$ and receive a welfare change of ${\CE}_1 - {\CE}_2$.
This is the rational behind the first metric.
With this metric, a change in welfare is measured as the difference between the {\CE} of the option chosen and the {\CE} of the highest ranked alternative option:
\begin{equation}
	\label{eq2:WCt}
	\Delta W_{nt} = {\CE}_{nyt} - {\CE}_{n1t}^Z = {\CE}_{nt}^R
\end{equation}

This welfare metric is similar to the notion of compensating equivalence in standard consumer theory.
If equation (\ref{eq2:WCt}) is positive, it calculates the minimum amount of money the agent would need in compensation in order to change her choice.
If this metric is negative, it calculates the maximum the agent should be willing to pay in order to change her choice.

Another metric, which also utilizes the {\CE}s, characterizes welfare received by choosing an option as a proportion of the {\CE} of the option chosen and the {\CE} that was ranked highest in the task:
\begin{equation}
	\label{eq2:WPt}
	\%W_{nt} = \frac{ {\CE}_{ny} }{ {\CE}_{n1} }
\end{equation}
\noindent A variation of (\ref{eq2:WCt}) which can be used to make statements about welfare across tasks could be:
\begin{equation}
	\label{eq2:WCT}
	\Delta W_{nT} = \sum_{t=1}^T \left( {\CE}_{nyt} - {\CE}_{n1t}^Z \right)
\end{equation}
\noindent A similar variation of (\ref{eq2:WPt}) could be:
\begin{equation}
	\label{eq2:WPT}
	\%W_{nT} = \frac{\displaystyle\sum_{t=1}^{T} {\CE}_{ny} }{\displaystyle\sum_{t=1}^{T} {\CE}_{ny}}
\end{equation}

All of these metrics have strengths and weaknesses.
Metric (\ref{eq2:WCt}) is more relevant to the case of deterministic choice as it can change from task to task, while metrics (\ref{eq2:WPt}) and (\ref{eq2:WPT}) will become more relevant when discussing stochastic choice models and modest claims about inter-subject welfare.
Any agent would be best off by making a choice which maximizes either of these metrics.

\singlespacing
\subsection{Special Case of Random Preferences: The Random Preference Per Option Model}
\doublespacing

Before beginning the discussion of welfare, a final model belonging to the RP class will be noted which requires a more involved explanation.
It could be the case that an agent's choices are best characterized by a version of the RP model where a different $\beta^*_{na}$ is drawn to evaluate every option in the set of alternatives.
I refer to this type of RP model as the \enquote{Random Preference Per Option} (RPPO) model.

When evaluating option $a$, a standard preference relation is drawn from a distribution of preference relations that ranks each of the $A$ alternatives, including option $a$, according to this preference relation:
\begin{equation}
	\label{eq2:RPPO.jorank}
	X_{1t} \succcurlyeq^{na} X_{2t} \succcurlyeq^{na} \ldots \succcurlyeq^{na} X_{at} \succcurlyeq^{na} \ldots \succcurlyeq^{na} X_{At}
\end{equation}
\noindent A utility function exists which represents these preference relations is shaped by $\beta_n^a$:
\begin{equation}
	\label{eq2:RPPO.jurank}
	G(\beta_n^a,X_{1t}) \geq G(\beta_n^a,X_{2t}) \geq \ldots \geq G(\beta_n^a,X_{at}) \geq \ldots \geq G(\beta_n^a,X_{At})
\end{equation}

As was shown in equation (\ref{eq2:CEcalc}), a {\CE} can be calculated for each of these $\beta_n^a$ conditional utility functions such that:
\begin{equation}
	\label{eq2:RPPO.jCErank}
	{\CE}^a_{n1} \geq {\CE}^a_{n2} \geq \ldots \geq {\CE}^a_{na} \geq \ldots \geq {\CE}^a_{nA}
\end{equation}

In equations (\ref{eq2:RPPO.jorank}), (\ref{eq2:RPPO.jurank}), and (\ref{eq2:RPPO.jCErank}), the ordinal ranking of the set of alternatives is the same.
As stated previously, the {\CE} of an option has the same utility function properties as $G(\cdot)$, and is additionally normalized by the units of the options themselves.
If only one $\beta_n$ vector is drawn to derive cardinal values for the set of alternatives, we have the RP model discussed previously.

But, with RPPO models, when evaluating another option $b$, such that $b \neq j$, another, potentially different, preference relation is drawn and each of the $A$ alternatives, including both $a$ and $b$, are ranked according to this preference relation:
\begin{equation}
	\label{eq2:RPPO.korank}
	X_{1t} \succcurlyeq^{nb} X_{2t} \succcurlyeq^{nb} \ldots \succcurlyeq^{nb} X_{at} \succcurlyeq^{nb} \ldots \succcurlyeq^{nb} X_{At}
\end{equation}
\noindent A utility function exists which represents these preference relations shaped by $\beta_n^b$:
\begin{equation}
	\label{eq2:RPPO.kurank}
	G(\beta_n^b,X_{1t}) \geq G(\beta_n^b,X_{2t}) \geq \ldots \geq G(\beta_n^b,X_{at}) \geq \ldots \geq G(\beta_n^b,X_{At})
\end{equation}
\noindent And again, a {\CE} can be calculated for each of these $\beta_n^b$ conditional utility functions such that:
\begin{equation}
	\label{eq2:RPPO.kCErank}
	{\CE}^b_{n1} \geq {\CE}^b_{n2} \geq \ldots \geq {\CE}^b_{na} \geq \ldots \geq {\CE}^b_{nA}
\end{equation}

The difference between the realizations of equations (\ref{eq2:RPPO.jCErank}) and (\ref{eq2:RPPO.kCErank}) is twofold.
First, the different $\beta_n^*$ vectors may lead to different ordinal rankings of the same set of alternatives.
Second, if $\beta_n^a \neq \beta_n^b$, then there may be two different cardinal evaluations for the same option.

The RPPO model takes the cardinal value of each option, evaluated using its own $\beta_n^*$  vector, and constructs an ordinal ranking of the set of alternatives based on these individual evaluations.
To deal with the potential issue of comparing different utility functions cardinally, the utility functions $G(\cdot)$ should be normalized somehow.\footnote{ Without the normalization, the RPPO model requires an unusual interpretation of preference relations to accommodate particular aspects of relatively common $G(\cdot)$ functions.
An example will be presented later that will make this clearer.} 
In line with the previous discussion, we will assume a normalizing function which produces a {\CE} for each option based on its individually drawn $\beta_n^*$ vector.

Adding the superscripts $\{1,2,\ldots,x,\ldots,X\}$  to the {\CE} to indicate the option that it is associated with, and the subscripts $\{1,2,\ldots,a,\ldots,A\}$ to the {\CE} to represent its rank in the set of alternatives conditional on its individual $\beta_n^x$ vector, we could have the following ordinal ranking:
\begin{equation}
	\label{eq2:RPPO.CErank.f}
	{\CE}^1_{n1} \geq {\CE}^2_{n2} \geq \ldots \geq {\CE}^x_{na} \geq \ldots \geq {\CE}^X_{nA}
\end{equation}

Notice that the superscripts match the subscripts in this example, but this need not always be the case.
The superscripts represent the unranked options, while the subscripts represent the RPPO ranked options.
The following could also represent an ordinal ranking from the RPPO model:
\begin{equation}
	\label{eq2:RPPO.CErank.a}
	{\CE}^3_{n1} \geq {\CE}^1_{n2} \geq \ldots \geq {\CE}^x_{na} \geq \ldots \geq {\CE}^X_{nA}
\end{equation}

The RPPO model, as discussed here, is characterized as having random preference parameters, but is otherwise deterministic in characterizing choice.
That is, this RPPO model characterizes an agent as choosing the option with the highest individually evaluated {\CE}.
Just as with the RP model, additional stochasticity can be imposed by including measurement error as with RE models and/or a tremble event as with TR models.
These additional elements create unnecessary mathematical complexity for the current discussion, but they will be touched on briefly later.
Just as in equation (\ref{eq2:emptyset}), the non-chosen options can be expressed as the set of $A-1$ alternatives in task $t$ that doesn't include the chosen option:
\begin{equation}
	\label{eq2:RPPO:emptyset}
	Z = t \,\backslash\, y = \{z \in t \;|\; z \notin y \}
\end{equation}
\noindent The RPPO model therefore constructs a choice function as follows:
\begin{equation}
	\label{RPPO.y0}
	y_t = x \Leftrightarrow {\CE}_{na}^x \geq {\CE}_{nb}^z \quad \forall z \in Z
\end{equation}

That is, $y_t$ is a function that indicates that option $x$ is chosen in task $t$ if and only if the {\CE} associated with option $x$ is greater than or equal to the {\CE} of any other option $z$.
The probability of $y_t = x$ is determined by the joint probability of observing the set $\bigl\{\beta_n^x,\{\beta_n^Z\}\bigr\}$, such that:
\begin{equation}
	\label{eq2:RPPO.y1}
	{\CE}(\beta_n^x,X_{1t}) \geq {\CE}(\beta_n^z,X_{at}) \forall z \in Z
\end{equation}
\noindent Call such a set $\mathbf{B^t_n}$.
The probability of $y_t=x$ is therefore:
\begin{equation}
	{\Prob}(y_t=x) = \int_{\beta_n^x \in \mathbf{B^t_n}}\int_{\beta_n^Z \in \mathbf{B^t_n}} f_{\mathbf{B^t_n}}\!\left(\beta_n^x,\{\beta_n^Z\}|\alpha\right) \;d\beta_n^x \; d\beta_n^Z
\end{equation}
\noindent where $f_{\mathbf{B^t_n}}(\beta_n^x,\{\beta_n^Z\}|\alpha)$ is the joint density of the elements of $\mathbf{B^t_n}$, the shape of which is governed by the vector $\alpha$.

\singlespacing
\section[The Stochastic Money Pump: A Tool for Describing Welfare Accumulation]{The Stochastic Money Pump: A Tool for Describing Welfare Accumulation}
\doublespacing

With the various classes of stochastic models defined, I begin the discussion of the welfare implications of these models by first introducing a decision problem which resembles the \enquote{Money Pump} argument against intransitive structures in deterministic choice theory, though with several important distinctions that will be made clear later.
I will refer to this relatively simple thought experiment as a \enquote{Stochastic Money Pump} (SMP).
Assume that there is an experimental economist who, through cleverly selected choice problems, is able to correctly identify the utility structure and stochastic process governing the choices of subjects with perfect knowledge.
That is, the utility structure and relevant parameters $\beta_n$, as well as the correct stochastic model and relevant parameters that completely characterize some subject $n$ are all known by the experimenter.

The experimenter then offers to sell a lottery ticket to the subject for some amount of money, and should the subject agree to buy the ticket, the experimenter offers to buy the ticket back from the subject for a lower amount of money.
The subject can refuse the initial purchase, buy the ticket and refuse to sell the ticket back, or buy the ticket and sell it back to the experimenter.
How often can the experimenter expect to be successful in extracting the difference between the buying and selling price of the ticket from the subject without giving the subject anything, and what are the welfare implications of this pair of transactions?
The various classes of stochastic models can all have different welfare implications while predicting similar observed choice behavior.

To make this example concrete, we can work this problem out numerically assuming 3 different subjects, Amy, Beth, and Cate.
Amy operates with a random preference model of choice, Beth operates with a contextual utility model of choice, and Cate makes choices deterministically but with a tremble.
Amy, Beth, and Cate all have the same utility structure of the RDU special case where $w_c(p)=p_c$ for all $p_c$ and incorporate the CRRA utility function from equation (\ref{eq2:CRRA}).
Amy's distribution function $F_n(\beta|\alpha)$ is $N(\mu,\sigma^2) = N(0,0.01)$, thus normal with $\alpha$ featuring a mean equal to 0 and a standard deviation of 0.1.
Beth operates a $\beta$ vector that is composed of $r=0$, and a $\lambda_n = 0.015$.
Cate operates with a $\beta$ vector that is composed of $r=0$ and a probability of trembling of $\phi = 0.816$.
All values picked in this example are for ease of calculation, but the implications hold when generalized to different parameter values.

The lottery ticket has a 0.5 probability of an outcome of 10 and a 0.5 probability of an outcome of 100, and thus an expected value of 55.
The experimenter offers to sell each of the subjects the lottery for 55.50, and to buy the lottery back at 54.50.
These values are 0.50 above and below the {\CE} of the lottery for Beth and Cate, and  0.50 above and below the mean {\CE} of the lottery for Amy.
The probability of the experimenter successfully extracting money costlessly is approximately equal for all three subjects:
\begin{equation}
	\label{eq2:pr.extraction}
	{\Prob}(\mathit{Extraction}) = {\Prob}(\mathit{Buy}) \times {\Prob}(\mathit{Sell}) \approx 0.167
\end{equation}
\noindent The manner in which the this probability is reached is different for each subject:
\noindent For Amy,
\begin{align}
	\label{eq2:Amy}
	\begin{split}
		B_{\mathit{Buy}} &= \{ \beta_A |\; G(\beta_A,X_{\mathit{Lottery}}) \geq G(\beta_A,X_{\mathit{Buy price}})\}\\
		&= \{ r_A \big|\; r \leq -0.0232 \} \\
		B_{\mathit{Sell}} &= \{ \beta_A |\; G(\beta_A,X_{\mathit{Sell price}}) \geq G(\beta_A,X_{\mathit{Lottery}})\}\\
		&= \{ r_A \big|\; r \geq 0.0232 \} \\[0.5ex]
		{\Prob}(y_{\mathit{Buy}}=\mathit{Buy}) &= \int_{\beta \in B_{\mathit{Buy}}} dF_A(\beta|\alpha) = \phi(B_{\mathit{Buy}},0,0.01)\\[0.5ex]
		&\approx 0.408 \\
		{\Prob}(y_{\mathit{Sell}}=\mathit{Sell}) &= \int_{\beta \in B_{\mathit{Buy}}} dF_A(\beta|\alpha) = \phi(B_{\mathit{Sell}},0,0.01)\\[0.5ex]
		&\approx 0.408 \\
		{\Prob}(\mathit{Extraction}_A) &= {\Prob}(y_{\mathit{Buy}} = \mathit{Buy}) \times {\Prob}(y_{\mathit{Sell}} = \mathit{Sell})\\
		&\approx .167
	\end{split}
\end{align}
\noindent where $\phi$ is the cumulative normal distribution.
\noindent For Beth,
\begin{align}
	\label{eq2:Beth}
	\begin{split}
	D(\beta_B,X) \times \lambda_B &= 0.015[u(100) - u(10)] \times 1\\
	&= 1.35 \\
	{\Prob}(y_{\mathit{Buy}} = \mathit{Buy}) &=\dfrac{\exp\!\left( \dfrac{ G(\beta_B,X_{\mathit{Lottery}}) }{ D(\beta_B,X)\lambda_B }  \right)}{ \exp\!\left( \dfrac{ G(\beta_B,X_{\mathit{Lottery}}) }{ D(\beta_B,X)\lambda_B }  \right) + \exp\!\left( \dfrac{ G(\beta_B,X_{\mathit{Buy\ price}}) }{ D(\beta_B,X)\lambda_B }  \right)  }\\[0.5ex]
	&\approx 0.408 \\
	{\Prob}(y_{\mathit{Sell}} = \mathit{Sell}) &=\dfrac{\exp\!\left( \dfrac{ G(\beta_B,X_{\mathit{Sell\ price}}) }{ D(\beta_B,X)\lambda_B }  \right)}{ \exp\!\left( \dfrac{ G(\beta_B,X_{\mathit{Lottery}}) }{ D(\beta_B,X)\lambda_B }  \right) + \exp\!\left( \dfrac{ G(\beta_B,X_{\mathit{Sell\ price}}) }{ D(\beta_B,X)\lambda_B }  \right)  }\\[0.5ex]
	&\approx 0.408 \\
	{\Prob}(\mathit{Extraction}_B) &= {\Prob}(y_{\mathit{Buy}} = \mathit{Buy}) \times {\Prob}(y_{\mathit{Sell}} = \mathit{Sell})\\
	&\approx .167
	\end{split}
\end{align}
\noindent For Cate,
\begin{align}
	\label{eq2:Cate}
	\begin{split}
		{\Prob}_0(y_{\mathit{Buy}}) &=
		\begin{cases}
			1 &, \quad G(\beta_C,X_{\mathit{Lottery}}) > G(\beta_C,X_{\mathit{Buy\ price}})\\
			0.5 &, \quad G(\beta_C,X_{\mathit{Lottery}}) = G(\beta_C,X_{\mathit{Buy\ price}})\\
			0 &, \quad G(\beta_C,X_{\mathit{Lottery}}) < G(\beta_C,X_{\mathit{Buy\ price}})
		\end{cases} \\
		{\Prob}_0(y_{\mathit{Sell}}) &=
		\begin{cases}
			1 &, \quad G(\beta_C,X_{\mathit{Sell\ price}}) > G(\beta_C,X_{\mathit{Lottery}})\\
			0.5 &, \quad G(\beta_C,X_{\mathit{Sell\ price}}) = G(\beta_C,X_{\mathit{Lottery}})\\
			0 &, \quad G(\beta_C,X_{\mathit{Sell\ price}}) < G(\beta_C,X_{\mathit{Lottery}})
		\end{cases}\\
		{\Prob}(y_{\mathit{Buy}}&=\mathit{Buy}) = (1-\phi) {\Prob}_0(y_{\mathit{Buy}}) + \frac{\phi}{A}\\
		&=(1-0.816)(0) + (0.816)/2\\
		&=0.408\\
		{\Prob}(y_{\mathit{Sell}}&=\mathit{Sell}) = (1-\phi) {\Prob}_0(y_{\mathit{Sell}}) + \frac{\phi}{A}\\
		&=(1-0.816)(0) + (0.816)/2\\
		&=0.408\\
	{\Prob}(\mathit{Extraction}_C) &= {\Prob}(y_{\mathit{Buy}} = \mathit{Buy}) \times {\Prob}(y_{\mathit{Sell}} = \mathit{Sell})\\
	&\approx .167
	\end{split}
\end{align}

While the observed choice behavior is identical for all three subjects, the welfare implications are not.
According to the metrics defined by equations (\ref{eq2:WCt}) and (\ref{eq2:WPt}), this decision problem has the same welfare implications for Beth and Cate:
\begin{align}
	\label{eq2:BCwelfare}
	\begin{split}
		\Delta W_{(B,C),\mathit{Buy}} = {\CE}_{(B,C),\mathit{Lottery}} - {\CE}_{(B,C),\mathit{Buy\ Price}} &= 55 - 55.5 = -0.50\\
		\Delta W_{(B,C),\mathit{Sell}} = {\CE}_{(B,C),\mathit{Sell\ Price}} - {\CE}_{(B,C),\mathit{Lottery}} &= 54.5 - 55 = -0.50\\
		\% W_{(B,C),\mathit{Buy}} = \frac{{\CE}_{(B,C),\mathit{Lottery}}}{{\CE}_{(B,C),\mathit{Buy\ Price}}} &= \frac{55}{55.5} \approx 0.99\\
		\% W_{(B,C),\mathit{Sell}} = \frac{{\CE}_{(B,C),\mathit{Sell\ Price}}}{{\CE}_{(B,C),\mathit{Lottery}}} &= \frac{54.5}{55} \approx 0.99
	\end{split}
\end{align}

In this case, the welfare implications of such decision problem are clear for both the TR and RE  models: with a roughly $0.167$ probability, Beth and Cate will make 2 consecutive \enquote{mistakes} or \enquote{choice errors,} which results in 1 unit of money and 1 unit of {\CE} being extracted from each of them.
The other potential results are easy to calculate.
With a ${1- {\Prob}(y_{\mathit{Buy}}} = \mathit{Buy}) = 0.592$ probability, Beth and Cate make no mistakes and experience a welfare gain:
\begin{align}
	\begin{split}
		\Delta W_{(B,C),\mathit{Buy}} &= {\CE}_{(B,C),\mathit{Buy\ Price}} - {\CE}_{(B,C),\mathit{Lottery}} = 55.5 - 55 = 0.50\\
		\% W_{(B,C),\mathit{Buy}} &= \frac{{\CE}_{(B,C),\mathit{Buy\ Price}}}{{\CE}_{(B,C),\mathit{Buy\ Price}}} = \frac{55.5}{55.5} = 1
	\end{split}
\end{align}

With a $ {\Prob}(y_{\mathit{Buy}} = \mathit{Buy})(1 - {\Prob}(y_{\mathit{Sell}}= \mathit{Sell})) \approx 0.242$ probability, Beth and Cate make the mistake of buying the lottery ticket, but not the mistake of selling it back for less:
\begin{align}
	\begin{split}
		\%W_{(B,C),T} &= \frac{\displaystyle\sum_{t=1}^{T} {\CE}_{(B,C),y,t} }{\displaystyle\sum_{t=1}^{T} {\CE}_{(B,C),1,t}} = \frac{55 + 55}{55.5 + 55} \approx 0.995 \\
		\Delta W_{(B,C),T} &= \sum_{t=1}^T \left( {\CE}_{(B,C),y,t} - {\CE}_{(B,C)1,T}^Z \right)= \genfrac{}{}{0pt}{}{55 - 55.5}{+ 55.5 - 55} = 0
	\end{split}
\end{align}

The RP model characterizing Amy's choices, however, does not provide an intuitive understanding of the welfare implications of this decision problem.
The RP model discussed here, the stand-alone RP model, requires that every choice by Amy be characterized by a deterministic preference relation according to some vector $\beta_A$ randomly drawn from a distribution.
Thus:
\begin{align}
	\label{eq2:RP.welfare}
	\begin{split}
		\Delta W_{(A),\mathit{Buy}} &= {\CE}_{(A),\mathit{Lottery}} - {\CE}_{(A),\mathit{Buy\ Price}} \geq 0\\
		\Delta W_{(A),\mathit{Sell}} &= {\CE}_{(A),\mathit{Sell\ Price}} - {\CE}_{(A),\mathit{Lottery}} \geq 0\\
		\% W_{(A),\mathit{Buy}} &= \frac{{\CE}_{(A),\mathit{Lottery}}}{{\CE}_{(B,C),\mathit{Lottery}}} = 1\\
		\% W_{(A),\mathit{Sell}} &= \frac{{\CE}_{(A),\mathit{Sell\ Price}}}{{\CE}_{(B,C),\mathit{Sell\ Price}}} = 1
	\end{split}
\end{align}

According to the metric definition in (\ref{eq2:WCt}) and the decision process for Amy defined in (\ref{eq2:Amy}), the $\Delta W$ welfare evaluations in (\ref{eq2:RP.welfare}) must be weak inequalities.
However, the only situation where $\Delta W_{(B,C),\mathit{Buy}} = 0$ is when $r=-0.0232$, which has a probability of $0$ given that $F_A(\beta|\alpha)$ is continuous.
Similarly for the choice between selling or not selling.
With a probability approaching $1$, the RP model predicts that should an extraction occur the choices causing the extraction leave the subject with positive welfare.

Before moving on to the normative implications of these welfare characterizations, we revisit the SMP thought experiment with two new entrants, Dana and Emma.
Suppose that Dana's choices are characterized as being in accordance with the RPPO model discussed previously, and Emma operates a tremble model as defined by \textcite{Loomes2002}, where $ {\Prob}_0$ is defined by the RP model.
We can pose the same questions concerning Dana and Emma's choices that we asked of Amy, Beth, and Cate's choices: \enquote{How often can the experimenter expect to be successful in extracting the difference between the buying and selling price of the ticket from the subject without giving the subject anything and what are the welfare implications of this pair of transactions?}
As we will see below, the answers to these questions for Dana are exactly the same as for Amy, and though the math involved with Emma is slightly more complicated, the counterintuitive interpretation of welfare caused by the RP model remains.

Suppose that for Dana, the marginal distributions of each option's $\beta_A^*$ vector used to construct $f_{\mathbf{B_n^t}}(\beta_n^ x,\beta_n^ z|\alpha)$ are identical and uncorrelated.\footnote{
	These assumptions are made for mathematical simplicity in the following example.
	There is no obvious reason why it should be necessary that these distributions be identical or uncorrelated, though the conceptual result would be the same even if they were not.
} 
Also, Dana's marginal distribution functions are all $N(\mu,\sigma^2) = N(0,0.01)$, thus normal with $\alpha$ consisting of a mean equal to $0$ and a standard deviation of $0.1$.
The joint density function, $f_{\mathbf{B_n^t}}(\beta_n^x,\beta_n^z|\alpha)$, is therefore:
\begin{align}
	\begin{split}
		N_2&(\mu,\Sigma) \\
		\mu =\begin{Bmatrix}\mu_x \\ \mu_z\end{Bmatrix} &= \begin{Bmatrix}0\\0\end{Bmatrix}\\
		\Sigma =\begin{Bmatrix} \sigma_x^2 & \rho\sigma_x\sigma_z \\  \rho\sigma_x\sigma_z & \sigma_z^2 \end{Bmatrix} &=
	\begin{Bmatrix} 0.01 & 0 \\  0 & 0.01 \end{Bmatrix}
	\end{split}
\end{align}
\noindent where $\mu$ is the vector of means, $\Sigma$ is the covariance matrix for the joint density function $f_{\mathbf{B_n^t}}(\beta_n^x,\beta_n^z|\alpha)$.
The choice behavior for Dana is as follows:

\noindent For Dana,
\begin{align}
	\begin{split}
		{\CE}_{\mathit{Buy\ Price}} &= \mathit{Buy\ Price} \;\forall\; \beta_D\\
		{\CE}_{\mathit{Sell\ Price}} &= \mathit{Sell\ Price} \;\forall\; \beta_D\\[1.5ex]
		\mathbf{B_D^{\mathit{Buy}}} &= \{ \beta_D^x,\beta_D^z | {\CE}_{\mathit{Lottery}}^x \geq {\CE}_{\mathit{Buy\ Price}}^Z \}\\
		&= \{ r_D^x,r_D^Z | r_D^x \leq -0.0232, r_D^Z \in \Re\}\\[1.5ex]
		\mathbf{B_D^{\mathit{Sell}}} &= \{ \beta_D^x,\beta_D^z | {\CE}_{\mathit{Sell\ Price}}^x \geq {\CE}_{\mathit{Lottery}}^Z \}\\
		&= \{ r_D^x,r_D^Z | r_D^x \geq 0.0232, r_D^Z \in \Re\}\\[1.5ex]
		{\Prob}(y_{\mathit{Buy}} = \mathit{Buy}) &= \int_{\beta_D^x \in \mathbf{B^t_D}}\int_{\beta_D^Z \in \mathbf{B^t_D}} f_{\mathbf{B^t_D}}\!\left(\beta_D^x,\{\beta_D^Z\}|\alpha\right) \;d\beta_D^x \; d\beta_D^Z\\
		&= \phi(r_D^Z \leq -0.0232,0,0.01) \times \phi(r_D^x \in \Re,0,0.01)\\
		&= 0.408 \times 1\\
		{\Prob}(y_{\mathit{Sell}} = \mathit{Sell}) &= \int_{\beta_D^x \in \mathbf{B^t_D}}\int_{\beta_D^Z \in \mathbf{B^t_D}} f_{\mathbf{B^t_D}}\!\left(\beta_D^x,\{\beta_D^Z\}|\alpha\right) \;d\beta_D^x \; d\beta_D^Z\\
		&= \phi(r_D^x \in \Re,0,0.01) \times \phi(r_D^Z \geq 0.0232,0,0.01)\\
		&= 1 \times 0.408\\
	{\Prob}(\mathit{Extraction}_D) &= {\Prob}(y_{\mathit{Buy}} = \mathit{Buy}) \times {\Prob}(y_{\mathit{Sell}} = \mathit{Sell})\\
	&\approx .167
	\end{split}
\end{align}

This thought experiment presents a special case where the RPPO model essentially reduces to the standard RP model.
This is due to the fact that the {\CE} of any certain amount of money is equal to that amount of money.
Because this is true, the distributions of the $\beta_D$ vectors associated with the buying and selling prices are irrelevant.\footnote{
	Take any degenerate lottery $X$ comprised of a single outcome $x$ with a probability of $p_x =1$.
	The utility of this lottery is $U_X = w_x( p_x )u_x(x )$, where $w_x$ is any probability weighting function and $u_x$ is any utility function.
	Since $w_x(p_x=1)=1$ for every probability weighting function, $U_X=u_x(x)=u_x({\CE}_x)$.
	Since $u_x$ is a well-defined utility function, ${\CE}_x=x$ is a solution for equation (\ref{eq2:CE.indiff}) for every $u_x$ and every $x$ when $p_x=1$.
	This solution is unique when $u_x$ is monotonic, as with the CRRA function employed in the example.
} 

The reason why the utility functions are normalized can also be made clear with this  example.
The CRRA function described in (\ref{eq2:CRRA}) and utilized in the above example has some interesting properties around $r=1$: $u(x|r) = \ln(x)$ at $r=1$, $u(x|r) \to \infty$ as $r \to 1$ from the left, and $u(x|r) \to -\infty$ as $r \to 1$ from the right.\footnote{
	The use of $\ln(x)$ for $r =1$ is not as \textit{ad hoc} as it may seem.
	\textcite[1333]{Wakker2008} shows that if equation (\ref{eq2:CRRA}) is normalized, it can be seen \enquote{that the normalized logarithmic function is the limit of the normalized power functions for $r$ tending to $0$ \textins{1}, both from above $(r >0)$ \textins{$r>1$} and from below $(r<0)$ \textins{$r<1$}:}
	\begin{align*}
		\lim_{r \to 0} \frac{x^r - c^r}{d^r - c^r} = \frac{\ln(x) - \ln(c)}{\ln(d) - \ln(c)} \quad \forall \; x>0 , d>c>0
	\end{align*}
	While \textcite{Wakker2008} uses the single exponent version of the power function, the same limit applies to the formulation of the CRRA function used in equation (\ref{eq2:CRRA}), with the bracketed values of $r$ in the above quote representing the revised limits.
}
If the RPPO model didn't normalize the CRRA function to its {\CE}, the set of $\{\beta_n^x,\{\beta_n^Z\}\}$  would be contradictory in its elements due to the properties of the CRRA function around $1$.
To demonstrate, assume that the utility of the lottery is evaluated with $r_n^x = -0.0232$: what values of $r_n^Z$ satisfy equation (\ref{eq2:CEcalc}) such that Dana would decide to purchase the lottery? We might expect that since $r_n^x = -0.0232$ is the value of $r_n$ that sets the utility of the lottery and the utility of the buy price equal to each other,  should the buy price be evaluated with greater risk aversion than the lottery, equation (\ref{eq2:RPPO.y1}) will hold.
That is, we might expect that should $r_n^Z > -0.0232$ Dana would prefer the lottery over the buy price.

Intuitively this makes sense: an increase in risk aversion corresponds to an increase in the concavity of the utility function under EUT (or holding a probability weighting function constant under RDU) which implies lower utility for a given outcome as risk-aversion increases.
While this is true when the CRRA function is normalized to its {\CE}, this isn't true without the normalization.
Without the normalization, $\beta_n^Z$ consists of $-0.023 \leq r_n^Z \leq 0.9814$ and $r_n^Z > 1$.

The gap of $(0.9814,1)$ is due to the properties of the CRRA function around 1.
The un-normalized RPPO model allows for a relatively risky option to be chosen over a relatively less risky option should the less risky option be evaluated using a preference relation indicating intense risk aversion, but not when the less risky option is evaluated using a preference relation indicating only moderate risk aversion.
Intuitively we might expect the reverse to be true, but it is mathematically possible.
The possibility of this kind of gap is not removed even if the marginal distributions which make up $f_{\mathbf{B^t_n}}(\beta_n^x,\{\beta_n^Z\}|\alpha)$ are correlated and can be exacerbated if they are not identical.

Now, suppose Emma operates a TR model with the ${\Prob}_0$ choice probabilities generated by the RP model described for Amy and the tremble parameter described for Cate.
Emma's choice behavior is defined as follows:

\noindent For Emma,
\begin{align}
	\label{eq2:Emma}
	\begin{split}
		B_{\mathit{Buy}} &= \{ \beta_E |\; G(\beta_E,X_{\mathit{Lottery}}) \geq G(\beta_E,X_{\mathit{Buy price}})\}\\
		&= \{ r_E \big|\; r \leq -0.0232 \} \\
		B_{\mathit{Sell}} &= \{ \beta_E |\; G(\beta_E,X_{\mathit{Sell price}}) \geq G(\beta_E,X_{\mathit{Lottery}})\}\\
		&= \{ r_E \big|\; r \geq 0.0232 \} \\[0.5ex]
		{\Prob}_0(y_{\mathit{Buy}}) &= \int_{\beta \in B_{\mathit{Buy}}} dF_E(\beta|\alpha) = \phi(B_{\mathit{Buy}},0,0.01)\\[0.5ex]
		&\approx 0.408 \\
		{\Prob}_0(y_{\mathit{Sell}}) &= \int_{\beta \in B_{\mathit{Buy}}} dF_E(\beta|\alpha) = \phi(B_{\mathit{Sell}},0,0.01)\\[0.5ex]
		&\approx 0.408 \\
		{\Prob}(y_{\mathit{Buy}}=\mathit{Buy}) &= (1-\phi) {\Prob}_0(y_{\mathit{Buy}}) + \frac{\phi}{A}\\
		&=(1-0.816)(0.408) + (0.816)/2\\
		&= 0.483072\\
		{\Prob}(y_{\mathit{Sell}}=\mathit{Sell}) &= (1-\phi) {\Prob}_0(y_{\mathit{Sell}}) + \frac{\phi}{A}\\
		&=(1-0.816)(0.408) + (0.816)/2\\
		&= 0.483072\\
		{\Prob}(\mathit{Extraction}_E) &= {\Prob}(y_{\mathit{Buy}} = \mathit{Buy}) \times {\Prob}(y_{\mathit{Sell}} = \mathit{Sell})\\
		&\approx 0.233
	\end{split}
\end{align}

We have a more complicated result with Emma when we attempt to describe her welfare.
The preferences in this model are provided by the aspects that belong to the RP model.
This means that not only is ${\Prob}_0(y_{\mathit{Buy}})$ the probability that Emma would chose to buy the lottery should she not experience a \enquote{tremble,} but it is also the probability that the choice to buy the lottery is the result of greater utility being accumulated from the lottery ticket than is associated with the buying price.
Similarly for ${\Prob}_0(y_{\mathit{Sell}})$ and the choice to sell the ticket.
Thus, given ${\Prob}(\mathit{Extraction}_E)$, ${\Prob}_0(y_{\mathit{Buy}})$, and ${\Prob}_0(y_{\mathit{Sell}})$, we have:
\begin{align}
	\begin{split}
		{\Prob}\left(\% W_{(E),\mathit{Extraction}} = 1\right) &= {\Prob}(\mathit{Extraction}_E) \times {\Prob}_0(y_{\mathit{Buy}}) \times {\Prob}_0(y_{\mathit{Sell}})\\
		&= 0.233 \times 0.408 \times 0.408\\
		&\approx 0.0388
	\end{split}
\end{align}

That is, we characterize Emma as having the 1 unit of money extracted by the SMP, \textit{and} this extraction as having resulted in optimal welfare for Emma $3.88\%$ of the time.
Similarly, this probability can be interpreted as a lower bound on ${\Prob}(\Delta W_{(E),\mathit{Extraction}} \geq 0)$.

The probability that the welfare surplus metric is positive in the event of an extraction is equivalent to the probability of an extraction occuring times the probability that the welfare change from buying the ticket plus the welfare change from selling the ticket is positive:
\begin{align*}
{\Prob}(\mathit{Extraction}_E) &\times {\Prob}(\beta_{\mathit{Buy}},\beta_{\mathit{Sell}} \big| \; {\CE}_{\mathit{Lottery}}^{Buy} - {\CE}_{\mathit{Buy Price}}^{Buy} + {\CE}_{\mathit{Sell Price}}^{Sell} - {\CE}_{\mathit{Lottery}}^{Sell} )\\
{\Prob}(\mathit{Extraction}_E) &\times {\Prob}(\beta_{\mathit{Buy}},\beta_{\mathit{Sell}} \big| \; {\CE}_{\mathit{Lottery}}^{Buy} - {\CE}_{\mathit{Lottery}}^{Sell} \geq 1 )
\end{align*}
If we set $\mathbf{\beta_{\mathit{B,S}}} = \{ \beta_{\mathit{Buy}},\beta_{\mathit{Sell}} \big| \; {\CE}_{\mathit{Lottery}}^{Buy} - {\CE}_{\mathit{Lottery}}^{Sell} \geq 1\}$,\footnote{
	Recall that the {\CE} of an certain amount is always that certain amount.
	So the {\CE} of the "Buy Price" is $55.5$, and the {\CE} of the sell price is $54.5$ for any utility function.
	Therefore ${\CE}_{\mathit{Buy Price}} - {\CE}_{\mathit{Sell Price}} = 1$ for all utility functions.
} we have:
\begin{align}
	0.233 \times \int_{\beta_{\mathit{Buy}} \in \mathbf{\beta_{\mathit{B,S}}}} \int_{\beta_{\mathit{Sell}} \in \mathbf{\beta_{\mathit{B,S}}}} F(\beta_{\mathit{Buy}} | \alpha) F(\beta_{\mathit{Sell}}|\alpha) d\beta_{\mathit{Buy}} d\beta_{\mathit{Sell}} \geq 0.0388
\end{align}

This complication arises because Emma's choice function has both TR and RP elements.
Sometimes Emma will value the prospect of buying the lottery ticket extremely highly, not tremble, and choose to buy, and then value the prospect of selling the ticket back only somewhat negatively, tremble, and then sell it back.
The welfare gained in the buying choice can therefore sometimes outweigh the welfare lost in the selling choice, resulting in a net positive change of welfare.
Note that every time Emma values the buying of the ticket \textit{and} the selling of the ticket positively, the net change in welfare will also be positive.
Thus, $0.0388$ constitutes a lower bound on ${\Prob}(\Delta W_{(E),\mathit{Extraction}} \geq 0)$.

\singlespacing
\section{The Normative Coherence of Stochastic Models}
\doublespacing

% The Craft of Economics
% Edward E. Leamer
% p 25 -  Fortunately, our goal as economists is not soundness, but usefulness.
% p 26 - We would make progress if we could agree that \textit{our models are neither true nor false; our models are sometimes useful and sometimes misleading}.
% p 30 - The primary goal \textins{of economics} should not be to amuse each other with mathematical complexities... \textins{it} should be to design policy interventions - policies that are intended to help achieve social objectives, notably the highest level of well-being for the largest number of people.

% Pigou - The Economics of Welfare, 3rd edition 1928
% The complicated analyses which economists endeavour to carry through are not mere gymnastic. They are instruments for the bettering of human life

Having clarified the welfare implications of several representative stochastic models, I can discuss the normative implications of these models.
When a variety of stochastic models were detailed by \textcite{Becker1963}, they were intended as a way to \enquote{circumvent the difficulty} associated with the problem that \enquote{the preference choices of the chooser are are often inconsistent with each other.}
In laying out the implications of these models, \textcite{Becker1963} detail descriptive implications about the frequency with which certain types of choices would be observed, but do not make any normative claims.
The intent behind developing these models was to provide greater \textit{descriptive} veracity to EUT, apparently while maintaining EUT as the \textit{normative} force behind these models.

The historical course that lead to EUT becoming the dominant orthodox theory appears to have started with a descriptive justification, then a normative justification.
\textcite{Moscati2016} details the correspondences between Paul Samuelson, Leonard Savage, Jacob Marschak, and Milton Friedman concerning the axiomatization of EUT by \textcite{VonNeumann1944} and its burgeoning acceptance as the orthodox theory of choice involving risky outcomes.
The correspondence highlights Samuelson's strong initial reluctance to accept EUT based on his dissatisfaction with what later became known as the independence axiom, which he called a \enquote{gratuitously-arbitrary-special-implausible hypothesis} \parencite[225]{Moscati2016}.
Savage, and to a lesser extent Marschak and Friedman, advocated for EUT as being descriptively accurate, theoretically simple, and, eventually, normatively robust.
Samuelson had strong reservations about the descriptive veracity of EUT, advocated by both Savage and Friedman, stating that the phenomena associated with gambling are \enquote{infinitely richer} than EUT permitted \parencite[227]{Moscati2016}.
Friedman also admitted that in order to be able to explain certain gambling phenomena, EUT would \enquote{need complication} \parencite[229]{Moscati2016}.
Eventually, however, Samuelson was persuaded of EUT's normative force by Savage's discussion of what would become known as the \enquote{Sure-Thing Principle,} without necessarily relenting on the descriptive claims.

What is meant by \enquote{normative force} in these discussions is the specification of what a \enquote{rational} agent \enquote{ought} to do.
Marschak, in correspondence with Samuelson, makes this distinction:
\enquote{It may be \textit{usual} for village carpenters \textelp{} to deviate from the advice of Euclidian geometers \textelp{} All the same, they would be better advised to behave rationally by following Euclid} \parencite[229]{Moscati2016}.
In relenting to Savage's normative arguments, Samuelson concedes that the normative value of the Independence Axiom, and by extension EUT, makes it useful as an assumption \enquote{defining \enquote{rational} behavior} \parencite[231]{Moscati2016} despite maintaining that EUT doesn't provide \enquote{a very illuminating explanation} of gambling or investment behavior even \enquote{as a first approximation} \parencite[232]{Moscati2016}.

The appeal of normative arguments, and their apparent superiority to descriptive veracity arguments in the case of accepting EUT, is based on their adding to a theory's potential to generate statements about the welfare of agents in incentivized environments.
\textcite[preface to the third edition]{Pigou1929} prefaces his third edition with a comment to future students of economics:

\singlespacing
\blockquote{
The complicated analyses which economists endeavour to carry through are not mere gymnastic.
They are instruments for the bettering of human life.
The misery and squalor that surround us, the injurious luxury of some wealthy families, the terrible uncertainty overshadowing many families of the poor - these are evils too plain to be ignored.
By the knowledge that our science seeks it is possible that they may be restrained.
Out of the darkness light!
To search for it is the task, to find it perhaps the prize, which the \enquote{dismal science of Political Economy} offers to those who face its discipline.
}
\doublespacing

\noindent \textcite[238]{Varian1996} writes: \enquote{economics is a policy science and, as such, the contribution of economic theory to economics should be measured on how well economic theory contributes to the understanding and conduct of economic policy.}
\textcite[30]{Leamer2012} echoes this sentiment:
\enquote{The primary goal \textins{of economics} should not be to amuse each other with mathematical complexities \textelp{} \textins{it} should be to design policy interventions - policies that are intended to help achieve social objectives, notably the highest level of well-being for the largest number of people.}\footnote{
	This utilitarian conceptualization of the primary goal of economics can be relaxed somewhat without losing force.
	Policy interventions can be, and often are, crafted to improve the welfare of specific sub-populations.
	For instance, the kind of individuals referenced by Pigou may have a disproportionate number of policies crafted to improve their lot relative to their numbers in the total population without this being at odds with the goals of economics.
}

It is with these sentiments, both historical and contemporary, that I interrogate the extent to which stochastic models presented in this chapter provide useful methodologies for the design and interpretation of policy interventions.
Critical to this objective is the ability to make statements on how changes in stocks of assets affect the welfare of individuals.
The various criteria used to assess the normative validity of stochastic models will largely be interpreted from the works of \textcite{Grune-Yanoff2014}, \textcite{Berg2014} and \textcite{Hands2014}.
Normative criteria are relations of particular means and ends that describe what an agent \enquote{ought} do or not do in certain circumstances.
Economic theories purporting to be normatively justified must provide the mechanisms that satisfy these relationships.
In this framework, I will primarily discuss the capacity of stochastic models provide to generate outcomes that have commonly been employed as normative criteria.

%These normative criteria, however, are often framed in the context of choice scenarios, which themselves may not make the specific underlying ends formally clear.
%What is also potentially unclear is whether these normative criteria are chosen on the basis of observed reality or if they are derived some idealistic construction of agency.
%That is, are normative criteria \enquote{naturalized} or are they \enquote{idealized}.
%We will attempt to make clear this distinction when possible.

\singlespacing
\subsection{Economic Existence and Objective Betterness Criteria}
\doublespacing

%The concept of continued economic existence as a requirement of an economic theory is likely the most common reason given as to why a theory is normatively justified.
For an economic theory to gain normative force, it should satisfy the constraint that an agent ought not to make choices systematically in such a way as to drive herself out of economic existence.
For example, this could mean that an economic theory should not normatively justify an agent's choice to deliberately put themselves into bankruptcy.\footnote{
	Bankruptcy here is in meant in the abstract sense of the loss of all assets without recourse to recover them.
	In the United States for instance, given what are called \enquote{Chapter 9} and \enquote{Chapter 11} bankruptcy provisions, it could be quite rational to engage in institutionally controlled bankruptcy under certain circumstances.
}

The popular, often informal, way EUT is justified in light of this criteria is through its invulnerability to money pumps.
The traditional money pump is defined as a series of trades that will succeed in extracting the entirety of an agent's stock of assets, say some amount of good $A$, if the agent has intransitive preferences, such as $B \succcurlyeq A$, $C \succcurlyeq B$, $A \succcurlyeq C$ with at least one relation being strict.
This occurs when the extractor offers to trade his $B$ for the agent's $A$, then his $C$ for her $B$, and finally his $A - \epsilon$ for her $C$, where $\epsilon$ is a sufficiently small, but positive amount of good $A$ such that $A - \epsilon \succcurlyeq C$, \textit{and those trades are accepted}.
This process is repeated until the agent has no remaining quantity of good $A$, and is thus economically eliminated.

\textcite[402-403]{Hands2014} refers to this argument as \enquote{empirical elimination:}

\singlespacing
\blockquote{
This is the argument that agents who act in ways that violate \textins{rational choice theory} will (in fact) cease to exist or at least cease to play an active role among the relevant class of decision-makers.
The two most common forms of this argument are the money pump (for agents who have intransitive preferences and thus make choice mistakes) and the Dutch book\textelp{}
}

\doublespacing
\noindent \textcite[336]{Grune-Yanoff2014} refer to \enquote{universal loss-avoidance considerations:}

\singlespacing
\blockquote{
If an agent violates the transitivity condition on preferences, then that individual can be \enquote{money pumped:}
all wealth can be taken from her, simply by trading goods with her in a way that exploits her preference intransitivity\textelp{}
Consequently, to the extent that any one wants to avoid such sure losses, one must satisfy the corresponding internal consistency criteria.
}
\doublespacing

\textcite{Cubitt2001} however, methodically decompose the argument that failure to satisfy consistency axioms, in particular transitivity, results necessarily in vulnerability to money pumps.
They develop a detailed methodology for describing decision problems without the need to specify an underlying theory of value that traditionally denotes rewards at nodes in decision trees, and provide several examples of how an agent could have preferences that violate consistency axioms and yet remain invulnerable to money pumps.\footnote{
	See \textcite{Cubitt2001} for a novel methodology on atheoretic representations of decision problems, as well as the examples indicated.
}
\textcite[154]{Cubitt2001} conclude: \enquote{Thus, in relation to what we take to be their original objectives, money pump arguments are a failure.}

The critiques of \textcite{Cubitt2001} show that adherence to EUT is not a necessary condition for invulnerability to a money pump, only a sufficient one.
Indeed, it is possible for an agent to conform to standard consistency axioms of completeness and transitivity and still be economically eliminated.\footnote{
	The poem \enquote{Smart} by Shell \textcite{Silverstein1974} exhibits such an agent.
	A son receives a dollar from his father and gleefully trades it for two quarters, and those two quarters for three dimes, and those three dimes for four nickles, and those four nickels for five pennies.
	It is clear that the son's preferences for these objects are both complete and transitive, satisfying the axioms for consistency.
	Perhaps the son has not been economically eliminated, since the five pennies do still have some value, but should there exist infinitely divisible denominations of hard currency, the son would approach elimination.
	The implied reaction of the father to his son's trades, lost on the son, suggests that such a set of preferences is normatively unacceptable.
}
However, the claim that an agent should not make choices in such a way as to have her stock of assets stripped away from her is still largely seen as a necessary condition for a normative theory.
More generally, we claim that for a theory to be useful in guiding policy it should accord with the intuition that being stripped of her stock of assets renders an agent worse off.\footnote{
	The requirement of \enquote{just} compensation in eminent domain provisions in the United States implies that policy crafters agreed that stripping a citizen of her assets, even for the public good, leaves her worse off.
}
I call this the economic sustainability (ES) criterion.

\textcite[141]{Cubitt2001} define a relation communicating this necessity of not valuing less assets to more, which they state \enquote{coincides with \textit{weak statewise dominance}} and that
\enquote{Many theories of choice under uncertainty generate choice functions which respect statewise dominance and hence, within this setup, objective betterness.
	Obviously, this is true of expected utility theory.
	But it is also true of, for example, \textcite{Quiggin1982} rank-dependent expected utility theory \textins{RDU}.}

\textcite[112]{Marschak1950} seems to endorse this notion of ES as a normative criterion, describing agents who choose dominated offers as worse off:
\enquote{In dealing with his environment (\enquote{nature} which includes \enquote{society}) a man who often makes mistakes in his inferences and his sums is, in the long run, apt to fare less well than if he had been a better logician and arithmetician.}

With this interpretation of ES as a necessary criterion for a normative theory of choice, we return to the SMP thought experiment presented earlier.
In the SMP, at no point does an agent face a single choice that incorporates FOSD.
Thus, taken in isolation, a choice to buy or sell the lottery ticket cannot be said to necessarily reveal anything about changes in the agent's welfare.
Should an agent buy the ticket and then proceed to sell the ticket back for less money, the agent's choices consider as a set lead directly to her stock of assets being reduced by 1 unit.
A normatively acceptable theory of choice must, at a minimum, describe the agent as having become worse off than if she had never engaged in the trades.

This concept of describing objective betterment across aggregated choices is no different to previous uses of the standard money pump argument.
Utilizing the above example of a standard money pump, it isn't until the extractor seeks to trade back a reduced quantity of the agent's original endowment, $A - \epsilon$, for her current stock of assets, $C$, that the agent is said to be made worse off.
Furthermore, all trades prior to the final trade cannot be said to necessarily make the agent worse off, but are simply transfers of different stocks of goods.
With all the trades aggregated together however, the final trade is what completes the extraction that leaves the agent with a smaller stock of assets.

All the stochastic models examined in the SMP thought experiment allow for the choices resulting in an extraction to occur.
All of the examples except for Emma even predict the extraction will occur with the same probability.
Additionally, every model allows for a mechanism to describe the agent's welfare at every choice.
However, it is only the TR and CU models associated with the Beth and Cate examples that satisfy the normative requirement described in this section.

Equations (\ref{eq2:Beth}) and (\ref{eq2:Cate}) show that the CU and TR models allow for the observed extraction to occur, while equation (\ref{eq2:BCwelfare}) states that these agents are worse off than they would have been had they not engaged in the trades.
The TR and CU models would make similar allowances for the selection of dominated lotteries in FOSD lottery pairs, and also correctly align the subjective welfare assessment with the ES criterion.
Even considering alternative RE models that assign a probability of $0$ to dominated lotteries in FOSD lottery pairs, and thus econometrically collapse with every proposed utility function having equal, $0$, likelihood, all proposed utility functions would \textit{still} respect ES.
Thus, it is a general result that stand-alone RE and TR models successfully adhere to the normative criteria described in this section.

The RP, RPPO, and the TR-RP models of Amy, Dana, and Emma however, all share the property that the extraction of 1 unit of wealth from the agents can be described subjectively as an increase in welfare.
For the RP and RPPO models, this description of an extraction as welfare improvement is guaranteed, while for the TR-RP model this description is applicable under some conditions.
Thus, I argue that this property of the RP class of stochastic models violates the normative criteria described in this section.

\singlespacing
\subsection{Willingness to \enquote{Correct} Choices Criterion}
\doublespacing

The willingness to correct choices (WCC) criteria is often interpreted in multiple ways.
A general version of WCC requires that should an agent deviate from the requirements of a theory of choice, and should she then be confronted with the deviation, she will willingly \enquote{correct} her choices to conform with the theory.
Sometimes this argument states that the theory in question is normatively justified if such a willingness to correct choices be observed empirically.
%In some other interpretations the willingness to correct choices need only be hypothetical.

This criteria seems to require multiple moving pieces.
First, an agent must make choices that apparently violate the economic theory in question, such as the kind of choices described in the money pump and SMP examples.
Secondly, someone, often characterized as an \enquote{expert} in decision problems, must confront the agent with the theory and a prescription of how the agent should make choices in light of this theory.
Finally, the agent, presumably having been convinced about the validity of the theory, chooses again and selects a set of choices that conform to the theory.
An alternate version of the criterion only requires that the agent in question be an \enquote{expert} or some kind of exemplary decision maker herself.

Such a confrontation of experts is famously said to have occurred at a conference on decision theory held in Paris in May 1952 between Leonard Savage and Maurice Allais \parencites[1]{Allais1953}[221]{Moscati2016}.
Savage, having presented arguments for EUT during the conference, was asked by Allais to choose a lottery ticket he would prefer to own from two pairs of lottery tickets.
Savage made choices over these two lottery pairs that violated the Independence Axiom of EUT, and was confronted by Allais with proof of the violation.
Allais argued that if even Savage did not make choices in accordance with EUT, it could not be accepted as a normative theory.
Savage replied that he had made a mistake in his choices, and having been confronted with the error was willing to correct them.
He argued that his willingness to change his choices upon having been confronted with his error was evidence in favor of the normative validity of EUT.
In the terms of his private correspondence with Samuleson, he would have nothing to \enquote{reproach} himself for in having changed his choices to be in accordance with EUT \parencite[230]{Moscati2016}.

Most often, agents engaging in economically salient choices are not confronted by other agents directly with suggestions about how their choices could better conform to some normative theory.
Instead, agents are generally only confronted with the way their choices deviate from an economic theory indirectly, through consequences of the operation of market forces.
We can reformulate the WCC criterion with market forces as the confronter of agents and pose the following question:
\enquote{When confronted with salient market outcomes resulting from choices that are discordant with some theory, do agents willingly change their choices to be in accordance with the theory in similar subsequent market interactions?}

\textcite{Chu1990} deliver evidence in favor of EUT responding to exactly this question.
They conduct an experiment replicating the design of \textcite{Grether1979}, which found frequent deviations from EUT, specifically, apparent violations of transitivity.\footnote{
	There is an extensive literature, reviewed in Chapter 1, concerning the \textcite{Grether1979} experiments.
}
\textcite{Chu1990} differ from the previous replications of \textcite{Grether1979} by actively engaging in arbitrage with subjects who committed apparent violations of transitivity, thus experimentally simulating the kind of market forces that would operate outside of the laboratory.
\textcite[910]{Chu1990} find that incidences of apparent violations of transitivity were eliminated from all subjects' choices after an average of $1.71$ arbitrage transactions, and that \enquote{subjects displayed substantially fewer reversals \textins{apparently intransitive choices} after they were exposed to a marketlike environment in previous rounds of games.}
The largest number of arbitrage transactions needed to induce conformity to the transitivity axiom was $3$, and this number of transactions occurred for only one subject across all of their treatments.

Our revised WCC criterion requires that exposure to market forces must induce a \enquote{correction} to choices that do not consistently lead to worse welfare outcomes, such as the arbitrage transactions implemented by \textcite{Chu1990}.
TR and RE models handle this requirement in 2 ways.
First, both classes of models are structured such that the most likely choice in every scenario is one that will leave the agent at least as well off as she currently is.
Thus there is a built-in correcting mechanism in these models.
Secondly, one could model the relevant stochastic parameters, $\phi$ in the case of the TR model and $D(\beta,X)$ or $\lambda$ in the case of RE models, as being determined in part by the number of choice problems (or arbitrage transactions) experienced with the assumption of a negative coefficient.
With this specification, increased market interaction would lead to lower probabilities of mistakes, and in the limit would result in choices conforming with EUT, just as was observed by \textcite{Chu1990}.

However, it isn't clear that RP models provide any guidance as to what should be corrected.
In the stand-alone RP and RPPO models, \textit{all} choices result in optimal welfare, even if aggregated choices result in the strict reduction in the agent's stock of assets.
Even if the parameters of the RP model are themselves determined by the number of choice scenarios experienced in a way that decreases the probability of a deviation from EUT, there is inherently no welfare relevant \enquote{correction} occurring;
agents can be characterized as just as well off or potentially better off by \textit{not} approaching consistency with EUT.
Thus, the RP class of models do not provide normatively useful statements with respect to the WCC criterion.

\singlespacing
\section{Concluding Remarks}
\doublespacing

In this chapter I have shown that stochastic models of economic agents have been given an increasing amount of attention, and have discussed three classes of models at length: the \enquote{Tremble} (TR) model, the \enquote{Random Error} (RE) model, and the \enquote{Random Preference} (RP) models.
The primary purpose for which these models were developed was to account descriptively for the apparent deviations from EUT frequently observed in experimental data.
To further this descriptive purpose, these models were formulated in such a way as to make specific predictions about observed choice probabilities in particular choice scenarios.
The TR model requires that all deviations from EUT are equally likely;
the SU model requires Strong Stochastic Transitivity;
and the RP model requires that violations of FOSD have a zero probability of occurring.

Additionally, these models are formulated mathematically to be parsimonious and modular, meaning that their stochastic elements effectively never interfere with the elements concerned with utility.
The TR model and most of the RE models only require the estimation of one parameter in addition to whatever model of utility is employed, whereas common interpretations of the RP model only require the additional estimation of two parameters.
The modularity of these models also allows researchers such as \textcite{Loomes2002} to combine them to address potential descriptive shortfalls arising from their individual application.

However, the main purpose of this chapter is not to illustrate the descriptive capabilities of these models, but to draw attention to their normative implications and potential justifications.
I propose a simple thought experiment involving a contrived decision problem, the \enquote{Stochastic Money Pump} (SMP), and several hypothetical agents who individually operate as if using differing classes of stochastic models when making choices.
The SMP is structured in such a way as to demonstrate the possibility of an agent entering into a decision problem and then leaving with a strictly lower stock of assets, as in the traditional \enquote{money pump.}
With the SMP in hand, we show that, at least for this decision problem, each of the major classes of stochastic models can be parameterized in such a way that they produce exactly the same descriptive choice probabilities.
This descriptive equality, however, does not in any way imply that the welfare implications of these models are always equivalent or even coherent.

I show that for the examples of the models given, the TR and RE models make equivalent implications concerning the welfare of agents, in particular, that agents who have some of their assets extracted from them are modeled as strictly worse off than if this had not happened.
The same cannot be said about the RP model or any of its derivatives, such as the proposed \enquote{Random Preference Per Option} (RPPO) model or the RP-TR combination model.
The mathematical descriptions of welfare from these models beg the larger question of how or whether the stochastic models can be normatively justified.

I attend to the discussion of normative coherence by first noting the historical and contemporary emphasis on the potential of an economic theory to make statements about welfare that are useful for assessing policy.
I attempt to limit the discussion by focusing on two prominent criteria used in the literature on normative justifications: \enquote{Economic Sustainability} (ES) and the \enquote{Willingness to Correct Choices} (WCC).

I argue that the ES criterion requires adherence to a notion that strictly greater stocks of assets are objectively better than smaller stocks of assets, and normative models must require agents to subjectively conform to this valuation.
I conclude that only the RE and TR models make coherent statements with respect to ES; the SMP thought experiment demonstrates how the RP model allows for the implication that agents who have had assets extracted from them are subjectively better off than if they had not suffered extraction.

I argue that to effectively posit the WCC criterion as a condition for normative coherence, the role of the \enquote{confronter} must be delegated to the market.
In this interpretation, the WCC builds on the notion of \enquote{objective betterness} described by ES by requiring that having been confronted with the market outcomes of a choice, should the agent face the same decision again, the most likely choice she will make will leave her at least as well off as her previous choice.
I conclude that, again, the RE and TR models make coherent statements with respect to this criterion, but the RP models do not.

I find that the motivations underpinning the development of the RP model as a descriptive model of choice probabilities are relatively sound.
There has been some evidence showing the RP model does statistically fit choice data better than many alternative models, particularly when combined with the TR model, though there has been a significant amount of evidence showing that heteroscedastic RE models outperform RP.
The RPPO model presents the possibility of still better statistical fit, particularly since it allows for the violation of FOSD in certain choice scenarios.\footnote{
	The normalized RPPO model doesn't allow an option with a single certain outcome to be chosen over an option with a larger single certain outcome, but the un-normalized RPPO model might, depending on the particular utility functions being utilized and distributions of parameters.
}
Whether the RPPO model does in fact perform statistically better than other stochastic models is an empirical question that hasn't been given much attention.

I argue that the RPPO model shouldn't be given much attention.
The RP and RPPO models both exist in a territory of economic modeling that concerns itself with statistical fit and predictive quality, which are indeed things economists should be concerned about, but cannot be used to make persuasive arguments about how an agent maintains economic welfare through choices.
It is the latter of these two concerns which constitute the economic, as opposed to the technical, content of the inquiry.
The exercise presented in this chapter helps to inform the econometric question proposed by analysts of \enquote{what is the \enquote{best} stochastic model?} by suggesting that the \enquote{best} model is the one which has the greatest \enquote{fit} among the models \textit{that make normatively coherent statements about the welfare of the modeled agents.}

I conclude from this experiment that the RP model's failure to provide coherent statements on how the choice mechanism relates to a useful interpretation welfare should disqualify it as a model for economic agency, regardless of any evidence that suggests it is a better fitting model.
I recognize that rejecting a model that potentially fits choice data from economic experiments better than its alternative seems counter-empiricist, but if estimates from these models are only useful for describing choice probabilities, and not the welfare implications of the choices made, the model is not useful in economic applications.

%\onlyinsubfile{
%\newpage
%\printbibliography[segment=2, heading=subbibliography]
%}

\end{document}
