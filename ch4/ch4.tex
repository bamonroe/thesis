\documentclass[../main.tex]{subfiles}

\begin{document}

\onehalfspacing
\setcounter{chapter}{3}

\chapter{Welfare From Experimental Instruments}

\lltoc % Table of contents only when locally compiled

In Chapter 1 we described the efforts of economists to account for apparent violations of Expected Utility Theory (EUT) in economic experiments.
Some of these efforts were directed at the development of alternative deterministic theories of utility, such as Prospect Theory by \textcite{Kahneman1979}, Rank Dependent Utility (RDU) by \textcite{Quiggin1982}, and Regret Theory by \textcite{Bell1982}, \textcite{Loomes1982}.
Other efforts were focused on the redevelopment of stochastic models, such as the constant error or \enquote{tremble} model by \textcite{Harless1994}, the Strong Utility model by \textcite{Hey1994}, the random preference model by \textcite{Loomes1995}, along with many derivatives of the Strong Utility model.
Much of the newly proposed theoretical explanations of the apparent violations of EUT we tested experimentally.
A well known example is that of \textcite{Hey1994} (HO), who conduct an experiment to test if any of a variety of generalizations (and one restriction) of EUT can explain experimentally collected data significantly better than EUT while utilizing the Strong Utility model.
HO engage in picking "winning" models for each subject on the basis of their estimates for each model and whether each model can be said to be different from EUT.
They conclude that \enquote{our study indicates that behavior can be reasonably well modelled (to what might be termed a \enquote{reasonable approximation}) as \enquote{EU plus noise.}}

However, HO note:
\enquote{The inferences that can be drawn \textelp{} about the adequacy or otherwise of EU are not, however, clear cut - mainly because of the large number of generalizations of EU under consideration.
A this research has evolved, and then number of generalizations under consideration has increased, the number of subjects for whom EU emerges as \enquote{the winners} has declined. 
This is inevitable, though it is not clear how one should judge the rate of decline.
\textelp{} Monte Carlo work would be needed to shed more accurate light on such issues}

The concerns raised by HO can be considered that of statistical power and how much economists should care about potential misidentification, that is, how much should we care about type I and type II errors.
In this chapter, we will analyze the experimental instruments utilized by \textcite{Harrison2016} (HNG) for its ability to accurately recover the utility functions of agents faced with the instruments.
The analysis will focus firstly on the ability of the instrument to correctly classify an agent operating one of four different utility models, as is done in HNG, and secondly on the welfare consequences of this characterization.
Thus we attempt to remove some uncertainty about the power of the instrument, and propse metrics to address the question of how much economists should care about statistical power issues by linking them directly with welfare evaluations.

To begin this analysis, we will describe and replicate the classification and welfare calculation exercises of HNG.
Next we will conduct a simulation analysis on the lottery instrument used in HNG to determine the frequency of misclassification for two of the four models in question, and the welfare consequences thereof.
We next propose two ways to potentially alleviate the welfare concerns of misidentification.

\section{Estimating a Benchmark using \texorpdfstring{\textcite{Harrison2016}}{Harrison and Ng (2015)}}
\label{sec4:Bench}

\textcite{Harrison2016} report the results of an experiment intended to evaluate the welfare consequences of individuals' decisions to purchase insurance.
This is in part a response to the large literature cited by \textcite[92]{Harrison2016} which evaluates insurance on the basis of \enquote{take-up}: the rate at which individuals purchase insurance.
They argue that although a take-up metric is transparent and easy to measure, it doesn't allow for statements about whether an individual \textit{should} have taken up the insurance product, and it does not quantify the consumer surplus from making the correct insurance purchase decision.
These are, however, precisely the kind of normative welfare statements that economists should be making about the economic choices of agents.
They are also the kind of normative welfare statements that can be made from estimating the utility functions of individuals and evaluating their choices with respect to these functions.

\textcite{Harrison2016} address the problem of evaluating the welfare consequences of the decision to purchase insurance or not by conducting a 2-part experiment.
In the first part each subject is presented with a battery of 80 lottery pairs and asked to select one lottery from each pair that will be played out for payment.
This part will be referred to as the \enquote{lottery task} throughout.
The responses of each subject to the lottery task are used to estimate utility functions for that individual.
In the second part each subject is endowed with \money{20} and presented with 24 choices where they are asked to choose between a lottery which will result in a loss of \money{15} with some probability $p$ or no loss of the initial endowment with probability $(1-p)$, and a certain amount of money between \money{15.20} and \money{19.80}.
The choice of the certain amount of money is framed as the purchase of insurance against the risk of loss in the lottery option.
This part will be referred to as the \enquote{insurance task} throughout.
Both of these instruments are detailed in full in the Appendix (to be added).

For each individual, \textcite{Harrison2016} use the data recovered in the lottery task to estimate four models which can be described in the framework of Rank Dependent Utility (RDU) as first proposed by \textcite{Quiggin1982}:
\begin{equation}
	\label{eq4:RDU}
	RDU = \sum_{i=1}^{I} \left[ w_i(p) \times u(x_i) \right]
\end{equation}
\noindent where $i$ indexes the outcomes, $x_i$, from $\{1,\ldots,I\}$ with $i=1$ being the smallest outcome in the lottery and $i=I$ being the greatest outcome in the lottery, $u(\cdot)$ is a standard utility function, $w_i(\cdot)$ decision weight function applied to outcome $i$ given the distribution of probabilities in the lottery ranked by outcome, $p$.
The decision weight function, $w_i(\cdot)$, takes the form:
\begin{equation}
	\label{eq4:dweight}
	w_i(p) =
	\begin{cases}
		\omega\left(\displaystyle\sum_{j=i}^I p_j\right) - \omega\left(\displaystyle\sum_{k=i+1}^I p_k\right) & \text{for } i<I \\
		\omega(p_i) & \text{for } i = I
	\end{cases}
\end{equation}
\noindent where the probability weighting function, $\omega(\cdot)$, can take a variety of parametric or non-parametric forms.
HNG estimate four probability weighting functions (pwf).
The first pwf is the linear function:
\begin{equation}
	\label{eq4:pw:eut}
	\omega(p_i) = p_i
\end{equation}

\noindent The second pwf is the power function ($\mathit{RDU_{Pow}}$) used by \textcite{Quiggin1982}:
\begin{equation}
	\label{eq4:pw:pow}
	\omega(p_i)=p_i^\gamma
\end{equation}

\noindent where $\gamma > 0$. 
The third pwf is the \enquote{Inverse-S} shaped function ($\mathit{RDU_{Invs}}$) popularized by \textcite{Tversky1992}:
\begin{equation}
	\label{eq4:pw:inv}
	\omega(p_i) = \frac{p_i^\gamma}{\biggl(p_i^\gamma + {(1-p_i)}^\gamma\biggr)^{ \frac{1}{\gamma} } }
\end{equation}

\noindent where $\gamma > 0$. 
The fourth pwf is the flexible function proposed by \textcite{Prelec1998} ($\mathit{RDU_{Prelec}}$):
\begin{equation}
	\label{eq4:pw:pre}
	\omega(p_i)=\exp(-\beta(-\ln(p_i))^\alpha)
\end{equation}
\noindent where $\alpha > 0$ and $\beta > 0$.

The functional form of an RDU model with the linear probability weighting function in equation (\ref{eq4:pw:eut}) is equivalent to Expected Utility Theory (EUT), and will be referred to as EUT throughout.
In all the remaining probability weighting functions, there exist values for the probability weighting parameters which allow $w_i(p) = p_i$, the special case of EUT.
For all four models, \textcite{Harrison2016} use the CRRA utility function:
\begin{equation}
	\label{eq4:CRRA}
	u(x) = \frac{x^{(1-r)}}{(1-r)}
\end{equation}
\noindent where $r$ is the coefficient of relative risk aversion proposed by \textcite{Pratt1964}.

I continue to use the notation used in chapters 2 and 3 to describe a choice scenario by a subject, but limit it to a binary choice between two options, $j$ and $k$.
In this framework a choice of option $j$ in task $t$ is indicated by the function $y_t = j$, where $y_t = 1 \geq^n y_t = 2$.
The values of $j$ and $k$ do not indicate the order or frame with which the options in task $t$ were presented to the subject, but rather the ordinal rank the subject's utility function assigns to the options, with 1 always being the option of greatest utility.
This notation is useful when describing the welfare consequences of choices below.

\textcite{Harrison2016} also use Contextual Utility (CU), as defined by \textcite{Wilcox2008}, as the stochastic model.
Thus for the models utilized, the probability that option $j$ is chosen is given by:
\begin{align}
	\label{eq4:RE.2}
	\begin{split}
		{\Prob}(y_t = j) &= {\Prob}\left(  \epsilon_t \geq \frac{1}{\lambda_n} \left[ G(\beta_n,X_{kt}) - G(\beta_n,X_{jt}) \right] \right)\\
		&= 1 - F\left( \dfrac{G(\beta_n,X_{kt}) - G(\beta_n,X_{jt})}{D(\beta_n,X_t)\lambda_n }  \right)
	\end{split}
\end{align}

\noindent where $\epsilon_t$ is a mean 0 error term, $F$ is a symmetric cumulative distribution function (cdf), meaning $1 - F(x)  = F(-x)$, $G(\cdot)$ is the RDU utility model that takes the parameters $\beta_n$ to calculate the utility of lottery $j$ or $k$ in task $t$ comprised of outcomes and probabilities $X_{jt}$, and $\lambda_n$ is a precision parameter.
The function $D(\cdot)$ separates contextual utility from a Strong Utility model:
\begin{align}
	\label{eq4:W.cu}
	\begin{split}
		&D(\beta_n,X_t) = \mathit{max}[u(x_{it})] - \mathit{min}[u(x_{it})]\\
		&\mathit{st.}\; w_i(x_{it}) \neq 0
	\end{split}
\end{align}

Usually, the Normal or Logistic cdf is chosen for $F$, and I employ the Logistic cdf for all calculations throughout.
Given that each choice considered here only involves two lottery options, we can define the probability of choosing option $j$ given a particular model, parameter set $\beta_n$, precision parameter $\lambda_n$, and outcomes and probabilities of option $j$, $X_{jt}$, as
\begin{equation}
	\label{eq4:RE.f}
	{\Prob}(y_t=j) =\dfrac{\exp\!\left( \dfrac{ G(\beta_n,X_{jt}) }{ D(\beta_n,X_{t})\lambda_n }  \right)}{  \exp\!\left( \dfrac{ G(\beta_n,X_{jt}) }{ D(\beta_n,X_{t})\lambda_n }  \right) + \exp\!\left( \dfrac{ G(\beta_n,X_{kt}) }{ D(\beta_n,X_{t})\lambda_n }  \right)    }
\end{equation}

\noindent These choice probabilities in turn are logged and summed to produce a log-likelihood function for each of the four different models:
\begin{equation}
	\label{eq4:ll}
	\ensuremath{\mathit{LL_n}} = \sum_{t}^T \ln \left[ {\Prob}(y_t) \right]
\end{equation}

As a metric of welfare, \textcite{Harrison2016} primarily use the consumer surplus (CS) of each choice.
The CS of each choice is defined as the difference between the certainty equivalent ({\CE}) of the chosen option and the certainty equivalent of the unchosen option.
Since the CRRA utility function defined in equation (\ref{eq4:CRRA}) is used for all models discussed, we can define the {\CE} as:

\begin{align}
	\label{eq4:CEcalc}
	\begin{split}
		&\sum_{i=1}^{I} w_i(p) \frac{x_{ij}^{(1-r)}}{(1-r)} = \frac{ {\CE}_j^{(1-r)}}{(1-r)}\\
		&{\CE}_j =  \left( (1-r) \times \sum_{i=1}^{I} w_i(p) \frac{x_{ij}^{1-r}}{(1-r)} \right)^{ \displaystyle\nicefrac{1}{(1-r)} } ,
	\end{split}
\end{align}

\noindent and the welfare surplus metric derived from this {\CE} for any choice as:

\begin{equation}
	\label{eq4:wsurplus}
	\Delta W_{nt} =  {\CE}_{nyt} - {\CE}_{n1t}^Z ,
\end{equation}

\noindent and the accumulated welfare surplus as:

\begin{equation}
	\label{eq4:wsurplusT}
	\Delta W_{nT} = \sum_{t=1}^T \left( {\CE}_{nyt} - {\CE}_{n1t}^Z \right)
\end{equation}

\noindent where the $y$ subscript indicates the option chosen (either 1 or 2 in the binary scenario we consider here), and the $Z$ superscript indicates the remaining, unchosen options, of which the {\CE} with the greatest value, designated by the subscript 1, is considered the foregone opportunity.

\textcite[106]{Harrison2016} consider an additional metric of forgone welfare surplus as the difference between the maximal {\CE} for every choice and the {\CE} of the option actually chosen by the subject
\begin{equation}
	\label{eq4:wforgone}
	\Delta F_{nt} = -1 \times \left( {\CE}_{n1t} - {\CE}_{nyt} \right) , 
\end{equation}

\noindent and the accumulated forgone welfare surplus

\begin{equation}
	\label{eq4:wforgoneT}
	\Delta F_{nT} = \sum_{t=1}^T  -1 \times \left( {\CE}_{n1t} - {\CE}_{nyt} \right)
\end{equation}

\noindent With these metrics, the best possible value for any subject is 0, which would indicate that all choices made were optimal, whereas any positive value indicates the amount of welfare surplus forgone by the subject due to choice errors.
These metrics line up easily with the metrics defined in equations (\ref{eq4:wsurplus}) and (\ref{eq4:wsurplusT}), as should $y_t \neq 1$, ${\CE}_{n1t}^Z = {\CE}_{n1t}$.

\textcite{Harrison2016} estimate values of the CRRA utility parameter, $r$, the probability weighting parameters, $\gamma, \alpha, \beta$, and the stochastic parameter $\lambda$, for each of the models presented above via maximum likelihood estimation (MLE) using the choices made by the subjects in the lottery task.
\textcite[107,110]{Harrison2016} initially calculate the welfare consequences of the choices made by each subject by using only the point estimates from the MLE, and then employ a bootstrap method which incorporates the covariance matrix of the standard errors.

For the bootstrap method, a multivariate normal distribution of parameter sets is bootstrapped from the estimates using the point estimates of these parameters as the means of the marginal distributions, and the covariance matrix of standard errors used as the covariance matrix of standard deviations.
For each subject's parameter estimates 500 draws of parameter sets were taken, the welfare metrics calculated for each set of parameters, and then the values of the metrics averaged across the 500 draws.
Since the covariance matrix used in the bootstrap method draws parameters from the joint distribution with respect to their density in the joint distribution, only a simple average is needed.

The experimental subjects consisted of 111 undergraduate students enrolled in several different colleges at Georgia State University, USA.
All experimental sessions were conducted in 2014 at the ExCEN experimental lab of Georgia State University.
Every subject received, and expected to receive, a guaranteed \money{5} show up fee, but no specific information about the experiment or expected earnings was communicated to the subjects before the experiment \parencite[98]{Harrison2016}.
The full set of instructions delivered to the subjects is available in Appendix C of \textcite{Harrison2016}.

\subsection{Individual Level Estimation}
\label{sec4:ILE}

HNG employ a multi-step process for picking a \enquote{winning} model for each subject.
First, all four models models cited in equations (\ref{eq4:pw:eut}), (\ref{eq4:pw:pow}), (\ref{eq4:pw:inv}), (\ref{eq4:pw:pre}) are estimated for each subject.
Next, data are dropped from analysis on the basis of an \textit{ex ante} defined set of \enquote{exclusionary rules} applied to every model estimated on the subjects.
Finally, a \enquote{classification process} is employed to choose a model to categorize the subject as.
HNG propose 4 exclusionary rules:
\begin{itemize}
	\item Any estimate for which the optimizer did not return a convergence code indicating both a gradient near 0 and a negative definite Hessian.
	\item Any model with a CRRA coefficient estimated to be greater than 15 or less than -15.
	\item $\mathit{RDU_{Pow}}$ and $\mathit{RDU_{Invs}}$ models where the $\gamma$ parameter was estimated to be greater than 5.
	\item Any model with a CRRA coefficient estimated to be greater than .99 and less than 1.01.
\end{itemize}

\noindent The gradient and Hessian conditions indicate that the estimates are at a local maximum of a concave portion of the likelihood function.
The next 2 rules indicate parameter values that although mathematically possible for the given functionals, nonetheless are considered to be extreme to the point of not being reliable.
\textcite{Wakker2008} details how the CRRA utility function has certain asymptotic properties around 1.
These properties may create numerical issues for the optimizer and so estimated values very near 1 are viewed as less credible and are excluded from the analysis.

The classification process proposed by HNG applies to all the remaining, non-excluded data.
The log-likelihood function given in equation (\ref{eq4:ll}) is equally applicable to all four models considered by HNG, and seems a natural metric to declare a \enquote{winning} model among the 4 alternatives proposed.
However, since RDU models nest EUT as a special case (noted in equation \ref{eq4:pw:eut}), \textit{a priori} we would expect RDU models to produce greater log-likelihoods than an EUT model on any given dataset.
\textcite[102]{Harrison2016} note this issue and propose the additional qualification on RDU models that the probability weighting function implied by the estimated model must be statistically significantly different from a linear function, the special case of EUT, at the 10, 5, or 1 percent significance levels.

The null hypothesis for the $\mathit{RDU_{Pow}}$ and $\mathit{RDU_{Invs}}$ models is $H_0: \gamma = 1$, and the null for the $\mathit{RDU_{Prelec}}$ model is $H_0: \alpha = \beta = 1$.
Non-linear Wald tests are used to test these hypotheses.
Any RDU model that fails to reject the null hypothesis is removed from consideration as a \enquote{winning} model.
If the EUT model did not converge for the subject in question, the models considered will only consist of the RDU models which tested as different to EUT.
If the EUT model did not converge \textit{and} no RDU model tested as different to EUT, then all of the converged RDU models will be considered.
The \enquote{winning} model for each subject is chosen from among the models which have met criteria derived from the Wald test.
The winning model is then used to calculate the welfare consequences of the subject's choices on the insurance task.

When I utilize the same classification processes employed by \textcite{Harrison2016} on their data, we see a somewhat different distribution of subjects classified to the 4 models in Figure \ref{fig:HNG_pvals}.
These differences are relatively minor, showing somewhat more RDU subjects and less EUT subjects than reported by HNG.
Further details on differences are given in Appendix B (to be added).
I do however, replicate in Figure \ref{fig:HNG_CS} almost exactly the distribution of per-choice consumer surplus presented in Figure 10 of \textcite[108]{Harrison2016}.
This suggests a degree of leeway in the classification process in terms of its effect on the characterization of subjects' welfare.

\begin{figure}[h!]
	\center
	\caption{Classifying Subjects as EUT or RDU}
	%\caption{Estimates for each individual of EUT and RDU specifications \textcite[108]{Harrison2016} Data}
	\onlyinsubfile{
		\includegraphics[height=.25\paperheight]{figures/real/HNG_pvals.pdf}
	}
	\notinsubfile{
		\includegraphics[height=.25\paperheight]{ch4/figures/real/HNG_pvals.pdf}
	}
	\label{fig:HNG_pvals}
\end{figure}

\begin{figure}[h!]
	\center
	\caption{Distribution of Consumer Surplus, Using Data from \textcite{Harrison2016}}
	\onlyinsubfile{
		\includegraphics[height=.25\paperheight]{figures/real/HNG_CS.pdf}
	}
	\notinsubfile{
		\includegraphics[height=.25\paperheight]{ch4/figures/real/HNG_CS.pdf}
	}
	\label{fig:HNG_CS}
\end{figure}

\section{Individual Classification and Welfare Estimation Accuracy}
\label{sec4:IC}

Whether the results presented in Figure \ref{fig:HNG_pvals} demonstrate an accurate estimation of the proportions of subjects belonging to those models depends on our confidence in the classification process to correctly classify a subject as one of these four models, as well as our confidence that the subjects in the experiments actually belong to one of the four models we test for.
Our confidence that the classification process can correctly classify a subject in turn depends on the nature of the experimental instrument presented to the subject.

The degree of confidence of the classification process, and indeed most statistical tests in the economics literature, can be assessed through power analysis.
However, power analyses are rarely conducted in parallel with econometric estimations.
\textcite{McCloskey1996} find that only 4.4\% of the 181 papers published in \textit{The American Economic Review} considered the power of the test they were performing.
\textcite[6]{Zhang2013} review all papers published in the journal \textit{Experimental Economics} for the years 2010-2012, and find that no paper stated the optimal sample size for their analyses, and only one paper mentions power as an issue.

There are some examples of experimental economists utilizing power calculations to inform their analysis or experimental designs.
\textcite{Rutstrom2009} conduct a power analysis by simulating agent behavior in a matching pennies games and choosing payoffs that would result in the best chance of identifying the effect they sought to identify if it were there.
In this instance, \textcite{Rutstrom2009} conduct a power analysis in order to influence the design of their experiment.
\textcite[8]{Wilcox2015} conduct Monte Carlo simulations of agents responding to a lottery battery all of which operate the CRRA utility function, the $\mathit{RDU_{Prelec}}$ probability weighting function, and the CU stochastic model.
\textcite{Wilcox2015} designates four data generating processes (DGP) by specifying four parametrizations of these models and uses them to generate choice data, with each DGP making choices on the instrument 1000 times.
They then estimate non-parametric RDU models for each of the 1000 choice realizations per DGP and classify the resulting estimates into one of 5 categories, one for each of the DGP and an additional \enquote{unclassified} category.
This is an example of using power analysis to lend support to the researchers methods and conclusions.
%In this case, whether or not subjects actually operate an $\mathit{RDU_{Invs}}$-like probability weighting function.
Both of these kinds of analysis are useful for understanding the statistical support for experimental research as well as its limitations.

In this section, we interrogate the statistical power of the instrument and classification process to correctly classify subjects given the instrument presented in \textcite{Harrison2016}, and in turn how accurate the welfare calculations are given a classification.
I conduct this analysis via simulation methods similar to those defined by \textcite{Feiveson2002} and which resemble an extension of the Monte Carlo analysis performed by \textcite{Wilcox2015}.
\textcite[108]{Feiveson2002} briefly describes a simulation method for determining the power of an experiment:

\blockquote{\textins{W}e contemplate a hypothetical scenario in which the identically sized experiment could be run over and over, each time collecting new data and doing a new hypothesis test. 
If this scenario can be adequately modeled, we may thus estimate power by simulating data from multiple replications of the experiment and simply calculate the proportion of rejections \textins{of the null hypothesis} as an estimate of the power.}

\textcite[109]{Feiveson2002} outline this method in more detail, and conclude the process by noting \enquote{The estimated power for a \textins{specified significance level} test is simply the proportion of observations (out of \textins{some large number of replications}) for which the p-value is less than \textins{the specified significance level}.}
In this framework, and given the nature of the classification process defined previously, should an EUT subject be classified as operating an $\mathit{RDU_{Prelec}}$ model, this would constitute a type I error (a \enquote{false positive} of probability weighting), and should an $\mathit{RDU_{Prelec}}$ subject be classified as operating an EUT model this would constitute a type II error (a \enquote{false negative} of no probability weighting).
The probability of a type II error is called the \enquote{power} of the test and when researchers engage in \textit{ex ante} power analysis, they typically aim for a power of 80\% \parencite{Cohen1988, Gelman2014a}, and significance level (\enquote{p-value}) of either 1, 5, or 10\%.

I simulate subjects conforming to the EUT and $\mathit{RDU_{Prelec}}$ models, have these simulated subjects respond to both the lottery and insurance task, estimate the subjects' parameter sets given their responses to the lottery task, classify each subject based on the classification process employed by HNG described in the previous section, and calculate the welfare surplus for each subject based on the winning model.{\footnotemark}
A simulated subject is represented by a single parameter set and an assigned operating model.
For each model, we employ the CRRA utility function defined in (\ref{eq4:CRRA}) and the CU stochastic model defined in equations (\ref{eq4:RE.2}) and (\ref{eq4:W.cu}).
For EUT subjects, the parameter set consists of $\lbrace r, \lambda \rbrace$, and for $\mathit{RDU_{Prelec}}$ subjects $\lbrace r, \alpha, \beta, \lambda \rbrace$.
The $r$ parameter in every set is the CRRA parameter from equation (\ref{eq4:CRRA}) and $\lambda$ is the precision parameter defined in equation (\ref{eq4:RE.2}).
The remaining $\alpha$, and $\beta$ parameters relate to the probability weighting parameters of the $\mathit{RDU_{Prelec}}$ model defined in equation (\ref{eq4:pw:pre}).

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	I restrict the analysis and discussion in this chapter to only EUT and $\mathit{RDU_{Prelec}}$ subjects and estimated models to improve the clarity of the discussion.
	However, choice data exist for $\mathit{RDU_{Pow}}$ and $\mathit{RDU_{Invs}}$ subjects and $\mathit{RDU_{Pow}}$ and $\mathit{RDU_{Invs}}$ model estimations exist for all subject models.
}

For each model, we draw parameter sets from a joint uniform distribution over the parameters needed for that model, where the marginal distributions are uncorrelated.{\footnotemark}
For both models, the marginal distribution for $r$ is where $r \in [-1, 0.95]$ and  for $\lambda$ is $\lambda \in [0.01, 0.30]$.
For the $\mathit{RDU_{Prelec}}$ model the marginal distribution for $\alpha$ and $\beta$ is where $\alpha \in [0.10, 2]$ and $\beta \in [0.10, 2]$.

We draw 250k parameter sets for each model for a total of 500k simulated subjects.
The number of draws from these joint distributions was chosen in an attempt to fill as much of the relevant parameter space as possible.{\footnotemark}
Each simulated subject uses the parameter set and operating model assigned to it to calculate the choice probabilities for each option in each lottery pair of the lottery task and the insurance task.
A random number is drawn from a univariate uniform distribution, and if the choice probability calculated for the $A$ option was greater than the random number, the subject chooses A, otherwise they choose B.
This process ensures that subjects' choices are made probabilistically with respect to the subjects' model and parameter set.{\footnotemark}

\addtocounter{footnote}{-3}
\stepcounter{footnote}\footnotetext{
	To create uncorrelated joint uniform distributions, uncorrelated normal distributions were generated using a Gaussian copula process.
	The inverse normal cumulative distribution function was then applied to each marginal distribution to get uncorrelated uniformly distributed variables in the $[0,1]$ space.
	These uniformly distributed variables were then stretched and shifted to fit the uniform spaces described here while retaining the 0 correlation coefficient.
	This process was employed to ensure that the (admittedly low) probability of accidental correlation that might occur from simply drawing from a uniform distribution directly was minimized.
}
\stepcounter{footnote}\footnotetext{
	A limitation of choosing the same number of draws for each model is that the square uniform space for the EUT model will have smaller gaps than the cubic space of the $\mathit{RDU_{Pow}}$ and $\mathit{RDU_{Invs}}$ models, which in turn have smaller gaps than the hypercubic space of the $\mathit{RDU_{Prelec}}$ model.
	The smaller the gaps between parameter sets in their joint space, the better the prediction accuracy of classifying subjects for the parameter sets that exist in the empty space.
}
\stepcounter{footnote}\footnotetext{
	Consider a choice probability for option $A$ calculated to be $0.90$, and therefore the choice probability for option $B$ is $0.10$.
	A random number drawn from a univariate uniform distribution has a 90\% chance of being below or equal to $0.90$, so option A would be chosen 90\% of the time by the simulated subject.
}

After the subjects have made choices, each of the models we consider is estimated for each subject on the choices made in the lottery task.
Any model which didn't converge with a gradient close to 0 and a negative definite Hessian matrix or converged on parameters outside of exclusionary rules defined in the previous section was also dropped from consideration.
Each subject was then classified based on the classification process defined in the previous section using a 5\% significance level.
If no model met the consideration criteria, the subject was classified \enquote{NA}.
The welfare surplus of the choices made on the insurance task are then calculated using the parameters of the winning model.

This process of classification simulation differs from that employed by \textcite{Wilcox2015} in that I simulate a total of 500k DGP, each producing a single set of choice data, whereas \textcite{Wilcox2015} simulate 4 GDP, each producing 1000 sets of choice data.
The approach of \textcite{Wilcox2015} allows for individual DGP to not be characterized by a single set of choices, while the approach I employ allows us to see how the power of the instrument changes with resect to a wide range of DGP.
The limitation of the approach I employ of only generating one choice data set per DGP I believe is mitigated by the large number of simulated subjects employed.
Consider an EUT subject with a CRRA parameter of 0.5 and a $\lambda$ value of 0.1.
There is only one choice dataset for this particular subject, but there are approximately 430 subjects, and thus 430 more choice datasets, in the range CRRA $\in (0.475,0.525)$, and $\lambda \in (0.09, 0.11)$.
If we can consider the choice probabilities for subjects in this range of parameters to be similar, we can have relatively accurate power estimations for subjects in this range.

\subsection{\texorpdfstring{\textcite{Harrison2016}}{Harrison and Ng (2016)} Classification Power}

With this assumption of similarity, we fit generalized additive models (GAM) \parencite{Hastie1986} to the classification data to make predictions of classification likelihoods.
First we subset the data based on the models the simulated subjects actually operate, either EUT or $\mathit{RDU_{Prelec}}$.
For each pooled group we fit a GAM model predicting whether EUT or $\mathit{RDU_{Prelec}}$ was the winning model, or if no model was declared a winner (all considered models failed the exclusionary rules).
\begin{align}
	\label{eq4:GAM}
	\begin{split}
		(winner = N | A = EUT)                   &= s(r) + s(\lambda)\\
		%(winner = N | A = \mathit{RDU_{Prelec}}) &= s(r) + s(\alpha) + s(\beta) +s(\alpha, \beta) + s(r, \alpha, \beta) + s(\lambda)
		(winner = N | A = \mathit{RDU_{Prelec}}) &= s(r) + s(\alpha) + s(\beta) + s(\lambda)
	\end{split}
\end{align}

\noindent where $N$ is one of EUT, $\mathit{RDU_{Prelec}}$, or \enquote{NA}. and $s(\cdot)$ indicates some smooth, potentially non-linear function of its arguments.

The dependent variable in each of the GAM models in equation (\ref{eq4:GAM}) is either 1 if the subject was classified as model $N$, or 0 if the subject was not.
The independent variables in each model are smooth functions of the actual parameter values the subjects operates.
For every model, each parameter gets its own smooth function.
Thus, 3 fitted GAM models for each of the two model types in the population, resulting in 6 fitted models in total.
I then repeat this process but drop subjects that were classified as \enquote{NA} from the data before fitting the models.
This results in 4 additional models.
Given the fitted models and a parameter set for a model type, we can use a fitted GAM model to predict the probability that a subject with the given parameter set will be classified as any of the $N$ models.
The results of this fitting process are presented in Figures \ref{fig:HNG1_win_mu}, \ref{fig:HNG1_win_eut}, and \ref{fig:HNG1_win_pre}.

\begin{figure}[hp!]
	\center
	\caption{Probability of \enquote{Winning} for Given $\lambda$ Values}
	\onlyinsubfile{
		\includegraphics[width=\textwidth]{figures/HNG_1/win_05-all-win-HNG_1.pdf}
		%\includegraphics[height=.5\paperheight, width=\textwidth]{figures/mu-winners-HNG_1.pdf}
	}
	\notinsubfile{
		\includegraphics[width=\textwidth]{ch4/figures/HNG_1/win_05-all-win-HNG_1.pdf}
	}
	\label{fig:HNG1_win_mu}
\end{figure}

\begin{figure}[hp!]
	\center
	\caption{Probability of \enquote{Winning} for EUT subjects}
	\onlyinsubfile{
		\includegraphics[width=\textwidth]{figures/HNG_1/win_05-EUT-win-HNG_1.pdf}
		%\includegraphics[height=.5\paperheight, width=\textwidth]{figures/mu-winners-HNG_1.pdf}
	}
	\notinsubfile{
		\includegraphics[width=\textwidth]{ch4/figures/HNG_1/win_05-EUT-win-HNG_1.pdf}
	}
	\label{fig:HNG1_win_eut}
\end{figure}

\begin{figure}[hp!]
	\center
	\caption{Probability of \enquote{Winning} for $\mathit{RDU_{Prelec}}$ subjects}
	\onlyinsubfile{
		\includegraphics[width=\textwidth]{figures/HNG_1/win_05-PRE-win-HNG_1.pdf}
		%\includegraphics[height=.5\paperheight, width=\textwidth]{figures/mu-winners-HNG_1.pdf}
	}
	\notinsubfile{
		\includegraphics[width=\textwidth]{ch4/figures/HNG_1/win_05-PRE-win-HNG_1.pdf}
	}
	\label{fig:HNG1_win_pre}
\end{figure}

In Figures \ref{fig:HNG1_win_mu}, \ref{fig:HNG1_win_eut}, and \ref{fig:HNG1_win_pre} the X-axis is the simulated subjects' values of the parameter for that plot, and the Y-axis is the probability that a given model was declared the winner.
In each Figure the solid red line indicates the estimates for the EUT model, the dotted green line indicates the estimates for the $\mathit{RDU_{Prelec}}$ model, and the short dashed blue line indicates the estimates for non-convergence or excluded.
In all figures, the 95\% confidence interval is given by the long dashed lines surrounding the lines given above.
In each Figure, the second row contains estimates derived from all the subjects, while the first row only contains estimates derived from subjects that were classified as either EUT or $\mathit{RDU_{Prelec}}$.

In Figure \ref{fig:HNG1_win_mu}, the first column contains estimates for EUT subjects, while the second column contains estimates for $\mathit{RDU_{Prelec}}$ subjects.
The X-axis of this figure is the value of the $\lambda$ parameter.
Thus, the top-left plot shows how the probability of an EUT subject with a converged model is classified as either EUT or $\mathit{RDU_{Prelec}}$ for a range of $\lambda$ values.
In Figures \ref{fig:HNG1_win_eut} and \ref{fig:HNG1_win_pre}, the columns indicate the parameter given on the X-axis.
Thus, in Figure \ref{fig:HNG1_win_eut}, the top-left plot shows how the probability of an EUT subject with a converged model is classified as either EUT or $\mathit{RDU_{Prelec}}$ for a range of CRRA values.
In Figure \ref{fig:HNG1_win_pre}, the bottom-right plot shows how the probability of an $\mathit{RDU_{Prelec}}$ subject is classified as either EUT, $\mathit{RDU_{Prelec}}$, or \enquote{NA} for a range of $\beta$ values.

In Figure \ref{fig:HNG1_win_eut} and in the left column of Figure \ref{fig:HNG1_win_mu}, the red solid line shows the probability of EUT subjects being correctly classified as EUT.
In Figure \ref{fig:HNG1_win_pre} and in the right column of Figure \ref{fig:HNG1_win_mu}, the green dotted line shows the probability of $\mathit{RDU_{Prelec}}$ subjects being correctly classified as $\mathit{RDU_{Prelec}}$.

The results presented in Figures \ref{fig:HNG1_win_mu}, \ref{fig:HNG1_win_eut} and \ref{fig:HNG1_win_pre} offer both surprising and intuitive results.
In Figure \ref{fig:HNG1_win_mu} we see that the likelihood EUT subjects being misclassified as $\mathit{RDU_{Prelec}}$, $\mathit{RDU_{Prelec}}$ subjects being misclassified as EUT, and either type of subject failing to produce any estimates that pass the exclusionary criteria, increases with $\lambda$.
This is very intuitive. 
As the $\lambda$ parameter increases, the likelihood that a subject makes a choice error increases.
For EUT subjects, these choices errors can present as probability weighting when there is none, and for $\mathit{RDU_{Prelec}}$ subjects these choice errors can present as linear probability weighting.
Another way to characterize this effect is to say that as $\lambda$ increases the noise in the data increases, indeed as $\lambda \to \infty$ choice probabilities for every option are equal, resulting in totally random data.
The more noise there is in the data, the lower the likelihood of the optimizer converging on reasonable, or any, estimates, and the greater the likelihood that any latent process will be identified as another.

In in the third and fourth columns of Figure \ref{fig:HNG1_win_pre} we have additionally intuitive results.
We can see in these columns that the probability of an $\mathit{RDU_{Prelec}}$ subject being classified as EUT peaks when the probability weighting parameters approach the value of 1, and diminishes as the parameter values move away from 1.
Since the $\mathit{RDU_{Prelec}}$ model nests EUT when $\alpha = \beta = 1$, we should expect the likelihood of misclassification to increase around these values.
It appears the $\alpha$ parameter plays a more decisive role in the classification probability for the range of parameter values we consider; the probability of a $\mathit{RDU_{Prelec}}$ subject being classified as $\mathit{RDU_{Prelec}}$ drops at a greater rate as the $\alpha$ parameter approaches 1 than as the $\beta$ parameter approaches 1 from either the left or the right.

Also somewhat intuitive, in Figure \ref{fig:HNG1_win_eut} we see that the probability of an EUT subject being classified as EUT is greater for values of CRRA $> 0$ than for values of CRRA $< 0$, though only modestly so.
Values of CRRA $> 0$ indicate risk aversion in an EUT model, and the design of the HNG lottery instrument placed more emphasis on identifying degrees of risk aversion than identifying degrees of risk seeking (CRRA values $ < 0$) in EUT subjects.
Similarly, in Figure \ref{fig:HNG1_win_pre} we see that the CRRA parameter has very little effect on the probability of a $\mathit{RDU_{Prelec}}$ subject being classified as $\mathit{RDU_{Prelec}}$.
Since it is the probability weighing function that defines a $\mathit{RDU_{Prelec}}$ as being different from EUT, it should not be surprising that the utility parameter has little effect on on the probability of $\mathit{RDU_{Prelec}}$ subjects being correctly classified.

However, the relatively low probability with which $\mathit{RDU_{Prelec}}$ subjects are classified as $\mathit{RDU_{Prelec}}$ over a wide range of parameters is surprising.
Looking at Figure \ref{fig:HNG1_win_pre} we see that for most of the parameter values considered, the probability of an $\mathit{RDU_{Prelec}}$ subject being classified as $\mathit{RDU_{Prelec}}$ is below 50\% and that for most of these values, it is more likely that an $\mathit{RDU_{Prelec}}$ subject is classified as EUT than as $\mathit{RDU_{Prelec}}$.


%Recall, however, that the parameters for our simulated subjects were distributed uniformly over the parameter space of interest.
%This means that for the simulated subjects with $\alpha$ values between $(0.95, 1.05)$, 95\% of them will have $\beta$ values outside the range $(0.95, 1.05)$, and for for the simulated subjects with $\alpha$ values between $(0.9, 1.1)$, 90\% of them will have $\beta$ values outside the range $(0.9, 1.1)$,
%Thus, the vast majority of simulated subjects that are misclassified as EUT when one probability weighting parameter equals 1 still have some degree of probability weighting driven by the other parameter.


%\subsubsection{Case 2}
%
%In this case, rather than plot the results across a range of values, we can instead tabulate each element of equation (\ref{eq4:bayes}) assuming a 5\% significance level for the classification process.
%
%\onlyinsubfile{
%\begin{table}[ht!]
%	\centering
%	\captionsetup{justification=centering}
%	\caption{$P(A)$}
%	\label{tb:HNG1_PA}
%	\begin{adjustbox}{}
%	\pgfplotstabletypeset[
%		col sep=comma,
%		every head row/.style={
%			after row=\hline
%		},
%		display columns/0/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {EUT}
%		},
%		display columns/1/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Power}
%		},
%		display columns/2/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Inverse-S}
%		},
%		display columns/3/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Prelec}
%		},
%	]{tables/HNG_1-PA.csv} % path/to/file
%	\end{adjustbox}
%\end{table}
%%\bigskip
%\begin{table}[ht!]
%	\centering
%	\captionsetup{justification=centering}
%	\caption{$P(B)$}
%	\label{tb:HNG1_PB}
%	\begin{adjustbox}{}
%	\pgfplotstabletypeset[
%		col sep=comma,
%		every head row/.style={
%			after row=\hline
%		},
%		display columns/0/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {EUT}
%		},
%		display columns/1/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Power}
%		},
%		display columns/2/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Inverse-S}
%		},
%		display columns/3/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Prelec}
%		},
%	]{tables/HNG_1-PB.csv} % path/to/file
%	\end{adjustbox}
%\end{table}
%%\bigskip
%\begin{table}[ht!]
%	\centering
%	\captionsetup{justification=centering}
%	\caption{$P(B | A)$}
%	\label{tb:HNG1_PBA}
%	\begin{adjustbox}{}
%	\pgfplotstabletypeset[
%		col sep=comma,
%		every head row/.style={
%			before row={
%				&
%				\multicolumn{4}{c}{Classified Model (B)}\\
%			},
%			after row=\hline
%		},
%		display columns/0/.style={
%			assume math mode = true,
%			string type,
%			column name = {}
%		},
%		display columns/1/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {EUT}
%		},
%		display columns/2/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Power}
%		},
%		display columns/3/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Inverse-S}
%		},
%		display columns/4/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Prelec}
%		},
%	]{tables/HNG_1-PBA.csv} % path/to/file
%	\end{adjustbox}
%\end{table}
%%\bigskip
%\begin{table}[ht!]
%	\centering
%	\captionsetup{justification=centering}
%	\caption{$P(A | B)$}
%	\label{tb:HNG1_PAB}
%	\begin{adjustbox}{}
%	\pgfplotstabletypeset[
%		col sep=comma,
%		every head row/.style={
%			before row={
%				\cnline{} &
%				\multicolumn{4}{c}{Classified Model (B)}\\
%			},
%			after row=\hline
%		},
%		display columns/0/.style={
%			assume math mode = true,
%			string type,
%			column name = {}
%		},
%		display columns/1/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {EUT}
%		},
%		display columns/2/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Power}
%		},
%		display columns/3/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Inverse-S}
%		},
%		display columns/4/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Prelec}
%		},
%	]{tables/HNG_1-PAB.csv} % path/to/file
%	\end{adjustbox}
%\end{table}
%}
%\notinsubfile{
%\begin{table}[ht!]
%	\centering
%	\captionsetup{justification=centering}
%	\caption{$P(A)$}
%	\label{tb:HNG1_PA}
%	\begin{adjustbox}{}
%	\pgfplotstabletypeset[
%		col sep=comma,
%		every head row/.style={
%			after row=\hline
%		},
%		display columns/0/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {EUT}
%		},
%		display columns/1/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Power}
%		},
%		display columns/2/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Inverse-S}
%		},
%		display columns/3/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Prelec}
%		},
%	]{ch4/tables/HNG_1-PA.csv} % path/to/file
%	\end{adjustbox}
%\end{table}
%%\bigskip
%\begin{table}[ht!]
%	\centering
%	\captionsetup{justification=centering}
%	\caption{$P(B)$}
%	\label{tb:HNG1_PB}
%	\begin{adjustbox}{}
%	\pgfplotstabletypeset[
%		col sep=comma,
%		every head row/.style={
%			after row=\hline
%		},
%		display columns/0/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {EUT}
%		},
%		display columns/1/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Power}
%		},
%		display columns/2/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Inverse-S}
%		},
%		display columns/3/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Prelec}
%		},
%	]{ch4/tables/HNG_1-PB.csv} % path/to/file
%	\end{adjustbox}
%\end{table}
%%\bigskip
%\begin{table}[ht!]
%	\centering
%	\captionsetup{justification=centering}
%	\caption{$P(B | A)$}
%	\label{tb:HNG1_PBA}
%	\begin{adjustbox}{}
%	\pgfplotstabletypeset[
%		col sep=comma,
%		every head row/.style={
%			before row={
%				\cnline{} &
%				\multicolumn{4}{c}{Classified Model (B)}\\
%			},
%			after row=\hline
%		},
%		display columns/0/.style={
%			assume math mode = true,
%			string type,
%			column name = {}
%		},
%		display columns/1/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {EUT}
%		},
%		display columns/2/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Power}
%		},
%		display columns/3/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Inverse-S}
%		},
%		display columns/4/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Prelec}
%		},
%	]{ch4/tables/HNG_1-PBA.csv} % path/to/file
%	\end{adjustbox}
%\end{table}
%%\bigskip
%\begin{table}[ht!]
%	\centering
%	\captionsetup{justification=centering}
%	\caption{$P(A | B)$}
%	\label{tb:HNG1_PAB}
%	\begin{adjustbox}{}
%	\pgfplotstabletypeset[
%		col sep=comma,
%		every head row/.style={
%			before row={
%				 &
%				\multicolumn{4}{c}{Classified Model (B)}\\
%			},
%			after row=\hline
%		},
%		display columns/0/.style={
%			assume math mode = true,
%			string type,
%			column name = {}
%		},
%		display columns/1/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {EUT}
%		},
%		display columns/2/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Power}
%		},
%		display columns/3/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Inverse-S}
%		},
%		display columns/4/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Prelec}
%		},
%	]{ch4/tables/HNG_1-PAB.csv} % path/to/file
%	\end{adjustbox}
%\end{table}
%}
%
%\break
%
%In Table \ref{tb:HNG1_PA} we have the probability that any given subject drawn from our hypothetical population will belong to the model given by the columns.
%This is equal to the proportion of each model we specified initially.
%In Table \ref{tb:HNG1_PB} we have the probability that a subject drawn at random from our hypothetical population will be classified as the model given by the columns.
%In Table \ref{tb:HNG1_PBA} we have the probability that a subject will be classified as the column model, given that they actually operate the row model.
%The rows of this table therefore sum to 1.
%In Table \ref{tb:HNG1_PAB} we have the probability that a subject actually operates the row model, given that they have been classified as the column model.
%The columns of this table therefore sum to 1.

\subsection{\texorpdfstring{\textcite{Harrison2016}}{Harrison and Ng (2016)} Insurance Task Welfare Expectations}
\label{sec4:WT}

The probabilities provided in Figures \ref{fig:HNG1_win_mu}, \ref{fig:HNG1_win_eut} and \ref{fig:HNG1_win_pre} are useful in terms of describing the degree of success the classification process has in correctly identifying the model operated by a subject.
The classification process itself however is only useful to economists in as much as it provides us with a model that allows us to make normative characterizations of subjects' choices.
Given our simulation process, we can measure the success of the classification process in normative terms by calculating the difference in the estimated welfare surplus of the choices made in the HNG insurance task against actual welfare surplus for each subject.

Utilizing the definition of accumulated welfare surplus given by equation (\ref{eq4:wsurplusT}), we follow \textcite[110-111]{Harrison2016} and bootstrap the estimated welfare surplus of the subjects.
We generate 500 random draws from a multivariate normal distribution using the point estimates of the parameters of the winning model as the means of the marginal distributions, and the inverse of the estimated Hessian matrix as the covariance matrix.{\footnotemark}
With each draw we calculate equation (\ref{eq4:wsurplusT}) and define the estimated welfare surplus as the average of these 500 calculations.
Therefore the difference between the estimated and real welfare is given by:

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	Note that all the probability weighing parameters and the $\lambda$ parameter are restricted mathematically to be greater than 0.
	In the estimation process, this was accomplished by exponentiating the raw parameter values passed by the optimizer to the likelihood function.
	When making the multivariate normal distribution described, we use the raw parameter estimates to generate the distribution and exponentiate the marginal distributions of the parameters that are restricted to be greater than 0.
	Thus, these resulting marginal distributions are actually log-normal distributions.
}

\begin{equation}
	\label{eq4:wsurplusDiff}
	D_N = \Delta W_{nT}(\hat{\Omega}_N) - \Delta W_{nT}(\Omega)
\end{equation}

\noindent where $N$ is the model the subject has been classified as operating, $\Omega$ is the set of parameters that define the utility function actually employed by subject $n$, and $\hat{\Omega}_N$ is the set of estimated parameters for model $N$ for subject $n$.
If the subject has been misclassified, $\Omega$ and $\hat{\Omega}_N$ will not represent the same set of parameters.
Just as we predicted probabilities of classification in equation (\ref{eq4:GAM}), we can predict the difference in estimated welfare surplus and real welfare surplus given by equation (\ref{eq4:wsurplusDiff}).
\begin{align}
	\label{eq4:GAM_welfare}
	\begin{split}
		(D_{N,M} | M = EUT)                   &= s(r) + s(\lambda)\\
%		(D_{N,M} | M = \mathit{RDU_{Pow}})    &= s(r) + s(\gamma) + s(r, \gamma) + s(\lambda)\\
%		(D_{N,M} | M = \mathit{RDU_{Invs}})   &= s(r) + s(\gamma) + s(r, \gamma) + s(\lambda)\\
		(D_{N,M} | M = \mathit{RDU_{Prelec}}) &= s(r) + s(\alpha) + s(\beta) + s(\lambda)
	\end{split}
\end{align}

\noindent where $N$ indicates the model that the subject was classified as, $M$ indicates the model the subject actually operates, and $s(\cdot)$ indicates some smooth, potentially non-linear function of its arguments.
Values of 0 for equation (\ref{eq4:GAM_welfare}) thus indicate that the subject's estimate welfare surplus equals the subject's real welfare surplus.
Thus 4 fitted models, one for each combination of 2 $M$ models and 2 $N$ models.
Additionally, given our estimates of the probability of EUT and $\mathit{RDU_{Prelec}}$ subjects being classified as operating EUT or $\mathit{RDU_{Prelec}}$, we can calculate point estimates the expected welfare surplus difference by multiplying the probabilities presented in the top row of Figures \ref{fig:HNG1_win_eut} and \ref{fig:HNG1_win_pre} with the estimated welfare surplus difference given by equation (\ref{eq4:GAM_welfare}).
We present the welfare surplus difference estimates for subjects that were classified as either EUT or $\mathit{RDU_{Prelec}}$, as well as the expected welfare surplus estimates, in Figures \ref{fig:HNG1_wel_mu}, \ref{fig:HNG1_wel_eut}, and \ref{fig:HNG1_wel_pre}.
Since the subjects which didn't converge on either EUT or $\mathit{RDU_{Prelec}}$ (labeled \enquote{NA} previously) didn't produce estimates with which we can make welfare calculations, they are not plotted.

\begin{figure}[htp!]
	\center
	\caption{Welfare Surplus Difference of \enquote{Winning} Models for Given $\lambda$ Values}
	\onlyinsubfile{
		\includegraphics[width=\textwidth]{figures/HNG_1/win_05-mu-wel-HNG_1.pdf}
		%\includegraphics[height=.5\paperheight, width=\textwidth]{figures/mu-winners-HNG_1.pdf}
	}
	\notinsubfile{
		\includegraphics[width=\textwidth]{ch4/figures/HNG_1/win_05-mu-wel-HNG_1.pdf}
	}
	\label{fig:HNG1_wel_mu}
\end{figure}

\begin{figure}[hbp!]
	\center
	\caption{Welfare Surplus Difference of \enquote{Winning} Models for EUT subjects}
	\onlyinsubfile{
		\includegraphics[width=\textwidth]{figures/HNG_1/win_05-EUT-wel-HNG_1.pdf}
		%\includegraphics[height=.5\paperheight, width=\textwidth]{figures/mu-winners-HNG_1.pdf}
	}
	\notinsubfile{
		\includegraphics[width=\textwidth]{ch4/figures/HNG_1/win_05-EUT-wel-HNG_1.pdf}
	}
	\label{fig:HNG1_wel_eut}
\end{figure}

\begin{figure}[ht!]
	\center
	\caption{Welfare Surplus Difference of \enquote{Winning} Models for $\mathit{RDU_{Prelec}}$ subjects}
	\onlyinsubfile{
		\includegraphics[width=\textwidth]{figures/HNG_1/win_05-PRE-wel-HNG_1.pdf}
		%\includegraphics[height=.5\paperheight, width=\textwidth]{figures/mu-winners-HNG_1.pdf}
	}
	\notinsubfile{
		\includegraphics[width=\textwidth]{ch4/figures/HNG_1/win_05-PRE-wel-HNG_1.pdf}
	}
	\label{fig:HNG1_wel_pre}
\end{figure}

As before, the solid red line represents subjects that were classified as EUT and the dotted green line represents subjects that were classified as $\mathit{RDU_{Prelec}}$.
In these figures, however, the dashed blue line represents the expected welfare surplus difference.

%As was done in the previous section, we draw a sample of $100,000$ subjects from the joint population distribution defined in the previous section, with $100,000 \times 0.5 = 50,000$ EUT subjects, $100,000 \times 0.1 = 10,000$ $\mathit{RDU_{Pow}}$ and $\mathit{RDU_{Invs}}$ subjects, and $100,000 \times 0.3 = 30,000$ $\mathit{RDU_{Prelec}}$ subjects.
%For each of the sampled subjects, we fit the model from equation (\ref{eq4:GAM_welfare}) corresponding the model $M$ they actually operate for each of the four $N$ models they could potentially be classified as operating.
%This gives us the welfare surplus difference for each possible classification for every subject.

%\subsubsection{Case 1}
%First we present the welfare surplus difference for the subjects operating $\mathit{RDU_{Pow}}$ and $\mathit{RDU_{Invs}}$ models in Figure \ref{fig:HNG1_POW_INV_welfare}, and then for the subjects operating EUT and $\mathit{RDU_{Prelec}}$ models in \ref{fig:HNG1_EUT_PRE_welfare}.
%The left column of Figure \ref{fig:HNG1_POW_INV_welfare} plots the difference in welfare surplus for $\mathit{RDU_{Pow}}$ and the right column plots the difference in welfare surplus for $\mathit{RDU_{Invs}}$.
%Each row represents subjects grouped by their $\lambda$ parameter, with the top row containing subjects with $0.01 < \lambda < 0.10$, the middle row containing $0.1 < \lambda < 0.2$, and the bottom row $0.2 < \lambda < 0.3$.
%Thus, in the top left plot, we have subjects that operate a $\mathit{RDU_{Pow}}$ model with $0.01 < \lambda < 0.10$.
%Likewise, the bottom right plot shows subjects that operate a $\mathit{RDU_{Invs}}$ model with $0.2 < \lambda < 0.3$.
%The four lines in each plot show the calculated welfare surplus difference for subjects who were classified as operating each of the four possible models and how this difference changes with the probability weighting parameter for each model.
%
%Similarly, the left column of \ref{fig:HNG1_EUT_PRE_welfare} shows subjects that operate an EUT model, while the middle and right columns show subjects that operate a $\mathit{RDU_{Prelec}}$ model.
%The left column plots the welfare surplus difference by the CRRA parameter for the EUT subjects, while the middle column plots the welfare surplus difference by the $\alpha$ probability weighting parameter and the right column plots the welfare surplus difference by the $\beta$ probability weighing parameter for $\mathit{RDU_{Prelec}}$ subjects.
%Again, each row represents subjects grouped by their $\lambda$ parameter, with the top row containing subjects with $0.01 < \lambda < 0.10$, the middle row containing $0.1 < \lambda < 0.2$, and the bottom row $0.2 < \lambda < 0.3$.
%Thus, the top leftmost plot shows subjects that operate a EUT model with $0.01 < \lambda < 0.10$, and the bottom right plot shows subjects that operate a $\mathit{RDU_{Pow}}$ model with $0.2 < \lambda < 0.3$.
%
%Unlike Figure \ref{fig:HNG1_POW_INV_welfare}, Figure \ref{fig:HNG1_EUT_PRE_welfare} does not display subjects that were classified as $\mathit{RDU_{Invs}}$ as there were too few subjects to draw meaningful plots.
%For instance, of the subjects that operate a $\mathit{RDU_{Prelec}}$ model with $0.01 < \lambda < 0.10$, thus the top middle and rightmost plots, there were only 12 subjects that were classified as operating a $\mathit{RDU_{Invs}}$ model.
%This is compared to 25,678 subjects that were classified as operating an EUT model, 5,546 as operating an $\mathit{RDU_{Pow}}$ model, and 42,480 as operating a $\mathit{RDU_{Prelec}}$ model.
%
%As was done in Figures \ref{fig:HNG1_pre_a_win} and \ref{fig:HNG1_pre_b_win}, the middle column of Figure \ref{fig:HNG1_EUT_PRE_welfare} does not contain subjects that have $\beta$ parameters greater than 0.8 and less than 1.2, and the right column of Figure \ref{fig:HNG1_EUT_PRE_welfare} does not contain subjects that have $\alpha$ parameters greater than 0.8 and less than 1.
%It is important to note that no plot within either Figure \ref{fig:HNG1_POW_INV_welfare} or \ref{fig:HNG1_EUT_PRE_welfare} share a y-axis scale with any other plot.
%The difference in welfare surplus changes greatly from model to model and from parameter to parameter, keeping all plots on the same y-axis scale would hide some of the nuance of these differences.
%
%\begin{figure}[hp!]
%	\center
%	\caption{$P(B|RDU_{Prelec})$ for Given $\beta$ Values}
%	\onlyinsubfile{
%		\includegraphics[width=\textwidth]{figures/pi-welfare5.pdf}
%		%\includegraphics[width=\textwidth, height=.6\paperheight]{figures/pre-welfare5.pdf}
%	}
%	\notinsubfile{
%		\includegraphics[width=\textwidth]{ch4/figures/pi-welfare5.pdf}
%		%\includegraphics[height=.3\paperheight]{ch4/figures/pre-welfare5.pdf}
%	}
%	\label{fig:HNG1_POW_INV_welfare}
%\end{figure}
%
%\begin{figure}[hp!]
%	\center
%	\caption{$P(B|RDU_{Prelec})$ for Given $\beta$ Values}
%	\onlyinsubfile{
%		\includegraphics[width=\textwidth]{figures/ep-welfare5.pdf}
%		%\includegraphics[width=\textwidth, height=.6\paperheight]{figures/pre-welfare5.pdf}
%	}
%	\notinsubfile{
%		\includegraphics[width=\textwidth]{ch4/figures/ep-welfare5.pdf}
%		%\includegraphics[height=.3\paperheight]{ch4/figures/pre-welfare5.pdf}
%	}
%	\label{fig:HNG1_EUT_PRE_welfare}
%\end{figure}
%
%\subsubsection{Case 2}
%
%In Case 2, we present the predicted welfare surplus difference for our hypothetical population given the GAM models defined in equiation (\ref{eq4:GAM_welfare}).
%Table \ref{tb:HNG1_wsum} shows the average welfare surplus difference for subjects operating the models given by the rows, but classified as the models given by the columns.
%This, the top rightmost cell shows the average weflare surplus for subjects who operate an EUT model and were correctly classified as operating an EUT model, while the top rightmost column shows the average welfare surplus of subjects who operate a EUT model by were classified as operating a $\mathit{RDU_{Prelec}}$ model.
%
%Table \ref{tb:HNG1_PBA_wel} shows the expected average welfare surplus difference for subjects who actually operate the models given.
%This is defined by first multiplying element-wise the matrix given by table \ref{tb:HNG1_PBA} and the matrix given in table \ref{tb:HNG1_wsum}, and then summing each row of the resulting matrix:
%
%\begin{equation}
%	\label{eq4:PBA_wel}
%	E [ D_{NM} | M ] = \sum^N P(B = N | A = M) \times D_{NM}
%\end{equation}
%
%Table \ref{tb:HNG1_PAB_wel} shows the expected average welfare surplus difference for subjects who were classified as operating the models given.
%This is defined by first multiplying element-wise the matrix given by table \ref{tb:HNG1_PAB} and the matrix given in table \ref{tb:HNG1_wsum}, and then summing each column of the matrix:
%
%\begin{equation}
%	\label{eq4:PBA_wel2}
%	E [ D_{NM} | N ] = \sum^M P(A = M | B = N) \times D_{NM}
%\end{equation}
%
%
%\onlyinsubfile{
%\begin{table}[ht!]
%	\centering
%	\captionsetup{justification=centering}
%	\caption{Welfare Surplus Difference by Model and Classification}
%	\label{tb:HNG1_wsum}
%	\begin{adjustbox}{}
%	\pgfplotstabletypeset[
%		col sep=comma,
%		every head row/.style={
%			before row={
%				&
%				\multicolumn{4}{c}{Classified Model (B)}\\
%			},
%			after row=\hline
%		},
%		display columns/0/.style={
%			assume math mode = true,
%			string type,
%			column name = {}
%		},
%		display columns/1/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {EUT}
%		},
%		display columns/2/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Power}
%		},
%		display columns/3/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Inverse-S}
%		},
%		display columns/4/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Prelec}
%		},
%	]{tables/HNG_1-wsum.csv} % path/to/file
%	\end{adjustbox}
%\end{table}
%%\bigskip
%\begin{table}[ht!]
%	\centering
%	\captionsetup{justification=centering}
%	\caption{Expected Welfare Surplus Difference, Given Operating Model}
%	\label{tb:HNG1_PBA_wel}
%	\begin{adjustbox}{}
%	\pgfplotstabletypeset[
%		col sep=comma,
%		every head row/.style={
%			after row=\hline
%		},
%		display columns/0/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {EUT}
%		},
%		display columns/1/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Power}
%		},
%		display columns/2/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Inverse-S}
%		},
%		display columns/3/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Prelec}
%		},
%	]{tables/HNG_1-PBA-wel.csv} % path/to/file
%	\end{adjustbox}
%\end{table}
%%\bigskip
%\begin{table}[ht!]
%	\centering
%	\captionsetup{justification=centering}
%	\caption{Expected Welfare Surplus Difference, Given Classification Model}
%	\label{tb:HNG1_PAB_wel}
%	\begin{adjustbox}{}
%	\pgfplotstabletypeset[
%		col sep=comma,
%		every head row/.style={
%			after row=\hline
%		},
%		display columns/0/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {EUT}
%		},
%		display columns/1/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Power}
%		},
%		display columns/2/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Inverse-S}
%		},
%		display columns/3/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Prelec}
%		},
%	]{tables/HNG_1-PAB-wel.csv} % path/to/file
%	\end{adjustbox}
%\end{table}
%}
%\notinsubfile{
%\begin{table}[ht!]
%	\centering
%	\captionsetup{justification=centering}
%	\caption{Welfare Surplus Difference by Model and Classification}
%	\label{tb:HNG1_wsum}
%	\begin{adjustbox}{}
%	\pgfplotstabletypeset[
%		col sep=comma,
%		every head row/.style={
%			before row={
%				&
%				\multicolumn{4}{c}{Classified Model (B)}\\
%			},
%			after row=\hline
%		},
%		display columns/0/.style={
%			assume math mode = true,
%			string type,
%			column name = {}
%		},
%		display columns/1/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {EUT}
%		},
%		display columns/2/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Power}
%		},
%		display columns/3/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Inverse-S}
%		},
%		display columns/4/.style={
%			precision = 3,
%			fixed,
%			zerofill,
%			column name = {Prelec}
%		},
%	]{ch4/tables/HNG_1-wsum.csv} % path/to/file
%	\end{adjustbox}
%\end{table}
%%\bigskip
%\begin{table}[ht!]
%	\centering
%	\captionsetup{justification=centering}
%	\caption{Expected Welfare Surplus Difference, Given A}
%	\label{tb:HNG1_PBA_wel}
%	\begin{adjustbox}{}
%	\pgfplotstabletypeset[
%		col sep=comma,
%		every head row/.style={
%			after row=\hline
%		},
%		display columns/0/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {EUT}
%		},
%		display columns/1/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Power}
%		},
%		display columns/2/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Inverse-S}
%		},
%		display columns/3/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Prelec}
%		},
%	]{ch4/tables/HNG_1-PBA-wel.csv} % path/to/file
%	\end{adjustbox}
%\end{table}
%%\bigskip
%\begin{table}[ht!]
%	\centering
%	\captionsetup{justification=centering}
%	\caption{Expected Welfare Surplus Difference, Given B}
%	\label{tb:HNG1_PAB_wel}
%	\begin{adjustbox}{}
%	\pgfplotstabletypeset[
%		col sep=comma,
%		every head row/.style={
%			after row=\hline
%		},
%		display columns/0/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {EUT}
%		},
%		display columns/1/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Power}
%		},
%		display columns/2/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Inverse-S}
%		},
%		display columns/3/.style={
%			precision = 2,
%			fixed,
%			zerofill,
%			column name = {Prelec}
%		},
%	]{ch4/tables/HNG_1-PAB-wel.csv} % path/to/file
%	\end{adjustbox}
%\end{table}
%}
%
%\subsubsection{Discussion}

The discussion about how the classification process relates to the welfare surplus of the subject being classified is in many ways more important than the previous discussion of the accuracy of the process itself.
This is because economists distinguish themselves from decision theorists by making normative statements about how an individuals economic choices relate to their economic well-being.
The accuracy of the classification process is valuable only in as much as it can aid in the accuracy of the normative statements we can construct using this process.
\textcite[25]{Leamer2012} makes a similar statement when discussing the general fallibility of macroeconomic models: \enquote{Fortunately, our goal as economists is not soundness, but usefulness.}

From Figures \ref{fig:HNG1_wel_mu}, \ref{fig:HNG1_wel_eut} and \ref{fig:HNG1_wel_pre}, we can see that there are parameter values for every model where the welfare surplus difference is not noticeably different between correctly and incorrectly classified subjects, and in some cases the welfare surplus difference for misclassified subjects is closer to 0 than for correctly classified subjects.
In Figure \ref{fig:HNG1_wel_pre} we can see that of the $\mathit{RDU_{Prelec}}$ subjects that have $\alpha$ values close to 1, the subjects that have been classified as EUT instead of $\mathit{RDU_{Prelec}}$ have welfare surplus differences that are somewhat closer to 0 than the subjects that had been classified as $\mathit{RDU_{Prelec}}$.
In Figure \ref{fig:HNG1_wel_eut} we can see that of the EUT subjects that have CRRA values greater than 0.75, the welfare surplus difference estimates are indistinguishable between subjects classified as either $\mathit{RDU_{Prelec}}$ or EUT.
That misclassified subjects in these cases have welfare surplus estimates relatively close to the subjects' real welfare surplus demonstrates that even though the classification process has not been accurate for these subjects, it nonetheless can be useful when used to characterize the welfare surplus of subjects' choices in the insurance task.
We revisit this concept later.

However, we can also see that for wide ranges of parameter values, misclassified subjects have welfare surplus estimates that are significantly different from the real welfare surplus and are farther from the real welfare surplus estimates than the correctly classified subjects.
The cost of misclassification is particularly great for $\mathit{RDU_{Prelec}}$ subjects, which is evident in Figures \ref{fig:HNG1_wel_mu} and \ref{fig:HNG1_wel_pre}.
In Figure \ref{fig:HNG1_wel_mu} we see that for the entire range of $\lambda$ values considered, $\mathit{RDU_{Prelec}}$ subjects that were classified as EUT had less accurate welfare surplus difference estimates than EUT subjects classified as $\mathit{RDU_{Prelec}}$.
In the third column of Figure \ref{fig:HNG1_wel_pre}, we can see that as the $\alpha$ parameter approaches 0, $\mathit{RDU_{Prelec}}$ subjects that have been classified as EUT have welfare surplus estimates that increasingly diverge from the real welfare surplus estimates.
In the fouth column of Figure \ref{fig:HNG1_wel_pre}, we see generally that as $\beta$ increases past 1, the subjects that have been incorrectly classified as operating a EUT model also have welfare surplus estimates that increasingly differ from the real welfare surplus, but this divergence is of roughly the same magnitude seen in the third column of Figure \ref{fig:HNG1_wel_pre} as $\alpha$ increases above 1.

That subjects actually operating a $\mathit{RDU_{Prelec}}$ model are badly characterized by an EUT model when they have probability weighing parameters that differ greatly from 1 should not be surprising.
The $\mathit{RDU_{Prelec}}$ model is flexible, which allows it to fit data well, but also means there are more opportunities for misclassification as EUT to matter in meaningful ways.

\section{Artificial Laboratories}

The analyses thus far constitute \textit{ex post} power analyses of the experimental instrument and classification process employed by HNG, and an analysis of the expected welfare characterizations thereof, though it is limited to only 2 of the 4 models employed by HNG.
The power analysis aspect of this process constitutes a statistical inquiry into an experimental protocol and is similar to other \textit{ex ante} and \textit{ex post} power analyses.
The welfare characterization aspect of this analysis constitutes the economic inquiry into this experimental protocol.
Both inquires are important, but making accurate predictions or characterizations about the welfare consequences of choices by economic agents should be of greater value to economists than the descriptive accuracy of the model used to derived these calculations.{\footnotemark}

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	Indeed, I make this argument against the Random Preferences stochastic model in Chapter 2.
}

The two inquiries are related (as noted by \textcite[105]{Harrison2016}), a model is needed in order to make calculations of consumer surplus and thus we need a reasonable method for selecting a model to make these calculations.
However, if our objective is to generate accurate welfare characterizations, and not necessarily accurate model classifications, then we should explore how different experimental designs, model specifications, and model selection processes influence the accuracy of welfare characterizations.
For instance, the selection of the number and type of lottery pairs should be influenced by how they result in more accurate welfare predictions in the choice domain that is welfare relevant to the experimenter; the insurance task in the case of HNG.
Indeed, the argument of \textcite{Harrison2016} in favor of developing a structural model of risk preferences and estimating the parameters of these models to make welfare characterizations is that this process provides a better assessment of the welfare consequences of purchasing insurance than the \enquote{take-up} metric. {\footnotemark}

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	The \enquote{take-up} metric \textit{does} make a normative statement about insurance purchase decisions, \enquote{take-up is good, failure to take-up is bad}.
}

These kind of enquires into how differing experimental methods affect the accuracy of welfare characterizations are themselves experiments of a kind.
In this section we propose two modifications to the experimental protocol employed by HNG and investigate how they differ from the expected welfare surplus predictions.
The first of these proposals is a recommendation that would be familiar to any statistician: increase the sample size per subject by increasing the number of lottery pairs in the lottery instrument used in estimation.
The second proposal is to forego any attempt to accurately classify subjects as EUT or RDU and instead use the fitted $\mathit{RDU_{Prelec}}$ models when they have passed the exclusionary rules set by HNG, and use non-excluded EUT models otherwise.

For the second proposal, we utilize the choice data and model estimations from the simulation process described previously and simply change the critical value for the non-linear Wald test of linear probability from $0.05$ to $1$ so that the null hypothesis of linear probability weighting is rejected in every case.
For the first proposal, however, we use the same simulated subjects used in all the analyses thus far, but have them each respond to the HNG lottery instrument 13 times instead of once for a total of $1040$ choices per subject.
I refer to this 1040 choice lottery battery as the $\text{HNG}_{1040}$ instrument.
The estimation procedure, application of exclusionary rules, and classification process is then applied to this new choice data to select a winning model for each subject.
The parameter estimates of the winning model in each proposal are used to calculate the welfare surplus of the subject in the insurance task as before.
Thus, the first proposal changes the experimental instrument used to estimate models leaving the exclusionary rules and classification process unchanged, while the second proposal leaves the experimental instrument and exclusionary rules unchanged and alters the classification process.
The results of the classification process for the first proposal are presented in Figures \ref{fig:HNG_win_eut}, \ref{fig:HNG_win_pre}, and the estimated welfare surplus difference results are presented in Figures \ref{fig:HNG_wel_eut} and \ref{fig:HNG_wel_pre} for proposal 1, and in Figures \ref{fig:HNG1_def_wel_eut} and \ref{fig:HNG1_def_wel_pre} for proposal 2.
The plots of the expected welfare surplus difference for the original HNG method, and the two new proposals are given in Figures \ref{fig:exwel-eut} and \ref{fig:exwel-pre}.

 % WINNING
\begin{figure}[hp!]
	\center
	\caption{Probability of \enquote{Winning} for EUT subjects - 1040 choices}
	\onlyinsubfile{
		\includegraphics[width=\textwidth]{figures/HNG/win_05-EUT-win-HNG.pdf}
		%\includegraphics[height=.5\paperheight, width=\textwidth]{figures/mu-winners-HNG_1.pdf}
	}
	\notinsubfile{
		\includegraphics[width=\textwidth]{ch4/figures/HNG/win_05-EUT-win-HNG.pdf}
	}
	\label{fig:HNG_win_eut}
\end{figure}

\begin{figure}[hp!]
	\center
	\caption{Probability of \enquote{Winning} for $\mathit{RDU_{Prelec}}$ subjects - 1040 choices}
	\onlyinsubfile{
		\includegraphics[width=\textwidth]{figures/HNG/win_05-PRE-win-HNG.pdf}
		%\includegraphics[height=.5\paperheight, width=\textwidth]{figures/mu-winners-HNG_1.pdf}
	}
	\notinsubfile{
		\includegraphics[width=\textwidth]{ch4/figures/HNG/win_05-PRE-win-HNG.pdf}
	}
	\label{fig:HNG_win_pre}
\end{figure}

Looking first at the classification power of the $\text{HNG}_{1040}$ instrument in Figures \ref{fig:HNG_win_eut} and \ref{fig:HNG_win_pre}, we see that the new instrument has significantly improved power overall, and that the variation of power over the range of parameter values follows much the same pattern as the original HNG instrument.
In Figure \ref{fig:HNG_win_eut} we see that classification power is largely uniform across the entire range of parameters considered, with some small increase in the probability of EUT subjects classified as $\mathit{RDU_{Prelec}}$ as $\lambda$ values increase, and small increase in the probability of EUT subjects being classified as EUT as the CRRA value increases.
The probability of correctly classifying EUT subjects as EUT is greater under the $\text{HNG}_{1040}$ instrument than the HNG instrument across the entire range of parameters considered.
The rate of non-convergence in the $\text{HNG}_{1040}$ instrument however is also noticeably different in the $\text{HNG}_{1040}$ instrument.
It is not perceptibly different from 0 across the entire range of parameters considered.

In Figure \ref{fig:HNG_win_pre} we see that classification power of the $\text{HNG}_{1040}$ instrument follows the patterns of the HNG instrument, but more rapid changes in the slopes of the lines for each parameter except the CRRA parameter.
The probability of an $\mathit{RDU_{Prelec}}$ subject being misclassified as EUT increases rapidly as the $\lambda$ parameter increases, and as either of the probability weighting parameters approach 1 from either side.
The probability of correctly classifying $\mathit{RDU_{Prelec}}$ subjects as $\mathit{RDU_{Prelec}}$ however is again universally higher under the $\text{HNG}_{1040}$ instrument, and the probability of non-convergence is nearly 0 for almost the entire range of parameters considered.
The probability of misclassification increases somewhat as $\lambda$ increases, as $\alpha$ and the CRRA parameters decrease, and increases rapidly as the $\beta$ parameter goes below 0.5.

That we should generally see the same patterns as before, but with significantly greater probabilities of correctly classifying subjects across the whole ranges of parameter values considered should not be a surprise.
The probabilities of type I and type II errors generally decrease with sample size in any econometric test, and so we should expect this result when we increase the per-subject sample size 13-fold.
That the patterns of how the probabilities change with parameters values are much the same as before is due to the lottery pairs, considered models, and classification process being identical.
With a different composition of the type of lottery pairs we would expect to see somewhat different probability patterns, perhaps increasing power in the parameter ranges we would expect to see from real subjects.

The value of this increase classification power, as was stated before, lies in its ability to make better welfare characterizations.
We can see the estimates of welfare surplus given the classification based on the $\text{HNG}_{1040}$ instrument in Figures \ref{fig:HNG_wel_eut} and \ref{fig:HNG_wel_pre}.
In Figure \ref{fig:HNG_wel_eut} we see for EUT subjects that were classified correctly as EUT, that the expected welfare surplus difference is imperceptibly different from 0 across much of the range of parameters considered.
In addition, even though EUT subjects classified as $\mathit{RDU_{Prelec}}$ have generally worse welfare surplus difference estimates, given the high likelihood of EUT subjects being correctly classified as EUT the expected welfare surplus difference is also very close to 0 for much of the parameter ranges considered.
This indicates that not only is the classification process much more accurate, but the parameter estimates for the models are likely to more accurate as well.
We see that as the CRRA value goes below $-0.5$ and the $\lambda$ value increase, welfare surplus difference becomes more negative for both classified models.

% HNG WEL
\begin{figure}[htp!]
	\center
	\caption{Welfare Surplus Difference of \enquote{Winning} Models for EUT subjects - 1040 choices}
	\onlyinsubfile{
		\includegraphics[width=\textwidth]{figures/HNG/win_05-EUT-wel-HNG.pdf}
		%\includegraphics[height=.5\paperheight, width=\textwidth]{figures/mu-winners-HNG.pdf}
	}
	\notinsubfile{
		\includegraphics[width=\textwidth]{ch4/figures/HNG/win_05-EUT-wel-HNG.pdf}
	}
	\label{fig:HNG_wel_eut}
\end{figure}

In Figure \ref{fig:HNG_wel_pre} we see for $\mathit{RDU_{Prelec}}$ subjects that were classified correctly as $\mathit{RDU_{Prelec}}$, that the expected welfare surplus difference is also very close to 0 across much of the range of parameters considered, though not as close 

\begin{figure}[hb!]
	\center
	\caption{Welfare Surplus Difference $\mathit{RDU_{Prelec}}$ subjects - 1040 choices}
	\onlyinsubfile{
		\includegraphics[width=\textwidth]{figures/HNG/win_05-PRE-wel-HNG.pdf}
		%\includegraphics[height=.5\paperheight, width=\textwidth]{figures/mu-winners-HNG.pdf}
	}
	\notinsubfile{
		\includegraphics[width=\textwidth]{ch4/figures/HNG/win_05-PRE-wel-HNG.pdf}
	}
	\label{fig:HNG_wel_pre}
\end{figure}

% HNG_1 default WEL

\begin{figure}[htp!]
	\center
	\caption{Welfare Surplus Difference for EUT subjects - 80 choices}
	\onlyinsubfile{
		\includegraphics[width=\textwidth]{figures/HNG_1/default-EUT-wel-HNG_1.pdf}
		%\includegraphics[height=.5\paperheight, width=\textwidth]{figures/mu-winners-HNG_1.pdf}
	}
	\notinsubfile{
		\includegraphics[width=\textwidth]{ch4/figures/HNG_1/default-EUT-wel-HNG_1.pdf}
	}
	\label{fig:HNG1_def_wel_eut}
\end{figure}

\begin{figure}[hbp!]
	\center
	\caption{Welfare Surplus Difference for $\mathit{RDU_{Prelec}}$ subjects - 80 choices}
	\onlyinsubfile{
		\includegraphics[width=\textwidth]{figures/HNG_1/default-PRE-wel-HNG_1.pdf}
		%\includegraphics[height=.5\paperheight, width=\textwidth]{figures/mu-winners-HNG_1.pdf}
	}
	\notinsubfile{
		\includegraphics[width=\textwidth]{ch4/figures/HNG_1/default-PRE-wel-HNG_1.pdf}
	}
	\label{fig:HNG1_def_wel_pre}
\end{figure}

\begin{figure}[htp!]
	\center
	\caption{Welfare Surplus Difference for EUT subjects}
	\onlyinsubfile{
		\includegraphics[width=\textwidth]{figures/EUT-exwel-full.pdf}
		%\includegraphics[height=.5\paperheight, width=\textwidth]{figures/mu-winners-HNG_1.pdf}
	}
	\notinsubfile{
		\includegraphics[width=\textwidth]{ch4/figures/EUT-exwel-full.pdf}
	}
	\label{fig:exwel-eut}
\end{figure}

\begin{figure}[hbp!]
	\center
	\caption{Welfare Surplus Difference for $\mathit{RDU_{Prelec}}$ subjects - 80 choices}
	\onlyinsubfile{
		\includegraphics[width=\textwidth]{figures/PRE-exwel-full.pdf}
		%\includegraphics[height=.5\paperheight, width=\textwidth]{figures/mu-winners-HNG_1.pdf}
	}
	\notinsubfile{
		\includegraphics[width=\textwidth]{ch4/figures/PRE-exwel-full.pdf}
	}
	\label{fig:exwel-pre}
\end{figure}


\section{Conclusions}

% Wilcox (2008), pg. 264)
%In fact, this seems to be the case in most such simulated data sets with individual estimation when the fit comparison is confined to the same choice data used for estimation  that is, for in-sample fit comparisons.

\break
\section{Appendix - Numerical Optimizations}

\textcite{Harrison2016} use the popular statistical software Stata to conduct their analysis, and Stata's modified Newton-Rhapson (NR) algorithm to find the maximum likelihood estimates.
I, however, use the R statistical software, with the NR algorithm provided in the package \enquote{maxLik} to conduct our analysis throughout this chapter.
Both our approach and that of \textcite{Harrison2016} require \enquote{handwritten} likelihood functions due to the particular nature of recovering maximum likelihood estimates from non-linear structural models.
The handwritten program of \textcite{Harrison2016} is written in the Stata language, whereas our program is written in C++, and compiled and called by R.

For an in-depth discussion of how the NR algorithm finds the maximum of a function see \textcite[213-219]{Train2002}.
For our purposes, only a few key points about how the NR algorithm operates are useful to bear in mind.
Firstly, a maximum is declared when the gradient of the likelihood function approaches 0, and the matrix of second derivatives of the likelihood, the Hessian matrix, is negative definite.
These two conditions indicate that the likelihood function is locally concave at the point where these conditions hold (the Hessian condition), and that the point exists at a maximum of this local concavity (the gradient condition).
These conditions are shared by other \enquote{gradient-based} optimizers subject as the Boyden-Fletcher-Goldfarb-Shanno (BFGS) or Berndt-Hall-Hall-Hausman (BHHH) algorithms.

Secondly, the NR and other gradient-based algorithms are not \enquote{global} optimizers.
If the likelihood function is not globally concave, the NR optimizer is not guaranteed to reach a global optimum from any starting point.
For instance, if the log-likelihood function is highly bimodal and the initial values for the optimizer are near the smaller mode, the NR optimizer may converge on the maximum of the concave portion of the smaller mode.
The NR algorithm attempts to mitigate situations like this{\footnotemark}, but ultimately the NR algorithm is only guaranteed to find a global maximum if the likelihood function is globally concave \parencite[218]{Train2002}.
I do not, however, expect the likelihood functions applied to the data we recover from experiments to be globally concave \parencite[227]{Train2002}.
If we employed a linear-in-parameters utility function, as opposed to the non-linear functions we actually employ, we would have a globally concave log-likelihood function.

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	See \textcite[216]{Train2002}, for specifics about how NR attempts to mitigate this problem.
	Stata's \enquote{ml search} command additionally tries to mitigate this problem by initially searching for parameter sets that are near a global maximum, and using these found parameter sets as initial values for the NR algorithm.
}

% The Craft of Economics
% Edward E. Leamer
% p 25 -  Dissucesses the idea that two different statistical packages producing different results being shocking as "Economic Fiction"

Numerical optimization also takes place on a physical machine, and the limits of how computers manipulate and store numbers will also affect the result of an optimization exercise.
\textcite{Gould2006} discusses how real numbers are stored and processed in modern computers and how differences in the order of operations can lead to differences in how a computer stores a value.
Again, only a few points here are necessary to bear in mind.
Firstly, there is a limit at which a computer can effectively distinguish between two different real numbers.
Of particular interest to us is the limit at which a computer can distinguish between a small number and 0, as one of the conditions of finding an optimum was that the gradient must be equal to 0.{\footnotemark}
Indeed, even for peaked likelihood functions, there can be (and must be for continuous parameter sets), ranges of parameters at which the gradient is indistinguishable from 0.
Because of this, in the actual operation of an optimizer, a threshold is stipulated below which the gradient is considered equivalent to 0, and a (potentially different) threshold is stipulated below which the Hessian is considered negative definite.
These thresholds are very small numbers, but usually not the smallest numbers that a computer can distinguish from 0.
The issue with how computers store and manipulate numerical values is of course a very general issue and not unique to the NR algorithm, or the R or Stata statistical software.
Similar issues would arise for different optimization algorithms, such as BFGS or BHHH algorithms, though the differences between these algorithms and NR imply different issues as well.{\footnotemark}

\begin{lrbox}{\LstBoxR}
\begin{lstlisting}
.Machine$double.xmin == 0
FALSE
z <- .Machine$double.xmin + 0.1
z - 0.01 == 0
TRUE
\end{lstlisting}
\end{lrbox}

\begin{lrbox}{\LstBoxStata}
\begin{lstlisting}[language=bash]
di smallestdouble() == 0
0
scalar z = smallestdouble() + 0.1
di z - 0.1 == 0
1
\end{lstlisting}
\end{lrbox}

\addtocounter{footnote}{-2}
\stepcounter{footnote}\footnotetext{
For instance, in R, the smallest positive number recognized as different from 0 is given by the value of \texttt{.Machine\$double.xmin}, and in Stata is given by the value of \texttt{smallestdouble()}.
After having stored these values in either R or Stata, and then operating on them, these smallest values are then lost:

\noindent For R:

\usebox{\LstBoxR}

\noindent For Stata:

\usebox{\LstBoxStata}

The numerical precision is lost after the initial addition operation, and now the computer cannot distinguish between 0 and the operated on value, even though it is mathematically different from 0.
}
\stepcounter{footnote}\footnotetext{
	Again, \textcite[220-225]{Train2002} is a useful reference to understand how these algorithms operate.
}

What is important to note about these issues is that different optimization algorithms, initial values given to the optimizer, tolerances for convergence, or potentially even the order of operations in the \enquote{handwritten} programs used in optimizers can lead to different estimates of parameter values, or to different convergence codes.{\footnotemark}
Researchers generally recognise these issues and attempt estimation on a variety of optimizers within at least one statistical package, but seldom change the tolerances for convergence from those deemed \enquote{sane} defaults by the software's authors, or run estimations across different statistical software.{\footnotemark}

\addtocounter{footnote}{-2}
\stepcounter{footnote}\footnotetext{
	Generally optimizers give \enquote{codes} to signal the degree of confidence in the returned optimum.
	These may indicate that the gradient is below its tolerance level, that the gradient or likelihood hasn't decreased after many iterations, that the limit of iterations has been reached, or that the optimizer failed to converge on a single maximum.
	I follow \textcite{Harrison2016} in only considering estimates that have gradients below the given threshold value and a Hessian that is negative definite.
}
\stepcounter{footnote}\footnotetext{
	The NR, BFGS, and BHHH, optimizers are all available in Stata and R.
	Concerning tolerance levels, Stata's help file for its \texttt{ml\_maxopts} command states concerning the options for changing tolerance levels: \enquote{These options are seldom used.}
}

\onlyinsubfile{
\newpage
\printbibliography[segment=4, heading=subbibliography]
}

\end{document}
