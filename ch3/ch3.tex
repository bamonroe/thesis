\documentclass[../main.tex]{subfiles}

\begin{document}
\doublespacing
\setcounter{chapter}{2}

\singlespacing
\chapter{The Welfare Implications of Stochastic Models}
\doublespacing

\lltoc % Table of contents only when locally compiled

Given the discussion about how the various stochastic models generally support incorporation of the normative notion of welfare, I reintroduce the question asked earlier in Chapter 2, section 2.2:
\enquote{What are the likely welfare implications of an economic agent's choices in an incentivized risky environment given an assumed stochastic model of risky choice?}
The conclusion for the Random Preference (RP) model and its derivative, the Random Preference Per Option (RPPO) model, is \enquote{no perfectly coherent statements can be made.}
As stated in the conclusion of Chapter 2, the Random Error (RE) and Tremble (TR) models do not suffer from this inadequacy, and will be referred to as \enquote{coherent models.}
%In this chapter, I continue to answer the primary question by utilizing coherent stochastic models, a popular experimental preference elicitation instrument, and simulation methods to derive numerical characterizations of welfare.

Many stochastic models make specific restrictions on the probabilities associated with certain special choice scenarios.
For instance, options in a choice scenario which are first order stochastically dominated (FOSD) by another option are prohibited under the RP model and severely restricted under many heteroscedastic RE models.
For any choice scenario, an option which is FOSD by another option can also be said to provide the subject with less expected utility, and thus less expected welfare.
Thus, at the individual choice level there is a perfect relationship between likelihood and welfare realization.
In this chapter, I make clear that this relationship between likelihood and welfare realization does not hold for aggregated choice patterns.
I also show that choice patterns which display behavior that cannot be rationalized by a utility function can often result in greater welfare than choice patterns which can be rationalized.

How often this divergence between welfare realization and likelihood occurs, and the extent to which this divergence in welfare terms is meaningful for an individual agent, depends on the nature of the choice scenarios presented to the agent and the agent's preferences.
Additionally, the likelihood of an agent holding preferences in a population of interest will determine how likely we are to observe choice patterns that are able to be rationalized, but are in fact suboptimal.
To help understand the relationship between the likelihood of observing a choice pattern and its potential welfare consequences, I conduct a numerical exercise with methods related to Maximum Simulated Likelihood (MSL) and utilizing the Multiple Price List (MPL) proposed by \textcite{Holt2002} (HL) for a given hypothetical population of agents.
To understand how the distribution of preferences in a population influence the expected welfare realization of a population, I repeat this analysis for many different populations.
First however, I revisit some notation from Chapter 2, briefly describe some econometric methods for identification, and then propose some further notation to make concepts cleaner.

\singlespacing
\section{Notation and Estimation} \label{ssec:Notation}
\doublespacing

For any salient lottery $X_a$, and any vector of parameters $\beta_i$, there exists some certain outcome, $\CE_a$, such that subject $i$ is indifferent between the lottery and the certainty equivalent:
\begin{equation}
	\label{eq3:CE.indiff}
	X_a \sim^i {\CE}_a \;\Leftrightarrow\; G(\beta_i,X_a) = G(\beta_i, {\CE}_a)
\end{equation}
\noindent where $G(\cdot)$ is some utility function with all the usual properties.
For our purposes throughout this chapter, we will assume some variation of the Rank Dependent Utility (RDU) structure defined as follows:
\begin{equation}
	\label{eq3:RDU}
	RDU = \sum_{c=1}^{C} \left[ w_c(p) \times u(x_c) \right]
\end{equation}
\noindent where $u(\cdot)$ is the CRRA utility function throughout this chapter,
\begin{equation}
	\label{eq3:CRRA}
	u(x) = \frac{x^{(1-r)}}{(1-r)} ,
\end{equation}
\noindent and $w_i(p)$ is the decision weight applied to option $a$ defined as
\begin{equation}
	\label{eq3:dweight}
	w_c(p) =
	\begin{cases}
		\omega\left(\displaystyle\sum_{k=c}^C p_k\right) - \omega\left(\displaystyle\sum_{k=c+1}^C p_k\right) & \text{for } c<C \\
		\omega(p_c) & \text{for } c = C
	\end{cases}
\end{equation}
\noindent where $\omega(\cdot)$ is a probability weighting function and $\omega(\cdot)$ are decision weights.
In cases where $\omega(p_c) = p_c$, the RDU structure is equivalent to Expected Utility Theory (EUT) as the decision weights for each option will equal their objective probabilities, $p$.
Many parametrized probability weighting functions allow for this special case to occur.

Combining the RDU structure with a CRRA utility function, we can define the {\CE} as follows:
\begin{align}
	\label{eq3:CEcalc}
	\begin{split}
		G(\beta_i,X_a) &= \sum_{c=1}^{C} w_c(p) \frac{x_{ca}^{(1-r)}}{(1-r)} = \frac{ {\CE}_a^{(1-r)}}{(1-r)}\\
		{\CE}_a &=  \left( (1-r) \times \sum_{c=1}^{C} w_c(p) \frac{x_{ca}^{1-r}}{(1-r)} \right)^{ \displaystyle\nicefrac{1}{(1-r)} }
	\end{split}
\end{align}
\noindent where $c$ indexes the $C$ outcomes of option $a$ in task $t$.

I continue the notation from Chapter 2 where the value of $a$ also represents each option's ordinal rank among the alternative options in task $t$.
Thus $X_1 \succcurlyeq X_2$ and $X_a \succcurlyeq X_b$, where $b \geq a$.
Similarly, we define the set of unchosen options from the full set of alternatives as $Z = t \,\backslash\, y = \{z \in t \;|\; z \notin y \}$, with the subscript on the elements of $Z$ indicating their ordinal rank in the set of $Z$.
Thus $X_1^Z \succcurlyeq X_2^Z$ and $X_a^Z \succcurlyeq X_b^Z$, where $b \geq a$.

The probability of any choice $a$ by some subject $i$, given some vector of parameters $\beta$, being observed for a task $t$, is denoted by $\Pr( y_t = a)$, where $y_t = a$ is an indicator function that records option $a$ as being chosen in task $t$.
To make explicit the dependency of this probability on the option in question, the subject, the task, and the $\beta$ vector, this relationship will be re-framed as follows:
\begin{equation}
	\label{eq3:Piat}
	P_{iat}(\beta_i) = \Pr(y_t = a)
\end{equation}

The likelihood of observing a series of choices is the product of the probability of observing the option chosen for each task across all tasks, $T$ :
\begin{equation}
	\label{eq3:PiT}
	P_{iT}(\beta_i) =  \prod_{t}^{T} P_{iat}(\beta_i)
\end{equation}
\noindent This is the standard likelihood function applied to binary choice data.
We could take the log of equation (\ref{eq3:PiT}) and conduct standard maximum likelihood estimation (MLE) by searching for the vector $\hat{\beta}_i$ which maximizes the log-likelihood function:
\begin{equation}
	\label{eq3:LPiT}
	\mathit{LP}_{iT}(\beta_i) = \sum_{t}^{T} \ln \left( P_{it}(\beta_i) \right)
\end{equation}
\noindent Thus, the maximum likelihood estimator $\hat{\beta}_i$ for subject $i$ is:
\begin{equation}
	\label{eq3:Bn}
	\hat{\beta}_i = \underset{x}{\operatorname{arg\,max}}\sum_t^T \ln \left( P_{it}(\beta_i) \right)
\end{equation}

We can utilize this estimator to recover the {\CE} for every option in every task, and then utilize these \CE s to recover our best estimate of the proportion of welfare the subject obtained.
While conducting welfare analysis given individually estimated parameter vectors is rare in the economics literature,\footnote{
	Examples of this kind of analysis are \textcite{Harrison2016, Harrison2017} and \textcite{Harrison2017a}.
} the recovery of parameter vectors through MLE is relatively common.
\textcite{Hey1994}, \textcite{Wilcox2015} and \textcite{Hey2001} provide several examples of parameter estimation.
These particular examples, however, are distinctly different from other uses of MLE in experimental economics, primarily because equation (\ref{eq3:Bn}) is estimated for every subject individually, as opposed to pooling all subject data together and estimating a parameter vector for one, representative agent (RA), as proposed in the pioneering \textcite{Camerer1994}.

%% EDIT
% Make sure the below Harrison and Rutstrom quote is correct, and the citation is for the correct paper

There are legitimate methodological (and practical) reasons for modeling choices across subjects as the choices of a single RA.
For instance, the analyst could be primarily concerned with the economic characteristics of the whole sample, rather than with the individuals composing the sample.
As shown in \textcite[63]{Harrison2008}, it is easy to allow the $\hat{\beta}$ to be determined by a linear combination of observable characteristics of the subjects and/or experimental treatments.
For instance, if the race, gender and age of each of the subjects were known, we could estimate:
\begin{equation}
	\label{eq3:BB}
	\bm{\hat{\beta}} = \hat{\beta}_0 + \hat{\beta}_1 \times \mathit{race} + \hat{\beta}_2 \times \mathit{gender} + \hat{\beta}_3 \times \mathit{age}
\end{equation}
\noindent where $\hat{\beta}_1$ through $\hat{\beta}_3$ represent the mean marginal effects\footnote{
	\enquote{Marginal} with reference to the default set of characteristics captured by the constant $\hat{\beta}_0$.
} of race through age respectively on the vector $\bm{\hat{\beta}}$.

%% EDIT
% Make sure the below quote is correct

Another useful technology demonstrated by \textcite{Harrison2009} for RA modeling is the use of finite mixture modeling.
This is when a finite mixture of stochastic specifications are estimated jointly on the same data along with mixture parameters.
For instance,
\begin{align}
	\label{eq3:PT_Mix}
	\begin{split}
		\bm{\mathit{P_T}} = \prod_t^T \left[ \sum_m^M \pi_m \times L_T^m(\beta^m) \right]\\
		\mathit{st.} \sum_m^M \pi_m = 1
	\end{split}
\end{align}
\noindent where $\pi_m$ is the proportion of model $m$ in the mixture, $\beta$ is the vector of parameters to be estimated in model $m$ and $L_T^m$ is the likelihood of the choice data across the $T$ tasks explained by model $m$ given the vector $\beta^m$.
Similarly, the log-likelihood for finite mixture models is defined as:
\begin{align}
	\label{eq3:LPT_Mix}
	\begin{split}
		\bm{\mathit{LP_T}} = \sum_t^T \left[ \ln \left( \sum_m^M \pi_m \times L_T^m(\beta^m) \right) \right]\\
		\mathit{st.} \sum_m^M \pi_m = 1
	\end{split}
\end{align}
\noindent Thus $M$ $\beta^m$ vectors and $M-1$ $\pi_m$ scalars need to be estimated.
These parameters can additionally each be determined by observed characteristics, as in equation (\ref{eq3:BB}).
This method can be useful if the analyst wishes to estimate the proportion of a sample which more closely adheres to RDU versus EUT for instance, or if the analyst wants to determine if there is some heterogeneity in the sample that is revealed by choice, but unobservable otherwise.
\textcite[141]{Harrison2009} use this method to jointly estimate a specification composed of Prospect Theory (PT) and EUT.
They employ a Strong Utility (SU) stochastic model to generate the probabilities.
Although there does not appear to be any literature doing so, it is possible to estimate a mixture of two differing stochastic models.
For instance, an analyst could use a mixture model to determine what proportion of subjects in a dataset are better characterized by the SU or TR models.\footnote{
	This process could be used to help with the econometric limitations of the pure RP model, since those subjects who violate FOSD can be picked up by an alternative model which permitted such violations.
	This process, of course, doesn't resolve the RP model's normative failures discussed in chapter 2.
}

There are also some methodological problems, or at least limitations, when conducting estimation on pooled data.
The estimates represent the means of the relevant parameters in the sample, but the distributions of these parameters and whether these distributions are correlated can potentially provide more important information to analysts.\footnote{
	For an example of why it could be problematic to make inferences about a population from an estimate which represents the mean of a distribution of preferences consider a population that has preferences distributed as $\textit{Logit-Normal} \sim \mathcal{N}(0,5)$.
	Logit-Normal is a distribution in which the logistic function, $\Lambda$, is applied to the realization of a Normal distribution $N(\mu, \sigma^2)$.
	See Figure 2 of \textcite[83]{Andersen2012}.
	This distribution is highly bi-modal, and the area around the mean of the distribution has very low density.
	Thus, if a single stochastic specification is estimated on a sample from this population, the estimated parameters representing their distributional means give highly misleading information about the choice behavior we would expect from individual agents sampled from this population.
	In this case a mixture model of two models could potentially identify the modes, thus providing more, but still limited, information about the population.
	A similar approach is utilized by \textcite{Conte2011}.
}
While the methods described in equations (\ref{eq3:BB}) and (\ref{eq3:PT_Mix}) provide some insight into the heterogeneity of a pooled sample, this is mostly limited to estimating average deviations from the mean due to observable heterogeneity.
While it is theoretically possible to have a mixture model with greater than two underlying stochastic specifications, in reality this is computationally demanding and thus the mixture model presented in (\ref{eq3:PT_Mix}) is often only utilized with two mixtures.

Estimating parameter vectors for every subject in a sample helps to improve on this limitation, as can \enquote{random coefficients,} discussed below.
If every subject has an individually estimated parameter vector, then an analyst can use the distribution of these estimates to approximate the distribution of parameter vectors of the population from which this sample was drawn.
However, the individually estimated parameters are still estimates, and thus they all have associated standard errors and positive probabilities of misidentification.
The likelihood of misidentification typically decreases with the number of choice tasks presented to subjects, just as standard errors are negatively correlated with sample size.
\textcite{Hey1994} estimate parameters for individual subjects utilizing 100 choice tasks per subject in order minimize the potential for misidentification.
\textcite{Hey2001} utilized 500 choice problems per subject.

However, conducting experiments where subjects are required to give responses to a large number of tasks has practical problems, which then spill over and generate theoretical problems.
Subjects can become bored or tired, which may make the tasks less salient or cause them to fail to satisfy the dominance criteria described by \textcite{Smith1982} and \textcite{Harrison1992}.
Often experimenters utilize a random lottery incentive mechanism (RLIM) in experiments, selecting one choice by the subject at random for payment.
While in theory this is incentive compatible with EUT, it is not necessarily incentive compatible with any utility theory that doesn't require the independence axiom (IA), such as RDU \parencite{Harrison2014, Cox2015}.
Furthermore, each additional choice task presented to the subject dilutes the expected outcomes of the other choice tasks.
This means that the task could fail the dominance criteria unless the outcomes are sufficiently scaled up, even if the outcomes and the payment mechanism are salient.
Thus, when the experimenter implements the RLIM for practical reasons, such as not needing to resolve and then compensate a subject for all of potentially hundreds of choices, he potentially introduces a serious theoretical concern.

These qualifications to estimation of individual parameter vectors should not be considered fatal for this method, but they should be noted when conducting this kind of estimation.
\textcite{Hey2001} split the 500 choice tasks over 5 days to help mitigate the potential for subjects to become bored.
Other experimenters split the $T$ lottery tasks into smaller sets of tasks which are split by other, potentially unrelated, tasks.
These kinds of designs help mitigate the procedural problems with such estimation, though sometimes they may introduce other concerns.
While subjects may be less bored by doing choice tasks over 5 days rather than all on 1 day, subjects may experience events in between sessions that change their beliefs about the lottery pairs presented during the sessions.

An alternative method to recover greater information on entire samples of agents is to estimate the distributions of the parameter vectors describing individual preferences directly from pooled data.\footnote{
	\textcite{Andersen2012} discuss the application of these well-known econometric methods to the estimation of standard models of risk (and time) preferences.
}
Instead of estimating preference parameters, the parameters which shape the distributions of preferences in the population are estimated.
We can call equation (\ref{eq3:Piat}), which is at the heart of equations (\ref{eq3:PiT}) through (\ref{eq3:LPT_Mix}), a conditional probability, because the probability is conditional on a particular $\beta$ vector.
We can however weight this function by the likelihood of observing the $\beta$ vector from a given distribution.\footnote{
	It is worth noting the relation of these statements to a Bayesian approach.
	Having knowledge of the distribution of preferences in a population is akin to holding a prior in a Bayesian approach.
	This prior could then be incorporated to condition individual level estimates and produce an individual level choice probability.
	This Bayesian technique is different from the two approaches discussed here.
	The individual level approach discussed here does not incorporate a distributional prior in its estimation process, while the unconditional approach generates choice probabilities directly from pooled data, not individual data.
}
We call this weighted probability the unconditional probability:
\begin{equation}
	\label{eq3:Pit}
	P_{it}(\theta) = \int P_{it}(\beta_i) f(\beta | \theta) d\beta
\end{equation}
\noindent where $f(\beta|\theta)$ is the density function of the $\beta$ vector given some vector of hyper-parameters $\theta$ shaping the distribution of the $\beta$.

This unconditional probability can be substituted for the conditional probability used in equations (\ref{eq3:PiT}) and (\ref{eq3:LPiT}) to give us the unconditional likelihood equation:
\begin{equation}
	\label{eq3:LiT}
	L_{iT}(\theta) = \prod_t^T P_{it}(\theta)
\end{equation}
\noindent and its counterpart, the unconditional log-likelihood equation:
\begin{equation}
	\label{eq3:LLiT}
	\mathit{LL}_{iT}(\theta) = \sum_t^T \ln \left( P_{it}(\theta) \right)
\end{equation}

Equations (\ref{eq3:Pit}) through (\ref{eq3:LLiT}) are computationally impossible to estimate directly due to the general \enquote{inability of computers to perform integration} for non-trivial distributions in a closed-form \parencite[2]{Train2002}.
However, equation (\ref{eq3:Pit}) can be approximated by simulation as follows:
\begin{equation}
	\label{eq3:SPit}
	\mathit{SP}_{it}(\theta) = \mathlarger{\sum}_h^H \frac{ P_{it}(\beta^h) }{H}
\end{equation}

Equation (\ref{eq3:SPit}) needs some explanation.
The integration involved in equation (\ref{eq3:Pit}) is approximated by taking $H$ random draws of $\beta^h$ from the distribution governed by $\theta$, evaluating equation (\ref{eq3:Piat}) with each of these $H$ randomly drawn $\beta^h$, and taking a simple average across these $H$ evaluations.
Only a simple average is needed because if the $\beta^h$ vectors are drawn at random from the distribution governed by $\theta$, then the likelihood of their occurrence is already weighted by the distribution's density.

The use of $H$ as the term characterizing draws from a distribution is not arbitrary.
It indicates that the random draws will often be approximated by a Halton sequence of numbers.
The Halton routine is a numerical method to produce a sequence of numbers which efficiently approximate random draws from a uniform distribution bounded between 0 and 1, and which has been shown to provide better coverage of the distribution than other pseudo-random\footnote{
	All \enquote{random} numbers generated by computers are in fact \enquote{pseudo-random} numbers produced algorithmically.
	\textcite[234]{Train2002} describes these numerical routines as follows:
	\enquote{The intent in \textins{the} design \textins{of pseudo-random routines} is to produce numbers that exhibit the properties of random draws.
		The extent to which this intent is realized depends, of course, on how one defines the properties of \enquote{random} draws.
		These properties are difficult to define precisely since randomness is a theoretical concept that has no operational counterpart in the real world.}
	Because of the non-existence of truly \enquote{random} number generators, the term \enquote{random} will be used in place of \enquote{pseudo-random} throughout this text.
} number generators.\footnote{
	See the remainder of \textcite[Chapter~9]{Train2002} for an in-depth discussion and derivation of why Halton sequences are widely viewed as being superior to many other pseudo-random number generators for the purposes of simulating estimators, at least when the dimensionality of the estimation problem is small.
}

The Halton sequence of uniformly distributed numbers can be transformed into a sequence of randomly drawn numbers from any invertible, univariate distribution.
That is, if $\mu$ is taken to be a random variable indicating a draw from a uniform distribution, and $F(\epsilon)$ is an invertible, univariate, cumulative distribution, then given $\mu$, draws of $\epsilon$ from this distribution can be obtained by solving $\epsilon = F^{-1}(\mu)$.
\textcite[236]{Train2002} discusses this method for obtaining random draws from invertible, univariate distributions, as well as using Choleski transformations to obtain draws from multivariate normal distributions.

With this simulated unconditional probability, we can obtain the simulated unconditional likelihood by substituting equation (\ref{eq3:SPit}) for equation (\ref{eq3:Pit}) in equation (\ref{eq3:LiT}):
\begin{equation}
	\label{eq3:SLiT}
	\mathit{SL}_{iT}(\theta) = \prod_t^T \left[ \mathlarger{\sum}_h^H \frac{ P_{it}(\beta^h) }{H} \right]
\end{equation}

Equation (\ref{eq3:SLiT}) is limited in terms of identifying $\theta$ because, as indicated by the $i$ subscript, this metric is defined for a single agent.
Since the normatively coherent stochastic models discussed in Chapter 2 have non-random elements composing $\beta_i$, there is no distribution of $\beta_i$ to be estimated from a single agent's choices.
The real power of this method is realized, however, when sample data are pooled together and the distribution of $\beta_i$ vectors is estimated from this pooled data.
This is an easy extension of equation (\ref{eq3:SLiT}), which is logged for numerical reasons:
\begin{equation}
	\label{eq3:SLLNT}
	\mathit{SLL}_{NT}(\theta) = \sum_{i=1}^N \left( \sum_t^T \left[ \ln\!\left( \sum_h^H \frac{ P_{it}(\beta^h) }{H} \right) \right] \right)
\end{equation}
\noindent We call equation (\ref{eq3:SLLNT}) the unconditional simulated log-likelihood function, or just the simulated log-likelihood function (SLL).
Maximum simulated likelihood (MSL) methods can be applied to this equation to return the MSL estimator $\hat{\theta}$ which maximizes this function.
The characteristics of simulated estimators are reviewed in depth by \textcite[Chapter~10]{Train2002}, and the critical insight is that the estimator $\hat{\theta}$ derived from equation (\ref{eq3:SLLNT}) approaches the estimator from equation (\ref{eq3:LLiT}) with a sufficiently large, $H$, number of draws from the distribution governed by $\theta$.

Estimating the distribution of preferences for a sample with MSL may improve the analyst's position over RA models with pooled data.
The limitation of estimating only the conditional mean preference parameter for pooled data with standard MLE is no longer binding.
Flexible distributions such as the Logit-Normal\footnote{
	\textcite[82]{Andersen2012} utilize the Logit-Normal distribution because of its high degree of flexibility and because \enquote{MSL algorithms developed for univariate or multivariate Normal distributions can be applied directly.}
	The figures they present \parencite*[83]{Andersen2012} display some of the flexible forms this distribution can take.
} can be employed to estimate higher moments of the distribution such as the variance, skewness and kurtosis.
Additionally, the individual elements of $\theta$ can be modeled as linear functions of observable covariates, as was done in equation (\ref{eq3:BB}) for pooled MLE.
This added flexibility allows the analyst to have greater information about preferences at the sample level, and can also be used to make characterizations of welfare at the individual level.
On the other hand, MSL routines are computationally intensive, and become even more so when MSL mixture models are estimated.

\singlespacing
\section{The \texorpdfstring{\textcite{Holt2002}}{Holt and Laury (2002)} MPL and the Unconditional Assessment of Expected Welfare}
\doublespacing

Each of the econometric methods detailed above provide some information about economics agents, either at the individual or collective level, and each have their own strengths and limitations.
Issues concerning statistical power and identification for individual level estimation will be discussed in more depth in Chapter 4.
In the discussion below, I detail the usefulness of knowledge of the distributions of preferences in a population, given by $\theta$, but not the methods and limitations of estimating $\hat{\theta}$ via MSL or some other estimation procedure.
The results presented below are therefore numerical approximations of statistics for candidate values of $\theta$, not estimates of statistics given an estimated $\hat{\theta}$.
The formulae presented below for $\theta$ could be extended to incorporate standard errors associated with he elements of $\hat{\theta}$, but additional assumptions about the sampled population would need to be made.
In this section I demonstrate that knowledge of the distribution of preferences in a population, given by $\theta$, can provide useful information about the expected welfare of individual agents from this population for any given pattern of choices.

To make this discussion more concrete, we can utilize one of the HL-MPL instruments alluded to earlier and displayed in Table \ref{tb:HL-MPL}.
In the HL experiment subjects were presented with this table, without the \enquote{Expected Payoff Difference} and \enquote{CRRA for Indifference} columns, and asked to select one option from each row.
The \enquote{Option A} column indicates the outcomes and associated probabilities for option A in each of 10 tasks, and similarly for the \enquote{Option B} column.
The \enquote{CRRA for Indifference} column indicates the CRRA value that would make an EUT agent indifferent between option A and option B.
Thus, an agent with a CRRA value of $0.5$ would theoretically select option A for rows 1-6, and then \enquote{switch} to selecting option B for the remaining rows.

\begin{table}[ht]
	\centering
	\captionsetup{justification=centering}
	\caption{The Ten Paired Lottery-Choice Decisions with Low Payoffs \newline \textcite[1645]{Holt2002} }
	\label{tb:HL-MPL}
	\begin{adjustbox}{width=1\textwidth}
	\pgfplotstabletypeset[
		col sep=colon,
		every head row/.style={
			after row=\hline
		},
		display columns/0/.style={
			string type,
			column name = {\cnline{Row \#}}
		},
		display columns/1/.style={
			string type,
			column name = {Option A}
		},
		display columns/2/.style={
			string type,
			column name = {Option B}
		},
		display columns/3/.style={
			string type,
			column name = {\cnline{Expected Value\\Difference}}
		},
		display columns/4/.style={
			string type,
			column name = {\cnline{CRRA for\\Indifference}}
		},
	]{tables/HL-MPL.csv} % path/to/file
	\end{adjustbox}
\end{table}

The popularity of this approach is in part due to its straightforward logic:
if a subject conforms to a deterministic EUT specification, then she should start off selecting option A, then at some point switch once, and only once, to selecting option B for the remaining rows or she should select B for every row.
The point at which the subject switches reveals an interval in which preference for risk must lie, at least under EUT.

However, this pattern need not necessarily occur given stochastic specifications.
Subjects may, and sometimes do, switch multiple times between option A and option B as they work their way down the rows.
Some subjects even select option A in row 10, despite it being dominated by option B.
The first of these observed choice behaviors is often referred to as multiple switching behavior (MSB), while the second is a form of FOSD since there is no risk involved in row 10.
\textcite[1647]{Holt2002} observe that 28 of their 212 subjects exhibited MSB.
Rather than discussing all of the potential reasons why a subject would exhibit MSB, we will assume a normatively coherent stochastic model and discuss the implications of MSB within it.

The HL-MPL instrument is a useful instrument to discuss the welfare implications of stochastic models because the observed MSB is an apparent violation of EUT that is easy to notice visually without estimation.
As discussed in Chapter 2, when utilizing normatively coherent stochastic models, observed violations of EUT necessarily imply that some welfare has been forgone by the agent because of the violation according to EUT.
In Savage's terms, one would have something to \enquote{reproach} oneself for by violating the theory \parencite[230]{Moscati2016}.
Since there is no deterministic EUT utility function which allows either the switching back and forth from option A to option B or the selection of a guaranteed, lower outcome over a guaranteed, higher outcome, it must be the case that the observance of MSB implies that some welfare has been forgone, at least under an EUT framework.

An important and often overlooked reality of stochastic models is that even if a subject doesn't display MSB, the subject may still have made choice errors and therefore have foregone some amount of welfare.
This may not seem obvious at first, since any non-MSB choice pattern can be rationalized by some preference relation.
Cases such as these arise when a subject makes a choice error with respect to the utility function they employ, but this choice error results in a choice pattern that is still rationalizable, or \enquote{consistent.}
When we incorporate knowledge of a sample's distribution of preferences governed by $\theta$, we can see that many observed, apparently \enquote{consistent} choice patterns contain more choice errors and are often more costly in terms of foregone welfare than apparently \enquote{inconsistent} choice patterns.
This will be made clear in the discussion below, but first we must define some notation.

Utilizing notation from the beginning of Section \ref{ssec:Notation}, an option in a set of alternatives $t$ is represented as $X_{at}$, where $a$ indicates the option's ordinal rank among the set of alternatives given the agent's utility parameter vector, $\beta_i$, and $y_t = a$ indicates that option $a$ was chosen by the agent in task $t$.
We can define a \enquote{choice error} as any choice where the option chosen was not ordinally ranked the highest among the set of alternatives with respect to the agent's preferences, given by $\beta_i$.
Therefore a choice error in task $t$ is when $y_t \neq 1$ (recall the description of subscripts given for equation (\ref{eq3:CEcalc})), and an indicator function for choice errors given some vector of assumed utility parameters $\beta_i$ is given by:
\begin{equation}
	\label{eq3:Itb}
	K_{t}(\beta_i) =
	\begin{cases}
		 1 & y_t \neq 1\\
		 0 & y_t = 1
	\end{cases}
\end{equation}
\noindent The frequency of choice errors by agent $i$ in the choice pattern $y_t \times T$ is:
\begin{equation}
	\label{eq3:MTBn}
	M_T(\beta_i) = \sum_t^T K(\beta_i)
\end{equation}
\noindent Given the distribution parameter vector $\theta$, we can define the expected frequency of choice errors in the choice pattern $y_t \times T$ as:
\begin{equation}
	\label{eq3:EMt}
	\E(M | \theta) = \int M(\beta_i) f(\beta | \theta) d\beta
\end{equation}
\noindent where, just as in equation (\ref{eq3:Pit}), $f(\beta|\theta)$ is the density function of the $\beta$ vector given the vector of hyper-parameters $\theta$ shaping the distribution of the $\beta$.
Equation (\ref{eq3:EMt}) is just the mean of the discrete distribution of choice errors in in the choice pattern $y_t \times T,$ given the distribution parameter vector $\theta$.
Because the distribution of choice errors is discrete, $M(\beta_i) \in [0,T] \subset \mathbb{N}^0$, we can define the probability mass function of choice errors as follows:\footnote{
	$\mathbb{N}^0$ indicates the set of natural numbers, inclusive of $0$. $\mathbb{N}^1$ or $\mathbb{N}^{+}$ would indicate the set of natural numbers not inclusive of 0.
}
\begin{equation}
	\label{eq3:PE}
	P_E(e | \theta) = \int N[M(\beta),e] f(\beta|\theta) d \beta
\end{equation}

\noindent where
\begin{equation}
	\label{eq3:NMB}
	N[M(\beta), e] =
	\begin{cases}
		1 & M(\beta) = e\\
		0 & M(\beta) \neq e
	\end{cases}
\end{equation}
\noindent and $e$ indicates the number of choice errors for the given choice pattern and $\theta$ vector.
Equation (\ref{eq3:PE}) provides useful information about whether an observed pattern deviates from a deterministic choice model, but is limited since it assigns equal weight to errors which are very costly in terms of welfare and errors that are not so costly.

We can incorporate two of the metrics developed in Chapter 2 for welfare assessment into this sample framework.
The first metric, calculated for a choice pattern $y_t \times T$, is equivalent to a standard consumer surplus calculation:
\begin{equation}
	\label{eq3:WST}
	\Delta W_{iT} = \sum_{t=1}^T \left( {\CE}_{iyt} - {\CE}_{i1t}^Z \right)
\end{equation}
\noindent where ${\CE}_{iyt}$ is the {\CE} of the option chosen, indicated by the subscript $y$, by agent $i$ in task $t$, and ${\CE}_{i1t}^Z$ is the {\CE} of the option that is ordinally ranked the highest among the set of \textit{unchosen} alternatives, $Z$, with respect to the agent's preferences, given by $\beta_i$, in task $t$.
Throughout this chapter, we will refer to the metric in equation (\ref{eq3:WST}) as the \enquote{welfare surplus} metric.
The second metric we propose to characterize the welfare implications of choices is similar to the concept of auction and market \enquote{efficiency} proposed by \textcite{Plott1978}:
\begin{equation}
	\label{eq3:WET}
	\%W_{iT} = \frac{\displaystyle\sum_{t=1}^{T} {\CE}_{iyt} }{\displaystyle\sum_{t=1}^{T} {\CE}_{i1t}}
\end{equation}
\noindent In the metric defined in equation (\ref{eq3:WET}), the {\CE}'s of the options chosen by the agent across all tasks $T$ are summed, and then divided by the {\CE}'s of the options that were ordinally ranked the highest with respect to the agent's preferences across all the tasks.
Therefore, should an agent never make a choice error, this metric would take on the value of $1$, and should the agent make at least one choice error, it would take on a value between $0$ and $1$.\footnote{
	There are a few mathematical peculiarities with this metric.
	This metric can lose its $(0,1)$ bounds if any of the $T$ tasks has a mixed frame, that is, a task that has both positive and negative outcomes.
	This metric would become negative if the {\CE} of a chosen option is negative and the {\CE} of the highest ranked option is positive.
	Also, this metric becomes undefined if the {\CE} of the highest ranked alternative is $0$.
	These general issues will not be of concern in this chapter because all examples of lotteries have outcomes in the strictly positive domain.
}
Throughout this chapter, we will refer to the metric in equation (\ref{eq3:WET}) as the \enquote{welfare efficiency} metric.

The welfare surplus and welfare efficiency metrics from equations (\ref{eq3:WST}) and (\ref{eq3:WET}) can be used in place of equation (\ref{eq3:MTBn}) in equation (\ref{eq3:EMt}) to gather useful information for a given choice pattern and $\theta$ vector:
\begin{align}
	E( \Delta W_T | \theta) &= \int \Delta W_T(\beta) f(\beta | \theta) d \beta \label{eq3:EWST}\\
	E( \% W_T | \theta) &= \int \% W_T(\beta) f(\beta | \theta) d \beta \label{eq3:EWET}
\end{align}

Given equation (\ref{eq3:NMB}), we can denote the expected welfare surplus and the expected welfare efficiency obtained by agents who have committed $e \in [0,T]$ errors by making choices $y_t \times T$ as follows:
\begin{align}
	E( \Delta W_T | \theta, e) &= \int \bigr( \Delta W_T(\beta) \times N[M(\beta),e] \bigr) f(\beta | \theta) d \beta \label{eq3:EDWTe}\\
	E( \% W_T | \theta, e) &= \int \bigl( \% W_T(\beta) \times N[M(\beta),e] \bigr) f(\beta | \theta) d \beta \label{eq3:EPWTe}
\end{align}

The same limitation mentioned about MSL concerning a computer's inability to perform closed-form integration in general applies to equations (\ref{eq3:EMt}), (\ref{eq3:PE}), and (\ref{eq3:EWST}) through (\ref{eq3:EWET}).
However, these equations can be approximated in the manner described for MSL in equation (\ref{eq3:SPit}): the terms in these equations between the integrand and the density function will be evaluated with $\beta$ vectors randomly drawn $H$ times from the distribution governed by $\theta$, and then averaged.
As $H$ gets sufficiently large, the simulated statistics approach the true statistics.

\singlespacing
\subsection{Sample Level Analysis with an EUT Population}
\doublespacing

The simulation methods described here and for the remainder of this chapter characterize an individual agent as having a single $\beta_i$ vector representing her preferences, and making choices in an economic environment that satisfies the \textcite{Smith1982} precepts for valid economic experiments.
An individual agent $i$ generates an \textit{observed} choice pattern $y_t \times T$ by resolving the stochastic process defined by her preferences.
In Chapter 2 we described normatively coherent stochastic models as those models that characterize agents as having non-random preferences, thus an agent's preferences do not change from choice to choice.\footnote{
	Not only are we assuming that agents do not have random preferences, we're also assuming that an agent's preferences are the same across choices generally.
	We could, as \textcite{Hey2001} does, model some or all of the parameters in an agent's utility function as being partly determined by the number of choices that the agent has encountered.
	Because preferences modeled in this way change from choice to choice in a non-random manner, the welfare analysis discussed in this chapter could be extended in a normatively coherent manner to incorporate this \enquote{learning,} potentially with interesting implications.
	This would involve specifying additional marginal distributions which characterize the parameters defining the \enquote{learning} process.
}
Individual $\beta_i$ parameter vectors are themselves drawn from a population of $\beta$ vectors.
This distribution of $\beta$ vectors in the population is characterized by the parameter vector $\theta$.
Throughout the following discussion, we will refer to a choice pattern's likelihood of being \textit{observed}, by which we mean the choice pattern's simulated likelihood as calculated in equation (\ref{eq3:SLiT}).
This is the probability of observing a choice pattern given that it has been generated by an agent randomly drawn from the population defined by $\theta$.
%This is the probability of a randomly drawn agent from the population defined by $\theta$ producing the choice pattern, thus the pattern's likelihood of being observed.
Likewise, when we discuss the expected welfare implications of a choice pattern for a given population, we are discussing the expected welfare implications for an agent from that population who generated that choice pattern.

To construct an explicit numerical example, we first define the models characterizing an individual agent's choice probabilities, and then the marginal distributions of the elements of $\beta$ which together define the population characterized by $\theta$.
For the sake of simplicity, we first consider a population entirely composed of agents conforming to an EUT utility model with a Contextual Utility (CU) stochastic model due to \textcite{Wilcox2008}.
Thus, choice probabilities for an individual agent are defined as follows:
\begin{align}
	\label{eq3:RE}
	\begin{split}
	P_{iat}(\beta_i) &= {\Prob}\left(  \epsilon_t \geq \frac{1}{D(\beta_i,X_t) \lambda_i} \left[ G(\beta_i,X_{kt}) - G(\beta_i,X_{jt}) \right] \right)\\
	&= 1 - F\left( \dfrac{G(\beta_i,X_{kt}) - G(\beta_i,X_{jt})}{D(\beta_i,X_t)\lambda_i }  \right)\\
	&= {\Prob}(y_t = a)
	\end{split}
\end{align}
\noindent where $\epsilon_t$ defines the random error associated with the measurement of utility, the functional form of the utility function, $G(\cdot)$, is the CRRA function of the form $u(x) = \frac{x^{1-r}}{(1-r)}$, $F(\cdot)$ is the logistic cumulative distribution function (cdf), and the adjusting function $D(\cdot)$ is as follows:
\begin{align}
	\label{eq3:CU}
	\begin{split}
		&D(\beta_i,X_t) = \mathit{max}[u(x_{it})] - \mathit{min}[u(x_{it})]\\
		&\mathit{st.}\; w_i(x_{it}) \neq 0
	\end{split}
\end{align}
\noindent Thus the $\beta_i$ vector for each agent is said to consist of only two parameters, $r$ and $\lambda$.
The joint distribution of these two parameters characterizes the population of agents and is characterized by the parameter vector $\theta$.
We assume the marginal distributions of the $r$ and $\lambda$ parameters to be independent and uncorrelated in the population.\footnote{
	This is done for convenience; adding correlation among the marginal distributions would require the specification of a covariance matrix.
	In samples of real populations, we might expect there to be correlation among these marginal distributions, and this analysis can be easily extended to accommodate it.
	This additional step is not difficult, but introduces more parameters to keep track of and doesn't significantly add to the narrative.
}
The $r$ parameter can conceivably take any value, but to make the bulk of the density lie in the familiar range of the literature employing the HL-MPL instrument, we assume it to be distributed normal, with mean of $0.65$ and a standard deviation of $0.3$, thus $r \sim \mathcal{N}(0.65 , 0.3^2 )$.
The $\lambda$ parameter must be strictly positive, so it will be assumed to be distributed as gamma with a mean of $0.35$ and a standard deviation of $0.3$.
This is equivalent to a gamma distribution with a shape parameter of $k \approx 1.36$ and a scale parameter of $t\approx0.26$, thus $\lambda \sim \Gamma(1.36 , 0.26)$.
Together these 4 parameters make the joint distribution-shaping parameter $\theta=\{0.65 ,0.3^2, 1.36 , 0.26\}$.

The metrics described in equations (\ref{eq3:SLiT}) and (\ref{eq3:MTBn}) through (\ref{eq3:EPWTe}) rely on a given choice pattern, $y_t \times T$.
In the HL-MPL instrument there are a total of $2^{10}=1024$ choice patterns that can be observed.
To begin the discussion of the welfare implications of stochastic choice models, we calculate the values for equations (\ref{eq3:SLiT}), and (\ref{eq3:MTBn}) through (\ref{eq3:EPWTe}) for all $\mathit{TT} =1024$ choice patterns and all $e \in[0,T]$ for the given $\theta$, with $H=2.5 \times 10^6$.

%%%%%
%%%%%
%EDIT: NEED TO CHANGE THE ABOVE
%%%%%
%%%%%

To make clear how the result of these equations are arrived at, we can work through the calculations step by step.
First, we select a choice pattern from one of the $1024$ choice patterns possible with the HL-MPL instrument, for example, the choice of option A for the first five rows and option B for rows $6$ through $10$.
Next a $\beta_i$ vector is drawn from the joint distribution defined by $\theta$.
As an example, we assume $\beta_i = \lbrace r = 0.65, \lambda = .35\rbrace$ was drawn; recall that we are also assuming EUT with a CU stochastic model.
Utilizing this $\beta_i$ and choice pattern, we can evaluate the various metrics proposed.
First, we evaluate equation (\ref{eq3:PiT}), the likelihood that agent $i$ would produce this choice pattern, utilizing equation (\ref{eq3:RE}) to calculate choice probabilities for the individual tasks:

\begin{align}
	\label{eq3:example_PiT}
	\begin{split}
		P_{i,a,1}  = Pr(y_1 = A    \,|\, \beta_i)    &= 0.82 \\
		P_{i,a,2}  = Pr(y_2 = A    \,|\, \beta_i)    &= 0.78 \\
		P_{i,a,3}  = Pr(y_3 = A    \,|\, \beta_i)    &= 0.74 \\
		P_{i,a,4}  = Pr(y_4 = A    \,|\, \beta_i)    &= 0.68 \\
		P_{i,a,5}  = Pr(y_5 = A    \,|\, \beta_i)    &= 0.62 \\
		P_{i,a,6}  = Pr(y_6 = B    \,|\, \beta_i)    &= 0.44 \\
		P_{i,a,7}  = Pr(y_7 = B    \,|\, \beta_i)    &= 0.51 \\
		P_{i,a,8}  = Pr(y_8 = B    \,|\, \beta_i)    &= 0.57 \\
		P_{i,a,9}  = Pr(y_9 = B    \,|\, \beta_i)    &= 0.63 \\
		P_{i,a,10} = Pr(y_{10} = B \,|\, \beta_i)    &= 0.95 \\
		P_{iT}     = \prod_{t = 1}^{T = 10} P_{iat}(\beta_i)  &= 0.0154
	\end{split}
\end{align}
\noindent Note that $P_{i,a,6} = 0.44 < 0.50$.
With the CU stochastic model, given in equation (\ref{eq3:RE}), the option with the greatest utility, expected or otherwise, will always have the greatest probability of being chosen.
Since there are only two alternatives in row \#6, and the choice probability of option $B$ is less than that of option $A$, it must be the case that option $B$ in this row had a lower expected utility than option $A$.
If $B$ has a lower expected utility than $A$, the choice of $B$ in row \#6 is a choice error.
Using the notation defined in Section \ref{ssec:Notation}, $y_6 = 2$, and for all $t \in \lbrace T \,\backslash\, 6 \rbrace$, $y_t = 1$.
This information allows us to evaluate equation (\ref{eq3:MTBn}), the frequency of choice errors in a given choice pattern, utilizing equation (\ref{eq3:Itb}):
\begin{align}
	\label{eq3:example_MTBn}
	\begin{split}
		P_{i,a,1}  = Pr(y_1 = A    \,|\, \beta_i) = 0.82 ~ \Rightarrow ~ K_{1}(\beta_i)  &= 0 \\
		P_{i,a,2}  = Pr(y_2 = A    \,|\, \beta_i) = 0.78 ~ \Rightarrow ~ K_{2}(\beta_i)  &= 0 \\
		P_{i,a,3}  = Pr(y_3 = A    \,|\, \beta_i) = 0.74 ~ \Rightarrow ~ K_{3}(\beta_i)  &= 0 \\
		P_{i,a,4}  = Pr(y_4 = A    \,|\, \beta_i) = 0.68 ~ \Rightarrow ~ K_{4}(\beta_i)  &= 0 \\
		P_{i,a,5}  = Pr(y_5 = A    \,|\, \beta_i) = 0.62 ~ \Rightarrow ~ K_{5}(\beta_i)  &= 0 \\
		P_{i,a,6}  = Pr(y_6 = B    \,|\, \beta_i) = 0.44 ~ \Rightarrow ~ K_{6}(\beta_i)  &= 1 \\
		P_{i,a,7}  = Pr(y_7 = B    \,|\, \beta_i) = 0.51 ~ \Rightarrow ~ K_{7}(\beta_i)  &= 0 \\
		P_{i,a,8}  = Pr(y_8 = B    \,|\, \beta_i) = 0.57 ~ \Rightarrow ~ K_{8}(\beta_i)  &= 0 \\
		P_{i,a,9}  = Pr(y_9 = B    \,|\, \beta_i) = 0.63 ~ \Rightarrow ~ K_{9}(\beta_i)  &= 0 \\
		P_{i,a,10} = Pr(y_{10} = B \,|\, \beta_i) = 0.95 ~ \Rightarrow ~ K_{10}(\beta_i) &= 0 \\
		                                M(\beta_i) = \sum_{t = 1}^{T = 10}{K_t(\beta_i)} &= 1
	\end{split}
\end{align}
\noindent Thus we see that our subject $i$ has committed one choice error across the $T$ tasks, the choice error in row \#6.
This result allows us to calculate equation (\ref{eq3:NMB}) for values of $e \in [ 0, T ]$.
Equation (\ref{eq3:NMB}) is just an indicator function that signals if the frequency of choice errors in the choice pattern, given by equation (\ref{eq3:Itb}), is equal to some scalar $e$.
Thus, if we know the result of equation (\ref{eq3:Itb}), which for the example shown above is 1, we know that the result of equation (\ref{eq3:NMB}) will be 1 for $e = 1$, and 0 for all other values of $e$.
\begin{align}
	\label{eq3:example_NMB}
	\begin{split}
		N( M_T(\beta_i) = 1, e = 0 )  &= 0 \\
		N( M_T(\beta_i) = 1, e = 1 )  &= 1 \\
		N( M_T(\beta_i) = 1, e = 2 )  &= 0 \\
		N( M_T(\beta_i) = 1, e = 3 )  &= 0 \\
		N( M_T(\beta_i) = 1, e = 4 )  &= 0 \\
		N( M_T(\beta_i) = 1, e = 5 )  &= 0 \\
		N( M_T(\beta_i) = 1, e = 6 )  &= 0 \\
		N( M_T(\beta_i) = 1, e = 7 )  &= 0 \\
		N( M_T(\beta_i) = 1, e = 8 )  &= 0 \\
		N( M_T(\beta_i) = 1, e = 9 )  &= 0 \\
		N( M_T(\beta_i) = 1, e = 10 ) &= 0 \\
	\end{split}
\end{align}
\noindent Next we can calculate the two welfare metrics from equations equations (\ref{eq3:WST}) and (\ref{eq3:WET}), indicating welfare surplus and welfare efficiency respectively.
First we calculate the {\CE} of option $A$ and option $B$ for all $T = 10$ tasks using equation (\ref{eq3:CEcalc}).
We note the {\CE} of the chosen and unchosen options for the given choice pattern, the difference between the two, and the greatest {\CE} of the two options for each task:

\begin{table}[ht]
	\centering
	\setlength{\tabcolsep}{1pt}
	\caption{ Example {\CE}'s of EUT Agent with HL-MPL}
	\label{tb:example_CE}
	\begin{adjustbox}{width=1\textwidth}
	\pgfplotstabletypeset[
		col sep=comma,
		every head row/.style={
			after row=\hline
		},
		every last row/.style={
			after row=\hline
		},
		display columns/0/.style={
			column type={c},
			column name = {Task}
		},
		display columns/1/.style={
			precision = 2,
			zerofill,
			column name = { \cnline{ {\CE} of A} }
		},
		display columns/2/.style={
			precision = 2,
			zerofill,
			column name = { \cnline{ {\CE} of B } }
		},
		display columns/3/.style={
			precision = 2,
			zerofill,
			column name = { \cnline{ {\CE} of Chosen\\Option } }
		},
		display columns/4/.style={
			precision = 2,
			zerofill,
			column name = { \cnline{ {\CE} of Unchosen\\Option} }
		},
		display columns/5/.style={
			precision = 2,
			fixed,
			zerofill,
			column name = { \cnline{ {\CE} of Chosen -\\{\CE} of Unchosen} }
		},
		display columns/6/.style={
			precision = 2,
			zerofill,
			column name = { \cnline{Greatest \\{\CE}}}
		}
	]{tables/Example_CE.csv} % path/to/file
	\end{adjustbox}
\end{table}

With the {\CE}'s calculated, we can substitute them in to equations (\ref{eq3:WST}) and (\ref{eq3:WET}).
For equation equation (\ref{eq3:WST}), we take the sum of column $6$ in Table \ref{tb:example_CE}:
\begin{equation}
	\label{eq3:example_WST}
	\Delta W_{iT} = \sum_{t=1}^T \left( {\CE}_{iyt} - {\CE}_{i1t}^Z \right) = 8.92
\end{equation}
\noindent and for equation (\ref{eq3:WET}), we take the sum of column $4$ and divide it by the sum of column $7$:
\begin{equation}
	\label{eq3:example_WET}
	\%W_{iT} = \frac{\displaystyle\sum_{t=1}^{T} {\CE}_{iyt} }{\displaystyle\sum_{t=1}^{T} {\CE}_{i1t}} = \frac{21.37}{21.74} = .983
\end{equation}
\noindent Finally, we multiply the welfare metrics derived in equations (\ref{eq3:example_WST}) and (\ref{eq3:example_WET}) by the indicator functions derived for $e \in [0,T]$ in equation (\ref{eq3:NMB}):
\begin{align}
	\label{eq3:example_NMBWST}
	\begin{split}
		N( M_T(\beta_i) = 1, e = 1 )  \times \Delta W_{iT} &= 1 \times 8.92 = 8.92\\
		N( M_T(\beta_i) = 1, e \neq 1 )  \times \Delta W_{iT} &= 0 \times 8.92 = 0
	\end{split}
\end{align}
\begin{align}
	\label{eq3:example_NMBWET}
	\begin{split}
		N( M_T(\beta_i) = 1, e = 1 )  \times \%W_{iT} &= 1 \times .983 = .983\\
		N( M_T(\beta_i) = 1, e \neq 1 )  \times \%W_{iT} &= 0 \times .983 = 0
	\end{split}
\end{align}
\noindent The indicator functions in equation (\ref{eq3:example_NMB}) are mutually exclusive, therefore the product of the indicator functions and the welfare metrics will be $0$ for all but one value of $e$, and equal to the welfare metrics for the remaining $e$, in this example, for $e = 1$.

%Thus, the derivation of the results in equations (\ref{eq3:example_PiT}) through (\ref{eq3:example_WET}) constitutes the core of the computational exercise that results in population level expectations.
Having derived the results of these equations for one given choice pattern, we iterate through the remaining $1023$ choice patterns for this particular agent, repeating the numerical exercise described above for each choice pattern.
With metrics defined for this particular agent across all $TT = 1024$ possible choice patterns, a new $\beta_i$ vector is drawn from $\theta$, and the entire process repeated.
For the calculations described below, we repeat the process of drawing a $\beta_i$ from $\theta$ and calculating the results of these metrics for all choice patterns $S = 2.5 \times 10^6$ times.
This process results in a $3$ dimensional array with $(\mathit{\#\ of\ metrics} \times \mathit{\#\ of\ choice\ patterns} \times S) = 33 \times 1024 \times (2.5 \times 10^6) = 8.448 \times 10^{10}$ elements.

To arrive at the population level metrics, we take the average of each metric defined in equations (\ref{eq3:example_PiT}) through (\ref{eq3:example_NMBWET}) across all $S$ simulated agents for each choice pattern.
Since each $\beta_i$ was drawn randomly from the distribution governed by $\theta$, only a simple average is needed.
This averaging leaves us with a dataset that has $33 \times 1024 = 33,792$ elements.

This resulting dataset, however, is too large to be usefully displayed in full, so for now we restrict attention to the 10 choice patterns most likely to be observed in a population governed by $\theta$, and discuss the metrics calculated in equations (\ref{eq3:EMt}), (\ref{eq3:EWST}), (\ref{eq3:EWET}), and (\ref{eq3:PE}) with $e = (0,1)$.
The results of these equations for the 10 most likely choice patterns to be observed in a population governed by $\theta$ are given in Table \ref{tb:TopTenEUT}.

\begin{table}[h]
	\setlength{\tabcolsep}{2pt}
	\centering
	\caption{HL-MPL Welfare and Error Expectations for Choice Patterns with Top Ten Simulated Likelihoods, EUT}
	\label{tb:TopTenEUT}
	\begin{adjustbox}{width=1\textwidth}
	\pgfplotstabletypeset[
		col sep=comma,
		every head row/.style={
			before row={
				\multicolumn{1}{c}{} &
				\multicolumn{10}{c}{Choice in Row} &
				\cnline{Simulated\\Likelihood} &
				\cnline{Expected\\Errors} &
				\cnline{Welfare\\Efficiency} &
				\cnline{Welfare\\Surplus} &
				$P_E(e=0)$ &
				$P_E(e=1)$\\
			},
			after row=\hline
		},
		every last row/.style={
			after row=\hline
		},
		display columns/0/.style={
			column name = {Rank},
			column type={c}
		},
		display columns/1/.style={
			column name = {1},
			column type={|p{.3cm}}
		},
		display columns/2/.style={
			column name = {2},
			column type={p{.3cm}}
		},
		display columns/3/.style={
			column name = {3},
			column type={p{.3cm}}
		},
		display columns/4/.style={
			column name = {4},
			column type={p{.3cm}}
		},
		display columns/5/.style={
			column name = {5},
			column type={p{.3cm}}
		},
		display columns/6/.style={
			column name = {6},
			column type={p{.3cm}}
		},
		display columns/7/.style={
			column name = {7},
			column type={p{.3cm}}
		},
		display columns/8/.style={
			column name = {8},
			column type={p{.3cm}}
		},
		display columns/9/.style={
			column name = {9},
			column type={p{.3cm}}
		},
		display columns/10/.style={
			column name = {10},
			column type={p{.4cm}|}
		},
		display columns/11/.style={
			precision = 4,
			fixed,
			%sci precision = 2,
			zerofill,
			column name = {}
		},
		display columns/12/.style={
			precision = 3,
			zerofill,
			sci precision = 3,
			column name = {}
		},
		display columns/13/.style={
			precision = 4,
			sci precision = 3,
			zerofill,
			column name = {}
		},
		display columns/14/.style={
			precision = 2,
			sci precision = 3,
			zerofill,
			column name = {}
		},
		display columns/15/.style={
			precision = 3,
			sci precision = 3,
			column name = {}
		},
		display columns/16/.style={
			precision = 3,
			sci precision = 3,
			zerofill,
			column name = {}
		}
	]{tables/TopTenEUT.csv} % path/to/file
	\end{adjustbox}
\end{table}

For the \enquote{Choice in Row} column in Table \ref{tb:TopTenEUT}, $0$ indicates a choice of A for the row, and $1$ indicates a choice of B.
Note that the choice pattern that is mostly likely to be observed from a sample drawn from the specified population governed by $\theta$, shown in the first row where \textit{Rank} is $1$, is the choice pattern we would observe from an agent described by a deterministic choice process with preferences at the mean of the distribution of $r$.
The next two most likely choice patterns, where \textit{Rank} is $2$ and $3$, correspond to the choice pattern we would observe from agents described by a deterministic choice process with preferences one standard deviation either side of the mean of the distribution of $r$.

Interestingly, for each of the three most likely choice patterns, it is far more likely than not that an agent displaying these choice patterns made at least one choice error, and thus did not obtain maximal welfare from her choices.
This is shown by the values in column $P_E(e=0)$, which reference equation (\ref{eq3:PE}), all being less than $0.50$, indicating that less than 50\% of subjects will commit 0 choice errors for these choice patterns.
Note that only $32.2\%$ of agents who display the most likely choice pattern in row $1$ are expected to have \textit{not} made any choice errors and therefore obtain maximal welfare.
This is despite the fact that any of these choice patterns can be rationalized by some set of preferences for our assumed model.
These patterns do, however, produce relatively high expected welfare efficiency and surplus.
The welfare surplus metric is less informative in this comparison: it is more useful in making absolute rather than relative statements about welfare.

The relatively large values of $ 1 - P_E(e = 0)$, which imply that most choice patterns contain at least $1$ choice error, is mainly due to the shape and location of the distribution of $r$.
The mean of $r$, at $0.65$, lies just next to the indifference boundary between rows 6 and 7 of the HL-MPL instrument, as indicated in the column \enquote{CRRA for Indifference} of Table \ref{tb:HL-MPL}.
That means that the bulk of the $r$ values drawn from this distribution define utility values that indicate near indifference between the A and B lotteries in row 7 of the HL-MPL instrument.
All RE models increase the probability of a choice error the closer an agent is to being indifferent between 2 options, so it should not be a surprise that with this particular choice of distribution for $r$ we have a large proportion of choice errors.
Similarly, since many agents in this population would be nearly indifferent between the A and B lotteries in row 7 of the HL-MPL, the expected cost of the choice errors in row 1 in welfare terms is realtively low, as can be seen by the value in the \enquote{Welfare Efficiency} being very close to 1.

The fourth and fifth most likely choice patterns in Table \ref{tb:TopTenEUT}, where \textit{Rank} is $4$ and $5$, are not consistent with any deterministic EUT preferences.
These patterns display what we will call \enquote{Light MSB}: not including the choice made in row 10, the agent has \enquote{switched} between choosing A and B three times.\footnote{
	The reason that row 10 is not included in this definition is because we are making a distinction between patterns which do and do not include a choice of A in row 10 later.
}
Because MSB is not consistent with any deterministic EUT preferences, $P_E(e=0)=0$ for these patterns.
In fact, the only choice patterns in which $P_E(e=0)>0$ will be those which are \enquote{Consistent}: displaying a choice pattern that can be rationalized by some deterministic EUT preferences.

Despite the patterns in rows 4 and 5 of Table \ref{tb:TopTenEUT} being obviously inconsistent with a deterministic EUT process, they both are more likely to be observed from agents drawn from a population defined by $\theta$, and obtain greater welfare surplus than the sixth most likely choice pattern which is \enquote{Consistent.}
The likelihood of the \enquote{Light MSB} choice patterns in rows 4 and 5, displayed in the \enquote{Simulated Likelihood} column, are greater than the likelihood of the choice pattern in row 6, which is consistent.
The welfare efficiency metric for row 5, displayed in the \enquote{Welfare Efficiency} column, is greater than that of row 6, and the welfare surplus metrics for both rows 4 and 5 are greater than for row 6.
Since metrics for all $TT = 1024$ choice patterns were calculated, we will see in the discussion below that these two Light MSB patterns are both more likely to be observed and to be less costly in terms of welfare surplus than 6 out of 10 \enquote{Consistent} patterns.

Another interesting aspect of this analysis is the correlation of welfare and the likelihood of observing a choice pattern.
The correlation between the simulated likelihood of the choice patterns and their expected welfare efficiency is $0.62$ across the whole dataset, while the simulated likelihood and expect welfare surplus has a correlation of $0.68$.
These are positive but far from 1.
That is, as the likelihood of observing a choice pattern increases, the expected welfare efficiency and surplus of the choice pattern generally increases as well, but not always.
This is apparent in rows 8 and 9 of Table \ref{tb:TopTenEUT}.
The choice pattern described in row 8 is more likely to be observed than the pattern in row 9, but the pattern in row 9 has a higher expected welfare efficiency than row 8, though not by much.
The very large number of draws employed in these calculations rules out the possibility that this is a statistical fluke caused by the random way these statistics were calculated.

This example illustrates how stochastic models are not \enquote{welfare ranking} models, but instead incorporate aspects of the choice process that are considered to be normatively desirable, while maintaining descriptive power.
The example of row 8 and 9 only depicts the most common occurrence where the expected welfare efficiency of a pattern and its likelihood diverge in this hypothetical population.
The most drastic divergence occurs between the patterns which have violated FOSD by selecting option A in row 10, and those that have not.

To make this distinction clear, Figure \ref{fig:ConFOSD} plots the log of the SL (SLL) against the expected welfare efficiency of the choice patterns that:
\begin{itemize}
 \setlength\itemsep{-.5em}
	\item are Consistent with deterministic EUT,
	\item are Consistent other than the choice of A in row 10 (FOSD Only),
	\item display Light MSB, the agent has \enquote{switched} between choosing A and B three times, with a choice of B in row 10,
	\item display Light MSB with a choice of A in row 10 (Light MSB + FOSD).
\end{itemize}

\begin{figure}[h!]
	\caption{Consistent and Light MSB, With and Without Row 10 Error}
	\includegraphics[width=\linewidth]{figures/SamPlots/EUT-ConFOSD.pdf}
	\label{fig:ConFOSD}
\end{figure}

In Figure \ref{fig:ConFOSD} each point represents a unique choice pattern.
For any given point plotted, any other point to the Southeast of that point indicates a pattern that \textit{is both more likely to be produced by an agent drawn randomly from this population and provides lower expected welfare efficiency.}
For instance, any point in the shaded region of Figure \ref{fig:ConFOSD} represents a choice pattern that is both more likely to be observed and has a lower expected welfare efficiency than pattern Y.

Figure \ref{fig:ConFOSD} shows that the choice of A in row 10 greatly decreases the SLL of the pattern, but barely decreases the expected ratio of obtained welfare to maximal welfare, all else being equal.
For example, the most likely consistent choice pattern is the top right-most red dot in Figure \ref{fig:ConFOSD}, labeled \enquote{X,} which corresponds to row 1 of Table 2 and has a welfare efficiency of 0.986 and a SL of 0.036.
The most likely choice pattern with a choice of A in row 10 is the top right-most green dot, labeled \enquote{Y.}
This pattern is identical to the \enquote{X} pattern other than the selection of A in row 10 and has a welfare efficiency of 0.938 and a SL of 0.00246.
The ratio of welfare obtained to maximum welfare differs only by 0.0483, but pattern X is about 14.63 times more likely to be observed than pattern Y.\footnote{
	Probability of X $\div$ Probability of Y = 0.036 $\div$ 0.00246 = 14.63
}
The seventh most likely consistent pattern, not displayed in Table 1, can be seen as the red dot in Figure \ref{fig:ConFOSD} labeled \enquote{Z} and is about 1.55 times more likely to be observed than pattern Y and has an expected welfare efficiency that is about 0.065 \textit{lower} than pattern Y.

The general implication of this exercise is to make it clear that stochastic models do not reliably link the likelihood of a choice pattern with its realized welfare as consumer surplus or efficiency.
This is due to the way in which heteroscedastic RE models disproportionately \enquote{punish} FOSD in welfare terms by assigning occurrences of it a very low likelihood.
The choice of A in row 10 is punished in welfare terms even more by the fact that there is no risk involved.

Empirically, experimental economists rarely observe behavior such as the choice of A in row 10 because the agents they study are in environments that incentivize them to reject dominated offers.
There is, by definition, no extra benefit to actively choosing a dominated offer.
But just because there is no extra benefit to be had, it shouldn't be inferred that the agent doesn't value the dominated option positively.
Any agent who selected option A in row 10 still receives a \$2 benefit from having had the choice problem presented to her if that choice is selected for payment.

There is a disconnect between how stochastic models map welfare and probability when considering individual choices versus patterns of choice.
When considering an individual choice RE models create a perfect mapping of welfare and probability; an option that is more likely to be chosen from a set of alternatives than another option always also provides greater welfare.
However, as we can see from Figure \ref{fig:ConFOSD} and Table \ref{tb:TopTenEUT}, when considering patterns of choice, this mapping breaks down, and it is no longer the case that a more likely pattern of choices also provides greater welfare.

\singlespacing
\subsection{Sample Level Analysis with a Mixed EUT-RDU Population}
\doublespacing

The above discussion focuses on a population that is entirely composed of EUT conforming agents.
Individual level estimates from \textcite{Hey1994} and the mixture model estimates from \textcite{Harrison2009} show that many populations are likely not composed entirely of EUT agents.
We can extend the example above, defining the population as being composed of some mixture of EUT agents and RDU agents.
By \enquote{mixture,} we mean that there will be two subpopulations of a grand population, with agents employing either the EUT or RDU functions.

Before beginning the analysis of this mixture population, we can extend the metrics utilized in equations (\ref{eq3:SLiT}) and (\ref{eq3:MTBn}) through (\ref{eq3:EPWTe}) to be defined for mixed populations.
This is implemented in much the same way as mixture models were defined in equation (\ref{eq3:PT_Mix}); each metric, $Q_m$, for subpopulation $m$ is weighted by the proportion of the subpopulation in the grand population, $M$.
\begin{align}
	\label{eq3:Metric_Mix}
	\begin{split}
		\bm{\mathit{Q^M}} = \sum_m^M \pi_m \times Q^m \\
		\mathit{st.} \sum_m^M \pi_m = 1
	\end{split}
\end{align}
\noindent where $\pi_m$ is the proportion of subpopulation $m$ in the grand population.
For example, the probability of observing any given choice pattern $y \times T$ for a grand population made of M subpopulations is:
\begin{align}
	\label{eq3:LnT_Mix}
	\begin{split}
		\bm{L_{iT}^M} = \sum_m^M \pi_m \times L_{iT}^m(\theta^m) \\
		\mathit{st.} \sum_m^M \pi_m = 1
	\end{split}
\end{align}
\noindent where $L_{iT}^m$ is as described in equation (\ref{eq3:LiT}) for some subpopulation $m$ defined by $\theta^m$.

A final metric before considering the example of the mixed population is the probability that any given choice pattern was produced by population $m$.
Utilizing equation (\ref{eq3:LnT_Mix}) we define the probability that a pattern was produced by population $m$ as the ratio of the weighted simulated likelihood of observing the pattern from subpopulation $m$ to the likelihood of observing the pattern in the grand population:
\begin{equation}
	\label{eq3:Propm}
	\mathit{Prop^m_{T}} = \frac{\pi_m \times L_{iT}^m(\theta^m) }{\bm{L_{iT}^M}}
\end{equation}

With this mixing framework in mind, we can define our grand population.
We assume that $70\%$ of agents in the grand population conform to EUT, while the remaining $30\%$ conform to RDU.
Given that the previous example thoroughly examined an EUT population, rather than duplicate the analysis, we assume that the EUT subpopulation is the same as the previous EUT-only example.
Thus, the EUT subpopulation is defined as using a CU stochastic model and CRRA function with the $r$ parameter normally distributed $r \sim \mathcal{N}(0.65 , 0.3^2 )$ and the $\lambda$ parameter following a gamma distribution $\lambda \sim \Gamma(1.36 , 0.26)$.
This results in $\theta^{EUT} = \lbrace 0.65 ,0.3^2, 1.36 , 0.26\rbrace$.

For the RDU subpopulation, we employ the flexible 2 parameter decision weighting function defined by \textcite{Prelec1998} as the probability weighting function to be substituted into equation (\ref{eq3:dweight}):
\begin{equation}
	\label{eq3:pw:pre}
	\omega(p)=\exp(-\beta(-\ln(p))^\alpha)
\end{equation}
\noindent where $\alpha > 0$ and $\beta > 0$.
We continue to use the CRRA utility function and CU stochastic model for the RDU subpopulation.

The $r$ parameter is assumed to be distributed identically to the $r$ parameter in the EUT population $r \sim \mathcal{N}(0.65 , 0.3^2 )$, and the $\lambda$ parameter still uses the gamma distribution, but is distributed $\lambda \sim \Gamma(0.563 , 0.26)$, which results in the mean of the $\lambda$ distribution at $0.15$ and a standard deviation of $0.2$.\footnote{
	With the mass of the $\lambda$ distribution closer to $0$, \textit{a priori} we should expect fewer choice errors among the RDU population than the EUT population.
}
Both the $\alpha$ and $\beta$ parameters for the decision weight function must be greater than $0$, so they will also be assumed to be distributed with a gamma distribution, $\alpha \sim \Gamma(169 , 7.69 \times 10^{-3})$ and $\beta \sim \Gamma(144 , 8.33 \times 10^{-3})$.
Thus the mean of $\alpha$ is $\approx 1.3$ and its standard deviation is $\approx 0.1$, and the mean of $\beta$ is $\approx 1.2$ and its standard deviation is $\approx 0.1$.
This results in $\theta^{RDU} = \lbrace  0.65 ,0.3^2,  0.563 , 0.26 , 169 , 7.69 \times 10^{-3} , 144 , 8.33 \times 10^{-3} \rbrace$.

Once again, we employ $2.5 \times 10^6$ draws from this joint distribution times and calculate the values for equations (\ref{eq3:SLiT}) and (\ref{eq3:MTBn}) through (\ref{eq3:EPWTe}) for all $\mathit{TT} =1024$ choice patterns and all $e \in[0,T]$ for the RDU subpopulation.
With the results of the calculations for the EUT subpopulation calculated previously, and the results of the same calculations for the RDU population, we can mix each of these metrics as described in equation (\ref{eq3:Metric_Mix}) with $\pi_{EUT} = 0.7$ and $\pi_{RDU} = 0.3$.
Again, it is impractical to display the results of all metrics for all $1024$ choice patterns, so first, we recreate Table \ref{tb:TopTenEUT} with the results of the RDU metrics.

\begin{table}[ht]
	\centering
	\caption{HL-MPL Welfare and Error Expectations for\\Top Ten Choice Patterns, RDU}
	\label{tb:TopTenRDU}
	\begin{adjustbox}{width=1\textwidth}
	\pgfplotstabletypeset[
		col sep=comma,
		every head row/.style={
			before row={
				\multicolumn{1}{c}{} &
				\multicolumn{10}{c}{Choice in Row} &
				\cnline{Simulated\\Likelihood} &
				\cnline{Expected\\Errors} &
				\cnline{Welfare\\Proportion} &
				\cnline{Welfare\\Surplus} &
				$P_E(e=0)$ &
				$P_E(e=1)$\\
			},
			after row=\hline
		},
		every last row/.style={
			after row=\hline
		},
		display columns/0/.style={
			column name = {Rank},
			column type={c}
		},
		display columns/1/.style={
			column name = {1},
			column type={|p{.3cm}}
		},
		display columns/10/.style={
			column name = {10},
			column type={p{.3cm}|}
		},
		display columns/11/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/12/.style={
			precision = 3,
			sci precision = 3,
			column name = {}
		},
		display columns/13/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/14/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/15/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/16/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		}
	]{tables/TopTenRDU.csv} % path/to/file
	\end{adjustbox}
\end{table}

There is a great deal of similarity between Table \ref{tb:TopTenRDU} and Table \ref{tb:TopTenEUT}.
In particular, the two subpopulations share the same $3$ most likely choice patterns, though with different simulated likelihood, welfare, and error metrics.
Again we note that the choice patterns which display Light MSB, rows 4, 5, 6, 8, 9, and 10, have $0$ probability of containing $0$ choice errors, and that there are a few examples of the disconnect between likelihood and welfare.
The Light MSB choice pattern in row 8 is expected to contain fewer choice errors than the Consistent pattern in row 7.
The Light MSB choice pattern in row 9 is expected to provide greater welfare surplus than the Consistent pattern in row 7.
Additionally, we observe that going from row 6 to 7 the Simulated Likelihood decreases, but the Welfare Proportion metric increases.

A major difference between the two subpopulations is that the RDU subpopulation's most likely choice pattern has a much greater likelihood (0.1617) than the EUT subpopulation's most likely choice pattern (0.0360).
Much of this is due to the greater mass of the $\lambda$ distribution close to $0$ in the RDU subpopulation compared to the EUT subpopulation, but it is also because the distributions chosen for the decision weighting parameters imply greater risk aversion.
This means that although the CRRA coefficients lie near the boundary of row 6 and 7 of the HL-MPL instrument, the way the RDU subpopulation weights probabilities makes them more risk averse, and therefore more likely to switch at row 7 than if they did not weight probabilities.
If instead of utilizing the mixture of EUT and RDU shown here, we utilized a mixture of two EUT subpopulations, with one subpopulation being more risk averse than the other, we would see similar results.
Agents from a more risk averse population of EUT agents would switch at later rows than agents from the less risk averse EUT population, just as agents from the RDU population discussed here switch at later rows than agents from the less risk averse EUT population.
These differences are important when we look at the grand population metrics.

\begin{table}[ht]
	\centering
	\caption{HL-MPL Welfare and Error Expectations for\\Top Ten Choice Patterns, EUT-RDU Mixture}
	\label{tb:TopTenMIX}
	\begin{adjustbox}{width=1\textwidth}
	\pgfplotstabletypeset[
		col sep=comma,
		every head row/.style={
			before row={
				\multicolumn{1}{c}{} &
				\multicolumn{10}{c}{Choice in Row}&
				\cnline{Proportion\\EUT}&
				\cnline{Simulated\\Likelihood}&
				\cnline{Expected\\Errors}&
				\cnline{Welfare\\Proportion}&
				\cnline{Welfare\\Surplus}&
				$P_E(e=0)$\\
			},
			after row=\hline
		},
		every last row/.style={
			after row=\hline
		},
		display columns/0/.style={
			column name = {Rank},
			column type={c}
		},
		display columns/1/.style={
			column name = {1},
			column type={|p{.3cm}}
		},
		display columns/10/.style={
			column name = {10},
			column type={p{.3cm}|}
		},
		display columns/11/.style={
			precision = 2,
			sci precision = 3,
			column name = {}
		},
		display columns/12/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/13/.style={
			precision = 3,
			sci precision = 3,
			column name = {}
		},
		display columns/14/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/15/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/16/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		}
	]{tables/TopTenMIX.csv} % path/to/file
	\end{adjustbox}
\end{table}

The grand population metrics displayed in Table \ref{tb:TopTenMIX} are barely noteworthy by themselves.
They easily could have been generated by a population composed entirely of EUT agents with a distribution of $\lambda$ somewhat closer to $0$ than the EUT subpopulation that actually composes $70\%$ of the agents in this population.
Many of the same features of the two subpopulations are apparent in the mixed grand population;
choice patterns displaying any form of MSB have $0$ likelihood of $0$ choice errors, and there are some disconnects between simulated likelihood and welfare as observed in rows $5$-$6$, and $7$-$8$ for the welfare efficiency metric and rows $9$-$10$ for the welfare surplus metric.

Of greater interest is the \enquote{Proportion EUT} column of Table \ref{tb:TopTenMIX}, defined by equation (\ref{eq3:Propm}).
This metric calculates the unconditional likelihood that a subject displaying a particular choice pattern belongs to the EUT subpopulation we defined.
For every choice pattern in the top ten most likely to be observed choice patterns, we observe that the proportion of the agents belonging to the EUT subpopulation that generated the choice pattern is smaller than the proportion of EUT agents in the grand population.
For the top 3 choice patterns, the difference between the proportion of EUT agents \textit{in the total population} and the proportion of EUT agents \textit{that generated the choice pattern} is greater than 20 percentage points.
In fact, it is more likely than not that these choice patterns are generated by the RDU subpopulation.
This is despite the fact that the EUT subpopulation makes up $70\%$ of the grand population, and that the top three most likely to be observed choice patterns in the grand population all correspond to the same top three choice patterns in the EUT subpopulation.

It should also be clear from Table 2 and Figure \ref{fig:ConFOSD} that not all choice patterns that are consistent with EUT should be judged as superior to choice patterns which are apparently inconsistent with EUT from the perspective of welfare realization as consumer surplus or welfare efficiency.

\singlespacing
\section{Population Level Analysis of Welfare: Preferences, Noise, and the Instrument}
\doublespacing

The proposed characterizations of the welfare of a sample, including the degree to which certain consistent choice patterns are expected to be more costly in welfare terms than inconsistent choice patterns, are ultimately determined by the distribution of preferences and stochastic parameters in the sample.
To analyze how the welfare characterizations change as the distribution of preferences change in the sample, we could repeat the computational exercise that led to Table \ref{tb:TopTenEUT} for a few different distributions and discuss implications pattern by pattern.
This exercise, however, will produce data only for the populations chosen, and will be less informative about how expectations of welfare change as the population changes.
Instead, it will be useful to define a few population-level metrics that allow us to look at the data at the aggregate level.
For instance, for each $y \times T$ choice pattern, we can weigh the expected welfare efficiency resulting from equation (\ref{eq3:EWET}) by the simulated likelihood resulting from equation (\ref{eq3:SLiT}) and then sum across all TT choice patterns to retrieve the sample expected welfare efficiency:
\begin{equation}
	\label{eq3:EPWTT}
	\E(\%W_T(\theta)) = \sum_{tt=1}^{TT} \mathit{SL}_{Ntt}(\theta) \times \E(\%W_{tt} | \theta)
\end{equation}

Similar expectations can be derived for any of the per-choice pattern statistics defined previously, but we pay particular interest to the statistics derived from equations (\ref{eq3:EMt}), and (\ref{eq3:PE}) where $e=0$.
We are not limited to looking at expectations however: we can utilize equation (\ref{eq3:EPWTT}) to derive higher moments of these statistics, such as the variance:
\begin{equation}
	\label{eq3:VPWTT}
	\Var(\%W_T(\theta)) = \sum_{tt=1}^{TT} \mathit{SL}_{Ntt}(\theta) \times \left[ \E(\%W_{tt} | \theta) - \E(\%W_T | \theta) \right]^2
\end{equation}

Having the means and variances of the statistics described allows us to make high-level inferences about the welfare implications of an instrument like the HL-MPL instrument on different populations for a given stochastic model.
That is, we can contribute to answering of our primary question of \enquote{what are the welfare implications of stochastic models} by solving equations (\ref{eq3:EPWTT}) and (\ref{eq3:VPWTT}) for various values of $\theta$ and relating the elements of $\theta$ to these results.
We can substitute any  $y \times T$ statistic derived from equations (\ref{eq3:EMt}), (\ref{eq3:PE}), (\ref{eq3:EWET}), and (\ref{eq3:EPWTe}) in place of $\%W_T$ in equations (\ref{eq3:EPWTT}) and (\ref{eq3:VPWTT}) to describe these statistics on a population by population basis.

While equations (\ref{eq3:EPWTT}) and (\ref{eq3:VPWTT}) may in fact have analytical solutions to determine these relationships, meaning we could attempt to solve for the partial derivative of equations (\ref{eq3:EPWTT}) and (\ref{eq3:VPWTT}) with respect to each element of $\theta$, any analytical solution will be unique with respect to so many idiosyncratic factors that this becomes infeasible and potentially uninformative.
These factors include:
\begin{itemize}
 \setlength\itemsep{-.25em}
	\item the stochastic model;
	\item the utility model;
	\item the location, dispersion and shape of the joint distribution governing the complete stochastic specification;
	\item the number of Halton draws used to simulate the probabilities;
	\item the base prime number used for the Halton sequences; and
	\item the specific tasks faced by the sample population.
\end{itemize}
\noindent Given these limitations, we instead examine the relationship of the parameters making up the stochastic specification, i.e. the elements of $\theta$, with the associated results of equations (\ref{eq3:EPWTT}) and (\ref{eq3:VPWTT}) visually and with the use of locally weighted polynomial regression (LOESS) developed by Cleveland, Grosse, Shyu, Chambers \& Hastie (1992).
To conduct this examination, we generate 500,000 unique population parameter sets, $\theta_i$, the elements of which are assumed to be uncorrelated, and solve equations (\ref{eq3:EPWTT}) and (\ref{eq3:VPWTT}) for the statistics derived in equations (\ref{eq3:EMt}), (\ref{eq3:PE}) and (\ref{eq3:EWET}) with $e=0$ and with each equation solved with $10,000$ draws from each unique population.

Each population has normally distributed preference parameters and gamma distributed stochastic error parameters.
Thus, each $\theta_i$ is comprised of 4 elements: the mean of the CRRA parameter and the $\lambda$ term, $\mu_r$ and $\mu_\lambda$, and standard deviation of the CRRA parameter and the $\lambda$ term, $\sigma_r$ and $\sigma_\lambda$. Each of the candidate $\theta_i$ vectors was randomly drawn from a joint uniform distribution of these elements.
The bounds of the marginal distributions of these elements are as follows: $\mu_r \in [-1.9 , 1.55 ]$, $\sigma_r \in [0 , 1]$, $\mu_\lambda \in [.05 , 2.25]$, $\sigma_\lambda \in [.01 , .75]$.
These bounds are almost arbitrary; the bounds for $\mu_r$ were chosen to be just outside the indifference bounds of the HL-MPL instrument, but the remaining marginal distributions were chosen to be broad enough to yield some interesting patterns.

This exercise results in 8 statistics for each $\theta_i$: the means and variances of the expected proportion of welfare to the maximum attainable welfare, the expected welfare surplus, the expected number of choice errors, and the expected proportion of agents who make no errors.
Each statistic can be plotted against the 4 elements of $\theta_i$.
The result is 32 plots of the raw data and 32 charts of the LOESS lines associated with the raw data plots.
All LOESS lines are plotted along with 95\% confidence intervals.

Each plot and chart also attempts to give information about another parameter not plotted on the $x$ or $y$ axes by color coding the plotted data with respect to different values of this \enquote{z} parameter.
For $\mu_r$ this \enquote{z} parameter is $\sigma_r$, for $\sigma_r$ it is $\mu_r$, for $\mu_\lambda$ it is $\sigma_\lambda$ and for $\sigma_\lambda$ it is $\mu_\lambda$.
For each of the charts, the \enquote{z} parameter is split into quartiles and for the LOESS line charts, LOESS lines are calculated for the \enquote{x} and \enquote{y} parameter values that belong to each quartile.
Additionally, in the raw data plots, each point has been given a large degree of transparency.
This means that the density of points in the plot is represented by the density of color in the plot.

I examine the LOESS line charts of these data.
First I discuss the effect on welfare expectations of the parameters governing the stochastic model, and then discuss the parameters governing the utility model.
Thus, we first look at Figures \ref{fig:S-Wel-um}, \ref{fig:S-Err-um}, \ref{fig:S-Wel-us} and \ref{fig:S-Err-us}.
Figures \ref{fig:S-Wel-um} and \ref{fig:S-Err-um} demonstrate the effect of the mean of the distribution of the $\lambda$ term on welfare and the error frequencies, while Figures \ref{fig:S-Wel-us} and \ref{fig:S-Err-us} demonstrate the effect of the standard deviation of the $\lambda$ term on the same statistics.

\begin{figure}[h!]
	\center
	\caption{Mean of $\lambda$ Compared to Welfare}
	%\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Wel-um.pdf}
	\includegraphics[width=\textwidth]{figures/AggPlots/S-Wel-um.pdf}
	\label{fig:S-Wel-um}
\end{figure}

\begin{figure}[h!]
	\center
	\caption{Mean of $\lambda$ Compared to Errors}
	%\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Err-um.pdf}
	\includegraphics[width=\textwidth]{figures/AggPlots/S-Err-um.pdf}
	\label{fig:S-Err-um}
\end{figure}

The results of the plots of stochastic model parameters are mostly intuitive and unsurprising.
Looking at Figures \ref{fig:S-Wel-um} and \ref{fig:S-Err-um}, as the mean of the distribution increases, the expected welfare and expected proportion of 0-error choice patterns monotonically decreases, while the expected number of choice errors monotonically increases.
Because $\lambda$ has a gamma distribution, for any given mean, a higher standard deviation implies that the mass of the distribution shifts closer towards $0$.
Thus, it is unsurprising that those populations with high standard deviations of $\lambda$ tend to exhibit choice patterns with fewer expected choice errors and greater expected proportions of no error choice patterns.
This is because for any given choice problem, a lower value of $\lambda$ implies a lower probability of committing a choice error.\footnote{
	Since $\lambda$ is in the denominator of each exponential transformation, as $\lambda \to 0$, ${\Prob}(y_t = a) \to 1$ for $a = 1$ and ${\Prob}(y_t = a) \to 0$ for $a\neq1$ regardless of the other parameters.
}
This directly translates into greater expected welfare than those populations with lower standard deviations holding the mean constant.

\begin{figure}[h!]
	\center
	\caption{Standard Deviation of $\lambda$ Compared to Welfare}
	%\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Wel-us.pdf}
	\includegraphics[width=\textwidth]{figures/AggPlots/S-Wel-us.pdf}
	\label{fig:S-Wel-us}
\end{figure}

\begin{figure}[h!]
	\center
	\caption{Standard Deviation of $\lambda$ Compared to Errors}
	%\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Err-us.pdf}
	\includegraphics[width=\textwidth]{figures/AggPlots/S-Err-us.pdf}
	\label{fig:S-Err-us}
\end{figure}

\begin{figure}[h!]
	\center
	\caption{Standard Deviation of CRRA Compared to Welfare}
	%\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Wel-rs.pdf}
	\includegraphics[width=\textwidth]{figures/AggPlots/S-Wel-rs.pdf}
	\label{fig:S-Wel-rs}
\end{figure}

\begin{figure}[h!]
	\center
	\caption{Standard Deviation of CRRA Compared to Errors}
	%\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Err-rs.pdf}
	\includegraphics[width=\textwidth]{figures/AggPlots/S-Err-rs.pdf}
	\label{fig:S-Err-rs}
\end{figure}

Looking at Figures \ref{fig:S-Wel-us} and \ref{fig:S-Err-us}, we can see that $\sigma_\lambda$ is far less influential than $\mu_\lambda$.
In the (A) and (C) charts of Figure \ref{fig:S-Wel-us}, the slopes of the LOESS lines are slightly positive, but mostly flat other than the line for the lowest quartile of $\mu_\lambda$.
In the (A) and (C) charts of Figure \ref{fig:S-Err-us}, we see much the same mostly flat lines indicating very little variation across the parameter space.
Again the exception is the line for the lowest quartile of $\mu_\lambda$.
This should not be surprising given that the populations were generated with a CU stochastic model.
The third quartile of $\mu_\lambda$ begins at $1.15$, which means that the majority of the mass of the distribution of lambda in any population will lie above 1 for any value of $\sigma_\lambda$ in the range explored.
At these high levels of lambda, most choice probabilities will converge to something close to $\Pr( y_t = a) \to 0.5$.
%This leaves little room for variation in expected welfare or expected choice errors.

%%%%%
% EDIT: This above sentence needs more consideration
%%%%%

In contrast to the monotonic relations of the lambda distribution, the effect of the CRRA parameters on the expected welfare and expected error statistics displays influences of the idiosyncratic aspects of the HL-MPL instrument.
This is most apparent in the plots of $\mu_r$.
In interpreting these plots, it is important to keep in mind that the CRRA parameters used in each population are normally distributed.
Thus, the mean of the distribution always represents the point of the distribution with the greatest density, with smaller standard deviations leading to greater concentration of the mass of the distribution around the mean and larger standard deviations leading to the reverse.

In Figures \ref{fig:S-Wel-rm} and \ref{fig:S-Err-rm}, each tick mark on the x-axis represent the values of the CRRA parameter at which an agent would be indifferent between lotteries for some row of the HL-MPL instrument.
From left to right, the first tick mark corresponds to the value of the CRRA parameter that would make an agent indifferent between the lotteries in the first row of the instrument, the second tick mark corresponds the second row of the instrument, and so on.
There are only 9 ticks because the there does not exist any CRRA parameter which would set an agent to be indifferent between the lotteries in row 10 of the instrument.

\begin{figure}[h!]
	\center
	\caption{Mean of CRRA Compared to Welfare}
	%\includegraphics[height=.28\paperheight]{figures/AggPlots/S-Wel-rm.pdf}
	\includegraphics[width=\textwidth]{figures/AggPlots/S-Wel-rm.pdf}
	\label{fig:S-Wel-rm}
\end{figure}

\begin{figure}[h!]
	\center
	\caption{Mean of CRRA Compared to Errors}
	\includegraphics[width=\textwidth]{figures/AggPlots/S-Err-rm.pdf}
	\label{fig:S-Err-rm}
\end{figure}

I begin by first discussing the effect of $\mu_r$ on choice errors as displayed in Figure \ref{fig:S-Err-rm}.
Something that is immediately apparent is that the orange LOESS line, depicting populations with low standard deviations of CRRA parameters, is much more volatile than the other quartile lines.
Interestingly, the orange line dips downward in plots (B), (C) and (D) and peaks upward in plot (A) at the values of $\mu_r$ that correspond to the indifference values described previously.
From plots (A) and (C), we draw the conclusion that as the mass of the distribution of preferences grows around parameter values which correspond to values which imply indifference in a choice scenario, we see an increase in the number of choice errors in the population.

In the case of the quartile described by the orange line, the idiosyncratic relationship between $\mu_r$ and the points that represent indifference also holds for the variance of expected choice errors, as depicted in plots (B) and (D) of Figure \ref{fig:S-Err-rm}.
That is, the increase in the average number of expected errors at these points is largely driven by a sharp reduction in the probability of observing a choice pattern with few expected errors relative to the probability of observing a choice pattern with a large number of errors.\footnote{
	Because $\Pr(y_t=a) = \Pr(y_t=b) \forall a,b$ as $\lambda \to \infty$, the maximum expected number of errors that can ever be observed is $\sum_{t=1}^T \frac{A_t -1}{A_t}$.
	That is, since every option is given equal probability in the limit, and only one option is not an error, the sum of ratio of choice errors to options across all tasks is the maximum expected number of choice errors in the limit.
	The maximum in the case of the HL-MPL instrument where $A_t = 2 \ \forall t$ and $T=10$ is therefore 5.
}

The remainder of the quartiles however do not follow this general pattern of heightened influence around the indifference points.
Instead, for plots (B),(C) and (D) of Figure \ref{fig:S-Err-rm}, the lines generally decrease until $\mu_r = 0.15$ and plot (A) increases until just about the same point.
This less volatile pattern is because the 3 highest quartiles all indicate populations with high standard deviations.
Consider the 3 upper quartile lines around $\mu_r = 0.15$.
The distances between this point and the two closest indifference points are $0.26$ and $0.29$.
The second lowest quartile's lower bound of $\sigma_r$ is $0.26$, which means that the density of the preference relation distribution at these points of indifference is much larger than for the lowest quartile, relatively.
It should be apparent from observing the lowest quartile line that as the density of the preference distribution increases around these points of indifference, the frequency of errors will increase.
We can attempt to see this more formally by creating a metric that characterizes how much the distribution of preferences \enquote{sits} on these points of indifference:
\begin{equation}
	\label{eq3:Dstat}
	D = \sum_r^R \frac{f(r)}{\max f(x)}
\end{equation}
\noindent where $f(r)$ is the density of the distribution of CRRA parameters for the population at point $r$ and $R$ is the set of values for the CRRA parameters at which an agent would be indifferent between the two options in each choice problem.
The denominator of the ratio is the maximum density of the distribution $f(\cdot)$ for the population.
Since the CRRA parameters were distributed normally, this value is always equivalent to the density at the mean, $\mu_r$.
The set of $R$ in for the HL-MPL instrument is:
\begin{equation}
	R \equiv \{-1.71, -0.95, -0.49, -0.14, 0.15, 0.41, 0.68, 0.97, 1.37\}
\end{equation}

We evaluate the metric from equation (\ref{eq3:Dstat}) against the 8 statistics utilized in Figures \ref{fig:S-Wel-rm} through \ref{fig:S-Err-us}.
%The raw plots of this data are provided in the Appendix, and the LOESS Figures will be discussed presently.
Since it can be seen in Figures \ref{fig:S-Wel-um} and \ref{fig:S-Err-um} that the effect of the $\mu_\lambda$ term asymptotes rapidly as $\mu_\lambda > 1$, we restrict our plots to populations for which $\mu_\lambda < 1$.
This leaves us with about 150k observations.
These 150k observations are first split into deciles of $\mu_\lambda$ and then the LOESS lines are calculated for each decile.
This splitting of the data helps to make clear the large effect of the stochastic elements on the statistics explored and also the large amount of heterogeneity in the effect of preference parameters caused by the stochastic parameters.

The metric developed in equation (\ref{eq3:Dstat}) is not perfect, we should expect to see clumping of data points around 0 and 1 where populations will be wholly sitting on one point or wholly between points, but it does provide a generally good description of the phenomenon we are concerned with.
Looking at Figure \ref{fig:D-Wel-smooth}, plots (A) and (C), we can confirm what was suspected to be driving the shape of the plots in Figure \ref{fig:S-Err-rm}.
As $D$ increases, and more of the density of the CRRA distribution is shifted onto the points describing indifference, the greater the expected number of errors we should observe.

\begin{figure}[h!]
	\center
	\caption{D Statistic Compared to Welfare}
	%\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Wel-D.pdf}
	\includegraphics[width=\textwidth]{figures/AggPlots/S-Wel-D.pdf}
	\label{fig:D-Wel-smooth}
\end{figure}

\begin{figure}[h!]
	\center
	\caption{D Statistic Compared to Errors}
	%\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Err-D.pdf}
	\includegraphics[width=\textwidth]{figures/AggPlots/S-Err-D.pdf}
	\label{fig:D-Err-smooth}
\end{figure}

This effect is remarkably monotonic across every decile of $\mu_\lambda$, though the effect is strongest for lower deciles.
What should be no surprise is that for the highest 3 deciles of $\mu_\lambda$ we observe close to $0\%$ of the populations considered to produce choice patterns with no choice errors, as can be seen in plot (C).
The variance statistics in plots (B) and (D) are generally monotonic, but not universally so.
In general, the variance in the number of expected errors across populations tends to decrease as $D$ increases.
This is in line with the populations becoming increasingly error prone.

In Figure \ref{fig:D-Err-smooth} we see the story of Figure \ref{fig:D-Wel-smooth} interpreted into welfare terms, but with an interesting and important difference: the expected welfare metrics in plots (A) and (C) are effectively equal around $D=0$ and $D=1$.
There doesn't exist an equality in the error metrics around these values of $D$ in Figure \ref{fig:D-Wel-smooth}, nor should there be.
$D=0$ corresponds to populations which have a $\mu_r$ and $\sigma_r$ such that the entire population sits between the indifference points in $R$.
$D=1$ will generally\footnote{
	Generally because there are multiple ways to get $D=1$. A population with $\mu_r$ close to one and a $\sigma_r$ such that there is some density on $r_j \in R \ \mathit{s.t.} \ i \neq j$ can potentially make $D \to 1$.
	However, $\mu_r \to r_j \in R$ and $\sigma_r \to 0$ is the most frequent scenario.
} represent the opposite; such a population will have a $\mu_r$ and $\sigma_r$ such that the entire population sits on top of one of the indifference points in $R$.
If the entire population sits far from an indifference point, holding the stochastic element constant, we expect there to be fewer errors compared to a population that sits on top of an indifference point because the average agent will not be close to indifference for the lottery pair in question.
But, this is also precisely why the welfare metrics are close to equivalent: if a population sits on an indifference point, it means that agents are mostly indifferent between the options in the lottery pair, and therefore any errors made for this lottery pair will be relatively less costly in terms of welfare.

Other than the particular case where $D=0$ and $D=1$, in Figure \ref{fig:D-Err-smooth} we see the general trend that we might expect from looking at Figure \ref{fig:D-Wel-smooth}: as the $D$ metric increases and the relative density of the CRRA distribution increases around points of indifference, expected welfare decreases monotonically.
This is because, other than the case of $D=1$ where errors should be relatively frequent but not costly in welfare terms, an increasing $D$ not only means that a greater proportion of agents lie on the indifference points, but also lie around it.
It is this greater proportion of agents lying sufficiently near an indifference point to make an error relatively likely, but sufficiently far to make it relatively costly which drives down expected welfare.
Similarly to what was seen in Figure \ref{fig:D-Wel-smooth}, in Figure \ref{fig:D-Err-smooth} we see that the effect of $D$ is stronger with populations with $\mu_\lambda$ in the lower deciles and weakest with populations with $\mu_\lambda$ in the higher deciles.

What is also clear from Figures \ref{fig:D-Wel-smooth} and \ref{fig:D-Err-smooth} is that the preference aspect of the utility model, represented by $D$, contributes far less to expected choice errors and, more importantly, to expected welfare than is contributed by the stochastic aspects of the model.
Looking at the lowest decile lines in Figures \ref{fig:D-Wel-smooth} and \ref{fig:D-Err-smooth}, we can see that a relatively large increase in $D$ is needed to cause the same effect as moving to the next lowest decile.
Comparing the lowest decile with the highest decile reveals tremendous changes in expected errors and expected welfare while holding $D$ constant for the populations analyzed.

The general analysis of the population level data reveals several somewhat expected results, and several somewhat unexpected results.
Firstly it is clear, and unsurprising, that the means of both the CRRA and $\lambda$ distributions individually drive a great deal of the variation in the number of expected choice errors and the expected welfare of a population.
Specifically, the finding that the effect of the mean of $\lambda$ on the expected number of choice errors was large should have been obvious \textit{a priori}.
The $\lambda$ parameter directly influences choice probabilities regardless of the underlying instrument.
Similarly, that populations with CRRA parameters tightly distributed around a point of indifference would have greater expected number of choice errors is intuitive.
That larger numbers of expected choice errors generally lead to lower welfare was already apparent from previous analyses.

Somewhat more surprising is just how dominant the stochastic elements of utility functions are over the preference aspects when deriving expectations around welfare.
Figures \ref{fig:D-Wel-smooth} and \ref{fig:D-Err-smooth} make clear, despite the potential flaws with the $D$ metric, that the way the preference parameters interact with the idiosyncratic aspects of the instrument matter a great deal, but the stochastic parameters unambiguously matter more.
This result should be important to economists and policy makers concerned with estimating the potential welfare implications of new policy instruments.

\singlespacing
\section{Summary of Analyses}
\doublespacing

In this chapter I analyze the relationship between an experimental instrument and the preferences of populations of agents.
I demonstrate a method for calculating the unconditional welfare surplus and efficiency for a given pattern of choices from an experimental instrument and for a given population of agents.

When considering a single hypothetical population of EUT agents several surprising results emerge.
First, it becomes clear that many choice patterns which are able to be rationalized by either EUT or RDU can be more likely to contain a choice error than not.
Given the hypothetical population chosen for analysis however, most of the choice errors for the choice patterns most likely to be observed can be considered to be not costly in welfare terms.
Second, there are several choice patterns that contain obvious violations of EUT that nonetheless are more likely to be observed \textit{and} are expected to produce greater welfare surplus than many choice patterns that do not contain any such apparent violations.
Third, and most interestingly, there is a less than perfect correlation between the likelihood of a choice pattern being observed and the expected welfare surplus of that choice pattern.
That is, there are many choice patterns that are less likely to be observed than other choice patterns, but nonetheless provide greater expected welfare.

Of particular note, shown in Figure \ref{fig:ConFOSD}, are choice patterns which include a choice of an option that is dominated by another option.
This figure shows that patterns of choice which contain FOSD choices are many times less likely to be observed from this hypothetical population than the equivalent choice pattern without the dominated choice, but only provide somewhat less expected welfare.
Additionally, choice patterns with FOSD choices can provide greater expected welfare than consistent choice patterns that are much more likely to be observed.
This is seen in Figure \ref{fig:ConFOSD} by comparing the choice pattern designated \enquote{Y,} which contains a FOSD choice, and the choice patterns designated by red circles in the shaded area of the same figure.
The choice patterns designated by the red circles in the shaded region are consistent with EUT, more likely to be observed than \enquote{Y,} and provide less welfare efficiency than \enquote{Y.}

The extent to which the distribution of preferences in a population influences the expected unconditional welfare is also assessed.
I simulate populations of agents and map their distributional parameters to expected unconditional welfare, as well as to the frequency of choice errors.
Additionally I construct a metric which relates the marginal distribution of risk preferences to the \enquote{indifference points} of an instrument called the \enquote{D} statistic.
These indifference points are the values of the parameters that would cause an individual agent to be indifferent between the options in the lottery pair.
From this exercise two interesting results arise.

First, as the density of the distribution of preferences increases around any value that would indicate indifference for a lottery pair in the instrument, the expected welfare surplus decreases and the rate of choice errors increases.
As the density of the distribution of preferences increases around multiple such indifference points, and thus the D statistic increases, this increase is even more apparent.
This can be seen in Figures \ref{fig:S-Wel-rm}, \ref{fig:S-Err-rm}, \ref{fig:D-Wel-smooth}, and \ref{fig:D-Err-smooth}.
Secondly, the value of the parameters governing the stochastic aspects of the model seem to play a larger than expected role in the expected welfare surplus for any given population.
This can be seen in Figure \ref{fig:S-Wel-um}, and in how the slope of the D statistic compares to moving from one gradient of $\lambda$ means to another in Figures \ref{fig:D-Wel-smooth}, and \ref{fig:D-Err-smooth}.

Theses results should caution economists and policy makers concerned with the design a policy or experimental instrument.
In a warning to those who would wish to \enquote{nudge} the behavior of agents to particular patterns of choice, this exercise has shown that policy makers should take care about the choice patterns they may which to nudge a agent into.
For many agents, it would be to their detriment to nudge them from a pattern of choices which contain obvious errors to one which, on appearance to an informed observer, would contain none.
Analyses of experiments involving risky choice that rely on tests which measure differences in choice frequencies around certain lottery pairs should note how the frequency of choice errors and \enquote{consistent} choice patterns vary with the distribution of risk preferences in a population.
%For the example population given in \ref{tb:TopTenEUT},


\newpage

\onlyinsubfile{
\newpage
\printbibliography[segment=3, heading=subbibliography]
}

\end{document}
