%DIF < \documentclass[../main.tex]{subfiles}
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL ch3.tex   Fri Mar 17 15:31:25 2017
%DIF ADD ref.tex   Tue Aug 23 18:06:06 2016
%DIF -------
\documentclass[11pt,a4paper]{article} %DIF > 
%\usepackage[a4paper,hmargin=.79 in, vmargin = 1 in]{geometry} %DIF > 
\usepackage[justification=centering]{caption} %DIF > 
\usepackage{lmodern} %DIF > 
\usepackage[T1]{fontenc} %DIF > 
\usepackage{geometry} %DIF > 
\usepackage{setspace}	% For line spacing %DIF > 
\usepackage{csquotes}	% For quoting %DIF > 
\usepackage[hidelinks]{hyperref} %To make references into links %DIF > 
\usepackage{amsmath}	% For equation stuff %DIF > 
\usepackage{amssymb}	% For math symbols %DIF > 
\usepackage{amsthm}		% For theorem environments %DIF > 
\usepackage{relsize}	% To resize math symbols %DIF > 
\usepackage{bm}			% For Bold math Symbols %DIF > 
\usepackage{nicefrac}	% For nice looking fractions %DIF > 
\usepackage{graphicx}	% For Graphics %DIF > 
 %DIF > 
% Stuff for Tables %DIF > 
\usepackage{caption} %DIF > 
\usepackage{adjustbox} %DIF > 
\usepackage{booktabs}		% For \toprule, \midrule, and \bottomrule %DIF > 
\usepackage{pgfplotstable}	% Generates a table from a CSV %DIF > 
\pgfplotsset{compat=1.12} %DIF > 
\setlength{\extrarowheight}{.5em} %DIF > 
 %DIF > 
\usepackage[stretch=15]{microtype}	% Read about this, tested it, and was sold. %DIF > 
\usepackage[backend=biber,doi=false,isbn=false,url=false,style=authoryear-comp, maxnames=99]{biblatex}	%For bibliography stuff %DIF > 
\renewbibmacro{in:}{} %DIF > 
\addbibresource{~/Dropbox/Papers/library.bib}	% The path to the bibliography file %DIF > 
 %DIF > 
% Convenience Commands %DIF > 
\newcommand\CE{\ensuremath{\mathit{CE}}}  % Certainty Equivalent %DIF > 
\newcommand\Prob{\ensuremath{\mathit{Pr}}}  % Probability %DIF > 
\newcommand{\E}{\operatorname{E}}			% Variance Operator %DIF > 
\newcommand{\Var}{\operatorname{Var}}		% Variance Operator %DIF > 
% Make a break a column title into 2 lines %DIF > 
\newcommand{\cnline}[2][c]{% %DIF > 
	\setlength{\extrarowheight}{0em} %DIF > 
	\begin{tabular}[#1]{@{}c@{}}#2\end{tabular} %DIF > 
	\setlength{\extrarowheight}{5em} %DIF > 
} %DIF > 
 %DIF > 
\setcounter{section}{3} %DIF > 
 %DIF > 
\title{\DIFaddbegin \DIFadd{The Welfare Implications of Stochastic Models: An Analysis by Simulation}\DIFaddend } %DIF > 
\author{\DIFaddbegin \DIFadd{Brian Albert Monroe}\DIFaddend } %DIF > 
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFaddtex}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdeltex}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF HYPERREF PREAMBLE %DIF PREAMBLE
\providecommand{\DIFadd}[1]{\texorpdfstring{\DIFaddtex{#1}}{#1}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{\texorpdfstring{\DIFdeltex{#1}}{}} %DIF PREAMBLE
\newcommand{\DIFscaledelfig}{0.5}
%DIF HIGHLIGHTGRAPHICS PREAMBLE %DIF PREAMBLE
\RequirePackage{settobox} %DIF PREAMBLE
\RequirePackage{letltxmacro} %DIF PREAMBLE
\newsavebox{\DIFdelgraphicsbox} %DIF PREAMBLE
\newlength{\DIFdelgraphicswidth} %DIF PREAMBLE
\newlength{\DIFdelgraphicsheight} %DIF PREAMBLE
% store original definition of \includegraphics %DIF PREAMBLE
\LetLtxMacro{\DIFOincludegraphics}{\includegraphics} %DIF PREAMBLE
\newcommand{\DIFaddincludegraphics}[2][]{{\color{blue}\fbox{\DIFOincludegraphics[#1]{#2}}}} %DIF PREAMBLE
\newcommand{\DIFdelincludegraphics}[2][]{% %DIF PREAMBLE
\sbox{\DIFdelgraphicsbox}{\DIFOincludegraphics[#1]{#2}}% %DIF PREAMBLE
\settoboxwidth{\DIFdelgraphicswidth}{\DIFdelgraphicsbox} %DIF PREAMBLE
\settoboxtotalheight{\DIFdelgraphicsheight}{\DIFdelgraphicsbox} %DIF PREAMBLE
\scalebox{\DIFscaledelfig}{% %DIF PREAMBLE
\parbox[b]{\DIFdelgraphicswidth}{\usebox{\DIFdelgraphicsbox}\\[-\baselineskip] \rule{\DIFdelgraphicswidth}{0em}}\llap{\resizebox{\DIFdelgraphicswidth}{\DIFdelgraphicsheight}{% %DIF PREAMBLE
\setlength{\unitlength}{\DIFdelgraphicswidth}% %DIF PREAMBLE
\begin{picture}(1,1)% %DIF PREAMBLE
\thicklines\linethickness{2pt} %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\framebox(1,1){}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\line( 1,1){1}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,1){\line(1,-1){1}}}% %DIF PREAMBLE
\end{picture}% %DIF PREAMBLE
}\hspace*{3pt}}} %DIF PREAMBLE
} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbegin}{\DIFaddbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddend}{\DIFaddend} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbegin}{\DIFdelbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelend}{\DIFdelend} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbegin}{\DIFOaddbegin \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbegin}{\DIFOdelbegin \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbeginFL}{\DIFaddbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddendFL}{\DIFaddendFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbeginFL}{\DIFdelbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelendFL}{\DIFdelendFL} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbeginFL}{\DIFOaddbeginFL \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbeginFL}{\DIFOdelbeginFL \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}
\DIFaddbegin 

\pagenumbering{gobble}	%DIF >  Don't start numbering pages until we pass the table of contents
\maketitle

\DIFaddend \onehalfspacing
\DIFdelbegin %DIFDELCMD < \setcounter{chapter}{2}
%DIFDELCMD < %%%
\DIFdelend 

\DIFdelbegin \chapter{\DIFdel{The Welfare Implications of Stochastic Models}}
%DIFAUXCMD
\addtocounter{chapter}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \tableofcontents
\DIFaddend 

\DIFdelbegin %DIFDELCMD < \lltoc %%%
%DIF <  Table of contents only when locally compiled
\DIFdelend \DIFaddbegin \pagenumbering{arabic}
\DIFaddend 

\DIFaddbegin \newpage

\DIFaddend Given the discussion about how the various stochastic models generally deal with the normative notion of welfare, we would like to reintroduce the question asked earlier\DIFdelbegin \DIFdel{in Chapter 2, section 2.2}\DIFdelend : 
\enquote{What are the likely welfare implications of an economic agent's choices in an incentivized environment given an assumed stochastic model of choice?}
Our conclusion for the \DIFdelbegin \DIFdel{Random Preference (RP ) }\DIFdelend \DIFaddbegin \DIFadd{RP }\DIFaddend model and its derivative, the \DIFdelbegin \DIFdel{Random Preference Per Option (RPPO ) }\DIFdelend \DIFaddbegin \DIFadd{RPPO }\DIFaddend model, is \enquote{no coherent statements can be made.}
As stated \DIFdelbegin \DIFdel{in the conclusion of Chapter 2, the Random Error (RE ) and Tremble (TR ) }\DIFdelend \DIFaddbegin \DIFadd{previously, the RE and TR }\DIFaddend models do not suffer from this inadequacy\DIFdelbegin \DIFdel{, and will be referred to as }%DIFDELCMD < \enquote{coherent models}%%%
\DIFdel{. 
In this chapter, we will continue to contribute to the answer of the primary question by utilizing }\DIFdelend \DIFaddbegin \DIFadd{. 
The answer to the primary question for }\DIFaddend coherent stochastic models, \DIFdelbegin \DIFdel{a popular experimental preference elicitation instrument, and simulation methods to derive numerical characterizations of welfare}\DIFdelend \DIFaddbegin \DIFadd{however, needs to be answered empirically}\DIFaddend .

In the \DIFdelbegin \DIFdel{Stochastic Money Pump (SMP ) thought experiment discussed in Chapter 2, section 2.4, }\DIFdelend \DIFaddbegin \DIFadd{SMP }\DIFaddend we were able to discuss the welfare implications of Beth and Cate's choices because we assumed an experimenter had already identified the stochastic specification which completely characterized their choices.
Identifying a stochastic specification \DIFaddbegin \DIFadd{in reality }\DIFaddend typically involves presenting an experimental subject with a series of incentivized choice problems in which the subjects are asked to select an option from a set of alternatives.
The subject's choices in such an experiment are \DIFdelbegin \DIFdel{assumed }\DIFdelend \DIFaddbegin \DIFadd{said }\DIFaddend to reveal their preferences.
But, as we have seen with the SMP example, \DIFdelbegin \DIFdel{even }\DIFdelend coherent stochastic models imply that most choices by an economic agent can be characterized as welfare suboptimal with a probability less than or equal to the probability of the choice being optimal.
This applies to choices which are apparently incompatible with optimality, as well as choices which can be rationalized as optimal.
This property of stochastic models can lead to misidentification of the parameter set $\beta$ which shapes the stochastic specification.
This, in turn, can lead to a mis-characterization of the welfare effects of certain choices.
To understand the consequences of an assumed stochastic model, we will look at another numerical example utilizing the popular Multiple Price List (MPL) utilized by Holt \& Laury (2002) (HL).
First however, we will \DIFdelbegin \DIFdel{revisit some notation from Chapter 2, briefly describe }\DIFdelend \DIFaddbegin \DIFadd{describe briefly }\DIFaddend some econometric methods for identification, and then propose some \DIFdelbegin \DIFdel{further }\DIFdelend \DIFaddbegin \DIFadd{more }\DIFaddend notation to make concepts cleaner.

\DIFdelbegin \DIFdel{We consider knowledge of the welfare of individual agents of great importance to economists.
It is often the case, however, in the application of economic principals knowledge of the welfare consequences to any given individual matters less than the welfare consequences to groups of individuals.
For example, when a government policy compels citizens to make an economic choice, such as the purchase of health insurance or the payment of a penalty, the government or those analyzing the policy may be concerned with the welfare effects of this policy across the entire citizenry, not just the effect of a given citizen.
With a policy such as this, many citizens with differing preferences may make exactly the same choice to purchase or not purchase the insurance product, with some citizens mistakenly purchasing or not purchasing the insurance product.
A government or policy analyst may want to know the distribution of welfare consequences among those who have or have not purchased the insurance in order to make better policy or policy recommendations with respect to welfare.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\section{\DIFdel{Notation and Estimation}} %DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
%DIFDELCMD < \label{ssec:Notation}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{For any salient lottery $X_j$, and any vector of parameters $\beta_n$, there exists some certain outcome, $\CE_j$, such that subject $n$ is indifferent between the lottery and the certainty equivalent:
}\begin{displaymath}
	\DIFdel{%DIFDELCMD < \label{eq3:CE.indiff}%%%
	X_j \sim^n }{\DIFdel{\CE}}\DIFdel{_j \;\Leftrightarrow\; G(\beta_n,X_j) = G(\beta_n, }{\DIFdel{\CE}}\DIFdel{_j)
}\end{displaymath}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \noindent %%%
\DIFdel{where $G(\cdot)$ is some utility function with all the usual properties.
For our purposes throughout this chapter, we will assume some variation of the Rank Dependent Utility structure defined as follows:
}\begin{displaymath}
	\DIFdel{%DIFDELCMD < \label{eq3:RDU}%%%
	RDU = \sum_{i=1}^{I} \left[ w_i(p) \times u(x_i) \right]
}\end{displaymath}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \noindent %%%
\DIFdel{where $u(\cdot)$ is the CRRA utility function throughout this chapter:
}\begin{displaymath}
	\DIFdel{%DIFDELCMD < \label{eq3:CRRA}%%%
	u(x) = \frac{x^{(1-r)}}{(1-r)}
}\end{displaymath}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \noindent %%%
\DIFdel{and $w_i(p)$ is the decision weight applied to option $i$ defined as follows:
}\begin{displaymath}
	\DIFdel{%DIFDELCMD < \label{eq3:dweight}%%%
	w_i(p) =
	\begin{cases}
		\omega\left(\displaystyle\sum_{j=i}^I p_j\right) - \omega\left(\displaystyle\sum_{k=i+1}^J p_k\right) & \text{for } i<I \\
		\omega(p_i) & \text{for } i = I
	\end{cases}
}\end{displaymath}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \noindent %%%
\DIFdel{where $\omega(\cdot)$ is a probability weighting function.
In cases where $\omega(p_i) = p_i$, the RDU structure is equivalent to Expected Utility Theory (EUT) as the decion weights for each option will equal their objective values.
Many paramterized probability weighting functions allow for this special case to occur.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Combining the Rank Dependent Utility (RDU) structure with a CRRA utility function, we can define the }%DIFDELCMD < {\CE} %%%
\DIFdel{as follows:
}\begin{align*}
	\DIFdel{%DIFDELCMD < \label{eq3:CEcalc}%%%
	\begin{split}
		G(\beta_n,X_j) &= \sum_{i=1}^{I} w_i(p) \frac{x_{ij}^{(1-r)}}{(1-r)} = \frac{ {\CE}_j^{(1-r)}}{(1-r)}\\
		{\CE}_j &=  \left( (1-r) \times \sum_{i=1}^{I} w_i(p) \frac{x_{ij}^{1-r}}{(1-r)} \right)^{ \displaystyle\nicefrac{1}{(1-r)} }
	\end{split}
}\end{align*}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \noindent %%%
\DIFdel{where $i$ indexes the $I$ outcomes of option $j$ in task $t$. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{We continue the notation from Chapter 2 where the value of $j$ also represents each option's ordinal rank among the alternative options in task $t$.
Thus $X_1 \succcurlyeq X_2$ and $X_j \succcurlyeq X_k$, where $k \geq j$.
Similarly, we define the set of unchosen options from the full set of alternatives as $Z = t \,\backslash\, y = \{z \in t \;|\; z \notin y \}$, with the subscript on the elements of $Z$ indicating their ordinal rank in the set of $Z$.
Thus $X_1^Z \succcurlyeq X_2^Z$ and $X_j^Z \succcurlyeq X_k^Z$, where $k \geq j$.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The probability of }\DIFdelend \DIFaddbegin \DIFadd{As shown previously, the probability of }\DIFaddend any choice $j$ by some subject $n$, given some vector \DIFdelbegin \DIFdel{of parameters }\DIFdelend $\beta$, being observed for a task $t$, is denoted by $\Pr( y_t = j)$\DIFdelbegin \DIFdel{, where $y_t = j$ is an indicator function that records option $j$ as being chosen in task $t$}\DIFdelend .
To make explicit the dependency of this probability on the option in question, the subject, the task, and the $\beta$ vector, this relationship will be re-framed as follows:
\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:Pnjt}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:Pnjt}
	\DIFaddend P_{njt}(\beta_n) = \Pr(y_i = j)
\end{equation}

The likelihood of observing a series of choices is \DIFaddbegin \DIFadd{simply }\DIFaddend the product of the probability of observing the option chosen for each task across all tasks, $T$ :
\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:PnT}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:PnT}
	\DIFaddend P_{nT}(\beta_n) =  \prod_{t}^{T} P_{njt}(\beta_n)
\end{equation}

\DIFdelbegin %DIFDELCMD < \noindent %%%
\DIFdelend This is the standard likelihood function applied to choice data.
We could take the log of equation (\DIFdelbegin \DIFdel{\ref{eq3:PnT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:PnT}}\DIFaddend ) and conduct standard maximum likelihood estimation (MLE) by searching for the vector $\hat{\beta}_n$ which maximizes the log-likelihood function:
\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:LPnT}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:LPnT}
	\DIFaddend \mathit{LP}_{nT}(\beta_n) = \sum_{t}^{T} \ln \left( P_{nit}(\beta_n) \right)
\end{equation}

\DIFdelbegin %DIFDELCMD < \noindent %%%
\DIFdelend Thus, the maximum likelihood estimator $\hat{\beta}_n$ for subject $n$ is:
\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:Bn}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:Bn}
	\DIFaddend \hat{\beta}_n = \underset{x}{\operatorname{arg\,max}}\sum_t^T \ln \left( P_{nit}(\beta_n) \right)
\end{equation}

We can utilize this estimator to recover the \DIFdelbegin %DIFDELCMD < {\CE} %%%
\DIFdelend \DIFaddbegin \DIFadd{CE }\DIFaddend for every option in every task, and then utilize these \CE s to recover our best estimate of the proportion of welfare the subject obtained. 
While conducting welfare analysis given individually estimated parameter vectors is rare in the economics literature,{\footnotemark} the recovery of parameter vectors through MLE is as common as the welfare analysis is rare.
\textcite{Hey1994}, \textcite{Wilcox2015}, and \textcite{Hey2001} provide several prominent examples of parameter estimation.
These particular examples, however, are distinctly different from other uses of MLE in experimental economics, primarily because equation (\DIFdelbegin \DIFdel{\ref{eq3:Bn}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:Bn}}\DIFaddend ) is estimated for every subject individually, as opposed to pooling all subject data together and estimating a parameter vector for one, representative agent (RA)\DIFdelbegin \DIFdel{, e.
g. \textcite{Camerer1994}}\DIFdelend .

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	An example of this kind of analysis is \DIFdelbegin \DIFdel{\textcite{Harrison2016}
}\DIFdelend \DIFaddbegin \DIFadd{\textcite{Harrison2015}
}\DIFaddend }

There are legitimate methodological (and practical) reasons for modeling choices across subjects as the choices of a single RA.
For instance, the analyst could be primarily concerned with the economic characteristics of the whole sample, rather than with the individuals composing the sample.
As shown in \textcite[142]{Harrison2008a}, it is easy to allow the $\hat{\beta}$ to be determined by a linear combination of observable characteristics of either the subjects or experimental treatments.
For instance, if the race, gender and age of each of the subjects were known, we could estimate:
\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:BB}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:BB}
	\DIFaddend \bm{\hat{\beta}} = \hat{\beta}_0 + \hat{\beta}_1 \times \mathit{race} + \hat{\beta}_2 \times \mathit{gender} + \hat{\beta}_3 \times \mathit{age}
\end{equation}
\noindent where $\hat{\beta}_1$ through $\hat{\beta}_3$ represent the mean marginal effects of race through age respectively on the vector $\bm{\hat{\beta}}$.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Another useful technology \DIFdelbegin \DIFdel{demonstrated by \textcite{Harrison2008a} }\DIFdelend for RA modeling is the use of finite mixture modeling.
This is when a finite mixture of stochastic specifications are estimated jointly on the same data along with mixture parameters.
For instance,
\begin{align}
	\DIFdelbegin %DIFDELCMD < \label{eq3:PT_Mix}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:PT_Mix}
	\DIFaddend \begin{split}
		\bm{\mathit{P_T}} = \prod_t^T \left[ \sum_m^M \pi_m \times L_T^m(\beta^m) \right]\\ 
		\mathit{st.} \sum_m^M \pi_m = 1
	\end{split}
\end{align}

\noindent where $\pi_m$ is the proportion of model $m$ in the mixture, $\beta$ is the vector of parameters to be estimated in model $m$ and $L_T^m$ is the likelihood of the choice data across the $T$ tasks is explained by model $m$ given the vector $\beta^m$.
Similarly, the log-likelihood for finite mixture models is defined as:
\begin{align}
	\DIFdelbegin %DIFDELCMD < \label{eq3:LPT_Mix}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:LPT_Mix}
	\DIFaddend \begin{split}
		\bm{\mathit{LP_T}} = \sum_t^T \left[ \ln \left( \sum_m^M \pi_m \times L_T^m(\beta^m) \right) \right]\\ 
		\mathit{st.} \sum_m^M \pi_m = 1
	\end{split}
\end{align}

\DIFdelbegin %DIFDELCMD < \noindent %%%
\DIFdelend Thus, $M$, $\beta^m$ and $M-1$, $\pi_m$ vectors need to be estimated.
These parameters can additionally be determined by observed characteristics, as in equation (\DIFdelbegin \DIFdel{\ref{eq3:BB}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:BB}}\DIFaddend ).
This method can be useful if the analyst wishes to estimate the proportion of a sample which more closely adheres to RDU versus EUT for instance, or if the analyst wants to determine if there is some heterogeneity in the sample that is revealed by choice, but unobservable otherwise.
\textcite[141]{Harrison2008a} use this method to jointly estimate a specification composed of Prospect Theory (PT) and EUT.
They employ a \DIFdelbegin \DIFdel{Strong Utility (SU ) }\DIFdelend \DIFaddbegin \DIFadd{SU }\DIFaddend stochastic model to generate the probabilities.
\DIFdelbegin \DIFdel{Although there does not appear to be }\DIFdelend \DIFaddbegin \DIFadd{Though we're not aware of }\DIFaddend any literature doing so, it is possible to estimate a mixture of two differing stochastic models.
For instance, \DIFdelbegin \DIFdel{an analyst could use a mixture model to determine what proportion of subjects in a dataset are }\DIFdelend \DIFaddbegin \DIFadd{finding a mixture of subjects }\DIFaddend better explained by the SU or TR models.{\footnotemark}

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	This process could be used to help with the econometric \DIFdelbegin \DIFdel{limitations }\DIFdelend \DIFaddbegin \DIFadd{failures }\DIFaddend of the pure RP model as those subjects who violate FOSD can be picked up by an alternative model which permitted such violations. 
	This process, of course, doesn't resolve the RP model's normative failures\DIFdelbegin \DIFdel{discussed in chapter 2.
}\DIFdelend \DIFaddbegin \DIFadd{.
}\DIFaddend }

There are also some methodological problems, or at least limitations \DIFdelbegin \DIFdel{, }\DIFdelend when conducting estimation on pooled data.
The estimates represent the means \DIFdelbegin \DIFdel{of }\DIFdelend the relevant parameters in the sample, but often the distributions of these parameters and whether these distributions are correlated provide more important information to analysts.{\footnotemark} 
While the methods described in equations (\DIFdelbegin \DIFdel{\ref{eq3:BB}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:BB}}\DIFaddend ) and (\DIFdelbegin \DIFdel{\ref{eq3:PT_Mix}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:PT_Mix}}\DIFaddend ) provide some insight into the heterogeneity of a pooled sample, this is mostly limited to estimating average deviations from the mean due to observable heterogeneity.
While it is theoretically possible to have a mixture model with greater than two underlying stochastic specifications, in reality this is computationally demanding and thus the mixture model presented in (\DIFdelbegin \DIFdel{\ref{eq3:PT_Mix}) is often only utilized with }\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:PT_Mix}) is effectively limited to being able to identify }\DIFaddend one or two \DIFdelbegin \DIFdel{mixtures}\DIFdelend \DIFaddbegin \DIFadd{unobservable characteristics}\DIFaddend .

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	For an example of why it could be problematic to make inferences about a population from an estimate which represents the mean of a distribution of preferences consider a population that has preferences distributed as $\textit{Logit-Normal} \sim \mathcal{N}(0,5)$. 
	See Figure 2\DIFdelbegin \DIFdel{of }\DIFdelend \DIFaddbegin \DIFadd{, }\DIFaddend \textcite[83]{Andersen2012}.
	This distribution is highly bi-modal, and the area around the mean of the distribution has very low density. 
	Thus, if a single stochastic specification is estimated on a sample from this population, the estimated parameters representing their distributional means give highly misleading information about the choice behavior we would expect from individual agents sampled from this population. 
	In this case a mixture model of two models could potentially identify the modes, thus providing more, but still limited, information about the population.
\DIFdelbegin \DIFdel{A similar approach is utilized by \textcite{Conte2011}.
}\DIFdelend }

Estimating parameter vectors for every subject in a sample helps to improve on this limitation.
If every subject has an individually estimated parameter vector, then an analyst can use the \DIFdelbegin \DIFdel{distribution }\DIFdelend \DIFaddbegin \DIFadd{distributions }\DIFaddend of these estimates to approximate the distribution of parameter vectors of the population this sample was drawn from.
This is not perfect however, the individually estimated parameters are still estimates, and thus they all have associated standard errors and positive probabilities of misidentification.
The likelihood of misidentification typically decreases with the number of choice tasks presented to subjects, just as standard errors are typically negatively correlated with sample size.
\textcite{Hey1994} estimate parameters for individual subjects utilizing 100 choice tasks per subject in order minimize the potential for misidentification.
\textcite{Hey2001} utilized 500 choice problems per subject.

\DIFdelbegin %DIFDELCMD < \addtocounter{footnote}{-1}
%DIFDELCMD < \stepcounter{footnote}%%%
\footnotetext{\DIFdel{As discussed in Chapter 2, the primary concern of economic analysis is to provide useful information about the welfare implications of policy or institutional rules.
	By and large, policy is written in a broad way so that it targets populations of individuals, as opposed to singling out individual agents.
	In this respect, knowledge of the distribution of preferences in a population provides useful information about the distribution of welfare effects of policy.
}}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend However, conducting experiments where subjects are required to give responses to a large number of tasks has practical problems which then spill over \DIFdelbegin \DIFdel{and generates }\DIFdelend \DIFaddbegin \DIFadd{into }\DIFaddend theoretical problems.
Subjects can become bored or tired, which may make the tasks less salient or cause them to fail to satisfy the dominance criteria as described by \textcite{Harrison1992}.
Often, experimenters utilize a random lottery incentive mechanism (RLIM) in experiments, selecting one choice by the subject at random for payment.
While in theory this is incentive compatible with EUT, it is not necessarily so with any utility theory that doesn't require the independence axiom (IA), such as RDU\DIFdelbegin %DIFDELCMD < \parencite{Harrison2014, Cox2015}%%%
\DIFdelend .
Furthermore, each additional choice task presented to the subject dilutes the expected outcomes of the other choice tasks.
This means that the task could fail the dominance criteria unless the outcomes are sufficiently scaled up, even if the outcomes and the payment mechanism are salient.
Thus, when the experimenter implements the RLIM for practical reasons, such as not needing to resolve and then compensate a subject for all of potentially hundreds of choices, he potentially introduces a serious theoretical concern.

These qualifications to estimation of individual parameter vectors should not be considered \DIFdelbegin \DIFdel{fatal for }\DIFdelend \DIFaddbegin \DIFadd{lethal to }\DIFaddend this method, but they should be noted when conducting this kind of estimation.
\textcite{Hey2001} split the 500 choice tasks over 5 days to help mitigate the potential for subjects to become bored.
Other experimenters split the \DIFdelbegin \DIFdel{$T$ }\DIFdelend \DIFaddbegin \DIFadd{T }\DIFaddend lottery tasks into smaller sets of tasks which are split by other, potentially unrelated, tasks.
These kind of \DIFdelbegin \DIFdel{designs help mitigate the procedural }\DIFdelend \DIFaddbegin \DIFadd{efforts help to mitigate the methodological }\DIFaddend problems with this kind of estimation, though sometimes they may introduce other concerns.
While subjects may be less bored by doing subjects over 5 days rather than all on one day, subjects may experience events in between sessions that change their beliefs about the lottery pairs presented during the sessions.
\DIFaddbegin \DIFadd{To a lesser extent, beliefs about lotteries could be changed when lottery tasks are split by other tasks in the lab.
}\DIFaddend 

An alternative method to \DIFdelbegin \DIFdel{recover }\DIFdelend \DIFaddbegin \DIFadd{recovering }\DIFaddend greater information on entire samples of agents is to estimate the distributions of the parameter vectors describing individual preferences directly from pooled data.
Instead of estimating preference parameters, the parameters which shape the distributions of preferences are estimated.
We can call equation (\DIFdelbegin \DIFdel{\ref{eq3:Pnjt}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:Pnjt}}\DIFaddend ), which is at the heart of equations (\DIFdelbegin \DIFdel{\ref{eq3:PnT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:PnT}}\DIFaddend ) through (\DIFdelbegin \DIFdel{\ref{eq3:LPT_Mix}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:LPT_Mix}}\DIFaddend ), a conditional probability, because the probability is conditional on a particular $\beta$ vector.
We can however weight this function by the likelihood of observing the $\beta$ vector from a given distribution.
\DIFdelbegin %DIFDELCMD < {\footnotemark}
%DIFDELCMD < %%%
\DIFdelend We call this weighted probability the unconditional probability:
\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:Pnt}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:Pnt}
	\DIFaddend P_{nt}(\theta) = \int P_{nt}(\beta_n) f(\beta | \theta) d\beta	
\end{equation}
\noindent where $f(\beta|\theta)$ is the density function of the $\beta$ vector given some vector of hyper-parameters $\theta$ shaping the distribution of the $\beta$.

\DIFdelbegin %DIFDELCMD < \addtocounter{footnote}{-1}
%DIFDELCMD < \stepcounter{footnote}%%%
\footnotetext{\DIFdel{It is worth noting the relation of these statements to a Bayesian approach.
	Having knowledge of distribution of preferences in a population is akin to holding a prior in a Bayesian approach.
	This prior could then be incorporated with to condition individual level estates and produce an individual level choice probability.
	This Bayesian technique is different from the two approaches discussed here.
	The individual level approach discussed here does not incorporate a distributional prior in its estimation process, while the unconditional approach generates choice probabilities directly from pooled data, not individual data.
	If the unconditional approach discussed here was applied at an individual level, it would be equivalent to the Random Preference stochastic model.
}}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend This unconditional probability can be substituted for the conditional probability used in equations (\DIFdelbegin \DIFdel{\ref{eq3:PnT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:PnT}}\DIFaddend ) and (\DIFdelbegin \DIFdel{\ref{eq3:LPnT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:LPnT}}\DIFaddend ) to give us the unconditional likelihood equation:
\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:LnT}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:LnT}
	\DIFaddend L_{nT}(\theta) = \prod_t^T P_{nt}(\theta)
\end{equation}

\noindent and its counterpart, the unconditional log-likelihood equation:
\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:LLnT}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:LLnT}
	\DIFaddend \mathit{LL}_{nT}(\theta) = \sum_t^T \ln \left( P_{nt}(\theta) \right)
\end{equation}

Equations (\DIFdelbegin \DIFdel{\ref{eq3:Pnt}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:Pnt}}\DIFaddend ) through (\DIFdelbegin \DIFdel{\ref{eq3:LLnT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:LLnT}}\DIFaddend ) are computationally impossible to estimate directly due to \DIFdelbegin \DIFdel{the general }%DIFDELCMD < \enquote{inability of computers to perform integration} %%%
\DIFdelend \DIFaddbegin \enquote{the inability of computers to perform integration} \DIFaddend in a closed-form \parencite[2]{Train2002}.
However, equation (\DIFdelbegin \DIFdel{\ref{eq3:Pnt}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:Pnt}}\DIFaddend ) can be approximated by simulation as follows:
\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:SPnt}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:SPnt}
	\DIFaddend \mathit{SP}_{nt}(\theta) = \mathlarger{\sum}_h^H \frac{ P_{nt}(\beta^h) }{H}
\end{equation}

\DIFdelbegin \DIFdel{Equation (\ref{eq3:SPnt}) }\DIFdelend \DIFaddbegin \DIFadd{This }\DIFaddend needs some explanation.
The integration involved in equation (\DIFdelbegin \DIFdel{\ref{eq3:Pnt}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:Pnt}}\DIFaddend ) is approximated by taking $H$ random draws of $\beta^h$ from the distribution governed by $\theta$, evaluating equation (\DIFdelbegin \DIFdel{\ref{eq3:Pnjt}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:Pnjt}}\DIFaddend ) with these randomly drawn $\beta^h$ \DIFdelbegin \DIFdel{, }\DIFdelend and taking a simple average across these evaluations.
Only a simple average is needed because if the $\beta^h$ vectors are drawn at random from the distribution governed by $\theta$, then the likelihood of their occurrence is already weighted by the distribution's density.

The use of $H$ as the term characterizing draws from a distribution is not arbitrary.
It indicates that the random draws will be approximated by a Halton sequence of numbers.
The Halton routine is a numerical method to produce a sequence of numbers which approximate random draws from a uniform distribution bounded between 0 and \DIFdelbegin \DIFdel{1, and which }\DIFdelend \DIFaddbegin \DIFadd{1.
The Halton sequence however }\DIFaddend has been shown to provide better coverage of the distribution than other pseudo-random{\footnotemark} number generators.{\footnotemark}

\addtocounter{footnote}{-2}
\stepcounter{footnote}\footnotetext{
	\DIFdelbegin \DIFdel{All }\DIFdelend \DIFaddbegin \DIFadd{It is important to recognize that all }\DIFaddend \enquote{random} numbers generated by computers are in fact \enquote{pseudo-random} numbers produced algorithmically. 
	\textcite[234]{Train2002} describes these numerical routines as follows: 
	\enquote{The intent in \textins{the} design \textins{of pseudo-random routines} is to produce numbers that exhibit the properties of random draws. 
		The extent to which this intent is realized depends, of course, on how one defines the properties of \enquote{random} draws.
		These properties are difficult to define precisely since randomness is a theoretical concept that has no operational counterpart in the real world.} 
	Because of the non-existence of truly \enquote{random} number generators, the term \enquote{random} will be used in place of \enquote{pseudo-random} throughout this text.
}
\stepcounter{footnote}\footnotetext{
	See the remainder of \textcite[Chapter~9]{Train2002} for an in-depth discussion and derivation of why Halton sequences are \DIFdelbegin \DIFdel{widely viewed as being }\DIFdelend superior to other pseudo-random number generators for the purposes of simulating estimators.
}

The \DIFdelbegin \DIFdel{Halton }\DIFdelend sequence of uniformly distributed numbers can be transformed into a sequence of randomly drawn numbers from any invertible, univariate distribution.
That is, if $\mu$ is taken to be a random variable indicating a draw from a uniform distribution, and $F(\epsilon)$ is an invertible, univariate, cumulative distribution, then given $\mu$ , draws of $\epsilon$ from this distribution can be obtained by solving $\epsilon = F^{-1}(\mu)$.
\textcite[236]{Train2002} discusses this method for obtaining random draws from invertible, univariate distributions, as well as using Choleski transformations to obtain draws from multivariate normal distributions.

With this simulated unconditional probability, we can obtain the simulated unconditional \DIFdelbegin \DIFdel{likelihood }\DIFdelend \DIFaddbegin \DIFadd{log-likelihood }\DIFaddend by substituting equation (\DIFdelbegin \DIFdel{\ref{eq3:SPnt}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:SPnt}}\DIFaddend ) for equation (\DIFdelbegin \DIFdel{\ref{eq3:Pnt}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:Pnt}}\DIFaddend ) in equation (\DIFdelbegin \DIFdel{\ref{eq3:LnT}):
}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:LnT}):
}\DIFaddend \begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:SLnT}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:SLnT}
	\DIFaddend \mathit{SL}_{nT}(\theta) = \prod_t^T \left[ \mathlarger{\sum}_h^H \frac{ P_{nt}(\beta^h) }{H} \right]
\end{equation}

Equation (\DIFdelbegin \DIFdel{\ref{eq3:SLnT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:SLnT}}\DIFaddend ) is limited in terms of identifying $\theta$ because \DIFdelbegin \DIFdel{, as indicated by }\DIFdelend \DIFaddbegin \DIFadd{of }\DIFaddend the $n$ subscript, \DIFaddbegin \DIFadd{indicating that }\DIFaddend this metric is \DIFdelbegin \DIFdel{defined }\DIFdelend \DIFaddbegin \DIFadd{returned }\DIFaddend for a single agent.
Since the normatively coherent stochastic models \DIFdelbegin \DIFdel{discussed in Chapter 2 }\DIFdelend \DIFaddbegin \DIFadd{we've been discussing }\DIFaddend have non-random elements composing $\beta_n$, there is no distribution of $\beta_n$ to be estimated from a single agent\DIFdelbegin \DIFdel{'s choices}\DIFdelend .
The real power of this method is realized \DIFdelbegin \DIFdel{, however, }\DIFdelend when sample data is pooled together and the distribution of $\beta_n$ vectors is estimated from this pooled data.
This is an easy extension of equation (\DIFdelbegin \DIFdel{\ref{eq3:SLnT}), which is logged for numerical stability:
}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:SLnT}):
}\DIFaddend \begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:SLLNT}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:SLLNT}
	\DIFaddend \mathit{SLL}_{NT}(\theta) = \sum_{n=1}^N \left( \sum_t^T \left[ \ln\!\left( \sum_h^H \frac{ P_{nt}(\beta^h) }{H} \right) \right] \right)
\end{equation}

\DIFdelbegin %DIFDELCMD < \noindent %%%
\DIFdelend We call equation (\DIFdelbegin \DIFdel{\ref{eq3:SLLNT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:SLLNT}}\DIFaddend ) the unconditional simulated log-likelihood function, or just the simulated log-likelihood function (SLL).
Maximum simulated likelihood (MSL) methods can be applied to this equation to return the MSL estimator $\hat{\theta}$ which maximizes this function.
The characteristics of simulated estimators are reviewed in depth by \textcite[Chapter~10]{Train2002}, \DIFdelbegin \DIFdel{and the critical insight is that the }\DIFdelend \DIFaddbegin \DIFadd{but importantly, the }\DIFaddend estimator $\hat{\theta}$ derived from equation (\DIFdelbegin \DIFdel{\ref{eq3:SLLNT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:SLLNT}}\DIFaddend ) approaches the estimator from equation (\DIFdelbegin \DIFdel{\ref{eq3:LLnT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:LLnT}}\DIFaddend ) with a sufficiently large, $H$, number of draws from the distribution governed by $\theta$.

Estimating the distribution of preferences for a sample with MSL improves the analyst's position on multiple accounts.
\DIFdelbegin \DIFdel{First}\DIFdelend \DIFaddbegin \DIFadd{For one}\DIFaddend , the limitation of estimating only the mean preference parameter for pooled data with standard MLE is no longer binding.
Flexible distributions such as the Logit-Normal{\footnotemark} can be employed to estimate higher moments of the distribution such as the variance, skewness and kurtosis.
\DIFdelbegin \DIFdel{Second}\DIFdelend \DIFaddbegin \DIFadd{Secondly}\DIFaddend , the necessity of asking every subject dozens of questions to estimate preference parameters subject-by-subject is eased by being able to pool data across subjects.

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	\textcite[82]{Andersen2012} utilize the Logit-Normal distribution because of its high degree of flexibility and because \enquote{MSL algorithms developed for univariate or multivariate Normal distributions can be applied directly.} 
	The figures they present \parencite*[83]{Andersen2012} display some of the flexible forms this distribution can take.
}

\DIFdelbegin \section{\DIFdel{The }%DIFDELCMD < \texorpdfstring{\textcite{Holt2002}}{Holt and Laury (2002)} %%%
\DIFdel{MPL and the Unconditional Assessment of Expected Welfare}}
%DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \DIFadd{As an example of statistical power concerns, consider an RDU stochastic specification where $\beta_n =\{r , \lambda , a , b\}$ , where $r$ is the CRRA parameter, $\lambda$ is the precision parameter, and $a$ and $b$ are the probability weighting parameters.
When estimating $\beta_n$ for each subject, a }\enquote{large} \DIFadd{number of questions needs to be asked in order to reach sufficient statistical significance given the 4 parameters that need to be jointly estimated.
If we were concerned with making inferences about the population that the sample was drawn from, this estimation must be repeated on many subjects to get good coverage of that distribution.
This could entail the estimation of hundreds of parameters, each with their own standard errors.
In contrast, if MSL is utilized and each element of $\beta_n$ is assumed to be independently distributed as Logit-Normal, $\hat{\theta}$ then consists of only 8 parameters that need to be estimated.
The assumption of independence of the marginal distributions can be relaxed and a covariance matrix of these distribution-shaping parameters could be estimated at the same time.
This would still only increase the number of parameters to be estimated to 12.
}\DIFaddend 

\DIFdelbegin \DIFdel{Issues }\DIFdelend \DIFaddbegin \subsection{\DIFadd{The }\texorpdfstring{\textcite{Holt2002}}{Holt and Laury (2002)} \DIFadd{MPL and the Unconditional Assessment of Expected Welfare}}

\DIFadd{The issues }\DIFaddend concerning statistical power and identification will be discussed in more depth \DIFdelbegin \DIFdel{in Chapter 4}\DIFdelend \DIFaddbegin \DIFadd{later}\DIFaddend , but first we revisit the primary question of this chapter given the above discussion on the technologies available to econometricians to make inferences about \DIFdelbegin \DIFdel{the preferences of individual }\DIFdelend \DIFaddbegin \DIFadd{preferences of }\DIFaddend agents.
Recall that it was assumed that the experimenter in the SMP thought experiment had complete knowledge of the stochastic specification utilized by each of the imagined subjects.
This knowledge could have derived from asking each subject a \DIFaddbegin \DIFadd{sufficiently large }\DIFaddend number of questions under \DIFdelbegin \DIFdel{experimental conditions that satisfy the precepts for a valid economic experiment laid out by \textcite{Smith1982}{\footnotemark}}\DIFdelend \DIFaddbegin \DIFadd{perfect experimental conditions}\DIFaddend , and then utilizing MLE and equation (\DIFdelbegin \DIFdel{\ref{eq3:PnT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:PnT}}\DIFaddend ) to retrieve highly accurate estimates of each subject's preferences.
Assume that another experimenter asked a sufficiently \enquote{large} sample of subjects a battery of questions and was able to estimate the $\theta$ vector for this sample.
\DIFdelbegin \DIFdel{In this section we will demonstrate that }\DIFdelend \DIFaddbegin \DIFadd{Can }\DIFaddend knowledge of the distribution of preferences \DIFdelbegin \DIFdel{in a population can provide estimates of individual agents' welfare that differ meaningfully from estimates of individual agents' welfare that would arise from individual level estimationof preferences}\DIFdelend \DIFaddbegin \DIFadd{provide knowledge about welfare that was previously obfuscated by individual estimation? 
Potentially, yes}\DIFaddend .

\DIFdelbegin %DIFDELCMD < \addtocounter{footnote}{-1}
%DIFDELCMD < \stepcounter{footnote}%%%
\footnotetext{\DIFdel{\textcite{Smith1982} proposes several precepts as potentially necessary for drawing valid inferences from experimental data. 
	These are }%DIFDELCMD < \enquote{Nonsatiation}%%%
\DIFdel{, }%DIFDELCMD < \enquote{Saliency}\parencite[72]{Smith1982}%%%
\DIFdel{, }%DIFDELCMD < \enquote{Dominance}%%%
\DIFdel{, and }%DIFDELCMD < \enquote{Privacy}\parencite[934]{Smith1982}%%%
\DIFdel{.
	}%DIFDELCMD < \enquote{Nonsatiation} %%%
\DIFdel{requires that an agent would prefer to have more of an outcome that she values to less.
	}%DIFDELCMD < \enquote{Saliency} %%%
\DIFdel{requires that the rules and mechanisms that map an agent's actions to outcomes be understood and actionable by the agent.
	}%DIFDELCMD < \enquote{Dominance} %%%
\DIFdel{requires that the value of the outcome resulting from one action is sufficiently different to the value of an outcome resulting from another action to the extent that this difference }\textit{\DIFdel{dominates}} %DIFAUXCMD
\DIFdel{the cost of performing such an action.
	}%DIFDELCMD < \enquote{Privacy} %%%
\DIFdel{requires that an agent only be given information on the alternative actions and outcomes available to her.
}}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend To make this discussion more concrete, we can utilize one of the HL-MPL instruments alluded to earlier and now displayed in Table (\ref{tb:HL-MPL}).
In the HL experiment, subjects were presented with \DIFdelbegin \DIFdel{this table }\DIFdelend \DIFaddbegin \DIFadd{the table above}\DIFaddend , without the \enquote{Expected Payoff Difference} and \enquote{CRRA for Indifference} columns, and asked to select one option from each row.
The \enquote{Option A} column indicates the outcomes and associated probabilities for option A in each of 10 tasks \DIFdelbegin \DIFdel{, }\DIFdelend and similarly for the \enquote{Option B} column.
The \enquote{CRRA for Indifference} column indicates the CRRA value that would make an \DIFdelbegin \DIFdel{EUT }\DIFdelend agent indifferent between option A and option B.
Thus, an agent with a CRRA value of $0.5$ would select option A for rows 1-6, and then \enquote{switch} to selecting option B for the remaining rows.

\begin{table}[ht]
	\centering
	\captionsetup{justification=centering}
	\caption{The Ten Paired Lottery-Choice Decisions with Low Payoffs \newline \textcite[1645]{Holt2002} }
	\label{tb:HL-MPL}
	\begin{adjustbox}{width=1\textwidth}
	\pgfplotstabletypeset[
		col sep=colon,
		every head row/.style={
			after row=\hline
		},
		display columns/0/.style={
			string type,
			column name = {\cnline{Row \#}}
		},
		display columns/1/.style={
			string type,
			column name = {Option A}
		},
		display columns/2/.style={
			string type,
			column name = {Option B}
		},
		display columns/3/.style={
			string type,
			column name = {\cnline{Expected Value\\Difference}}
		},
		display columns/4/.style={
			string type,
			column name = {\cnline{CRRA for\\Indifference}}
		},
	]{tables/HL-MPL.csv} % path/to/file
	\end{adjustbox}
\end{table}

The \DIFdelbegin \DIFdel{popularity of this approach }\DIFdelend \DIFaddbegin \DIFadd{HL study is one of the most cited experiments in experimental economics, with 3238 citations as of October 7, 2015, according to Google Scholar.
Its popularity }\DIFaddend is in part due to it's straightforward logic: if a subject conforms to a deterministic EUT specification, then she should start off selecting option A, then at some point switch once, and only once, to selecting option B for the remaining rows\DIFdelbegin \DIFdel{or she should select B for every row}\DIFdelend .
The point at which the subject switches reveals an interval in which her preference for risk must lie.

However, this pattern need not necessarily occur given stochastic specifications.
Subjects may, and \DIFdelbegin \DIFdel{sometimes }\DIFdelend \DIFaddbegin \DIFadd{often }\DIFaddend do, switch multiple times between option A and option B as they work their way down the rows.
Some subjects \DIFdelbegin \DIFdel{even }\DIFdelend select option A in row 10 even though it is dominated by option B.
The first of these observed choice behaviors is often referred to as multiple switching behavior (MSB), while the second is a form of FOSD where there is no risk involved.
\DIFaddbegin \DIFadd{MSB is frequently observed in economic experiments: For instance, }\DIFaddend \textcite[1647]{Holt2002} observe that 28 of their 212 subjects exhibited MSB.
Rather than discussing all of the potential reasons why a subject would exhibit MSB, we will assume a normatively coherent stochastic model and discuss the implications of MSB within it.

The HL-MPL is a useful instrument to discuss the welfare implications of stochastic models not only because it is popular, but because the \DIFaddbegin \DIFadd{frequently }\DIFaddend observed MSB is an apparent violation to EUT that easy to notice visually without estimation.
There is no deterministic EUT utility function which allows either the switching back and forth from option A to option B or the selection of a guaranteed, lower outcome over a guaranteed, higher outcome.
Thus, any observance of MSB by an agent suggests that, at least under an EUT framework, maximal welfare has not been obtained.

An important and often overlooked reality of stochastic models is that even if a subject doesn't display MSB, the subject may still not be realizing their optimal welfare.
This may not seem obvious at first, as any non-MSB choice pattern can be rationalized by some preference relation.
\DIFdelbegin \DIFdel{Cases such as these arrise when a subject makes a choice error with respect to the utility function they operate, but this choice error results in a choice pattern that is still rationalizable, or }%DIFDELCMD < \enquote{consistent}%%%
\DIFdel{.
As }\DIFdelend \DIFaddbegin \DIFadd{But as }\DIFaddend soon as we incorporate knowledge of a sample's distribution of preferences governed by $\theta$, it becomes clear that many observed, apparently \enquote{consistent} choice patterns contain more choice errors and are often more costly in terms of foregone welfare than apparently \enquote{inconsistent} choice patterns.
This will be made clear in the immediate discussion, but first we must define some notation.

Utilizing notation from the \DIFdelbegin \DIFdel{beginning of Section \ref{ssec:Notation}}\DIFdelend \DIFaddbegin \DIFadd{previous chapter}\DIFaddend , an option in a set of alternatives $t$ is represented as $X_{jt}$, where $j$ indicates the option's ordinal rank among the set of alternatives given the agent's \DIFdelbegin \DIFdel{utility parameter vector, }\DIFdelend $\beta_n$\DIFdelbegin \DIFdel{, and $y_t = j$ indicates that option $j$ was chosen by the agent in task $t$.
We }\DIFdelend \DIFaddbegin \DIFadd{.
Therefore, we }\DIFaddend can define a \enquote{choice error} as any choice where \DIFdelbegin \DIFdel{the option chosen didn't provide the greatest utility.
Therefore a choice error in task $t$ is when }\DIFdelend $y_t \neq 1$\DIFdelbegin \DIFdel{, and an indicator function for choice errors given some vector of assumed utility parameters $\beta_n$ is given by:
}\DIFdelend \DIFaddbegin \DIFadd{:
}\DIFaddend \begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:Itb}
%DIFDELCMD < 	%%%
\DIFdel{K_{t}}\DIFdelend \DIFaddbegin \DIFadd{I_{nt}}\DIFaddend (\beta_n) = 
	\begin{cases}
		 1 & y_t \neq 1\\
		 0 & y_t = 1
	\end{cases}
\end{equation}

\noindent The frequency of choice errors by agent $n$ in the choice pattern $y_t \times T$ is:
\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:MTBn}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:MBn}
	\DIFaddend M\DIFdelbegin \DIFdel{_T}\DIFdelend (\beta_n) = \sum_t^T \DIFdelbegin \DIFdel{K}\DIFdelend \DIFaddbegin \DIFadd{I}\DIFaddend (\beta_n)
\end{equation}

Given the distribution parameter vector $\theta$, we can define the expected frequency of choice errors in the choice pattern $y_t \times T$ as:
\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:EMt}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:EMt}
	\DIFaddend \E(M | \theta) = \int M(\beta_n) f(\beta | \theta) d\beta
\end{equation}

\noindent where, just as in equation (\DIFdelbegin \DIFdel{\ref{eq3:Pnt}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:Pnt}}\DIFaddend ), $f(\beta|\theta)$ is the density function of the $\beta$ vector given the vector of hyper-parameters $\theta$ shaping the distribution of the $\beta$.
Equation (\DIFdelbegin \DIFdel{\ref{eq3:EMt}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EMt}}\DIFaddend ) is just the mean of the discrete distribution of choice errors in in the choice pattern $y_t \times T$ , given the distribution parameter vector $\theta$.
Because the distribution of choice errors is discrete, $M(\beta_n) \in [0,T] \subset \mathbb{N}^0$,\DIFaddbegin {\footnotemark} \DIFaddend we can define the probability mass function of choice errors as follows\DIFdelbegin %DIFDELCMD < {\footnotemark}%%%
\DIFdel{:
}\DIFdelend \DIFaddbegin \DIFadd{:
}\DIFaddend \begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:PE}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:PE}
	\DIFaddend P_E(e | \theta) = \int N[M(\beta),e] f(\beta|\theta) d \beta
\end{equation}

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	$\mathbb{N}^0$ indicates the set of natural numbers, inclusive of $0$. $\mathbb{N}^1$ or $\mathbb{N}^{+}$ would indicate the set of natural numbers not inclusive of 0.
}

\noindent where:
\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:NMB}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:NMB}
	\DIFaddend N[M(\beta), e] = 
	\begin{cases}
		1 & M(\beta) = e\\
		0 & M(\beta) \neq e
	\end{cases}
\end{equation}

\noindent and $e$ indicates the number of choice errors for the given choice pattern and $\theta$ vector.
Equation (\DIFdelbegin \DIFdel{\ref{eq3:PE}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:PE}}\DIFaddend ) provides useful information about whether an observed pattern deviates from a deterministic choice model, but is limited in the case that it assigns equal weight to errors which are very costly in terms of welfare and errors that are not so costly.

We can incorporate \DIFdelbegin \DIFdel{two of }\DIFdelend the metrics developed \DIFdelbegin \DIFdel{in Chapter 2 }\DIFdelend \DIFaddbegin \DIFadd{previously }\DIFaddend for welfare assessment into this sample framework \DIFdelbegin \DIFdel{.
The first metric, calculated for a choice pattern $y_t \times T$, is equivalent to a standard consumer surplus calculation:
}\begin{displaymath}
	\DIFdel{%DIFDELCMD < \label{eq3:WST}%%%
	\Delta W_{nT} = \sum_{t=1}^T \left( }{\DIFdel{\CE}}\DIFdel{_{nyt} - }{\DIFdel{\CE}}\DIFdel{_{n1t}^Z \right)
}\end{displaymath}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \noindent %%%
\DIFdel{where ${\CE}_{nyt}$ is the }\DIFdelend \DIFaddbegin \DIFadd{to gather more useful information and determine the expected welfare surplus and the expected ratio of obtained }\DIFaddend {\CE} \DIFdelbegin \DIFdel{of the option chosen, indicated by the subscript $y$, by agent $n$ in task $t$, and ${\CE}_{n1t}^Z$ is the }\DIFdelend \DIFaddbegin \DIFadd{to maximum }\DIFaddend {\CE} \DIFdelbegin \DIFdel{of the option that provides the greatest utility among the set of unchosen options, $Z$, in task $t$.
Throughout this chapter, we will refer to the metric in equation (\ref{eq3:WST}) as the }%DIFDELCMD < \enquote{welfare surplus} %%%
\DIFdel{metric.
The second metric we propose to characterize the welfare implications of choices is similar to the concept of auction }%DIFDELCMD < \enquote{efficiency} %%%
\DIFdel{utilized by \textcite{Plott1978}:
}%DIFDELCMD < 

%DIFDELCMD < %%%
\begin{displaymath}
	\DIFdel{%DIFDELCMD < \label{eq3:WET}%%%
	\%W_{nT} = \frac{\displaystyle\sum_{t=1}^{T} {\CE}_{nyt} }{\displaystyle\sum_{t=1}^{T} {\CE}_{n1t}}
}\end{displaymath}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \noindent %%%
\DIFdel{In the metric defined in equation (\ref{eq3:WET}), the }%DIFDELCMD < {\CE}%%%
\DIFdel{'s of the options chosen by the agent across all tasks $T$ are summed, and then divided by the }%DIFDELCMD < {\CE}%%%
\DIFdel{'s of the options that would have provided the greatest utility across all the tasks.
Therefore, should an agent never make a choice error, this metric would take on the value of $1$, and should the agent make at least one choice error, it would take on a value between $0$ and $1$.}%DIFDELCMD < {\footnotemark}
%DIFDELCMD < %%%
\DIFdel{Throughout this chapter, we will refer to the metric in equation (\ref{eq3:WET}) as the }%DIFDELCMD < \enquote{welfare efficiency} %%%
\DIFdel{metric.
}%DIFDELCMD < 

%DIFDELCMD < \addtocounter{footnote}{-1}
%DIFDELCMD < \stepcounter{footnote}%%%
\footnotetext{\DIFdel{There are a few mathematical peculiarities with this metric.
	This metric can lose its $(0,1)$ bounds if any of the $T$ tasks has a mixed frame, that is, a task has both positive and negative outcomes.
	This would occur if the }%DIFDELCMD < {\CE} %%%
\DIFdel{of a chosen option has a different sign than the }%DIFDELCMD < {\CE} %%%
\DIFdel{of the highest ranked option.
	Also, this metric becomes undefined if the }%DIFDELCMD < {\CE} %%%
\DIFdel{of the highest ranked alternative is $0$.
	These general issues will not be of concern in this chapter as all examples of lotteries have outcomes in the strictly positive domain.
}}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The welfare surplus and welfare efficiency metrics from equations (\ref{eq3:WST}) and (\ref{eq3:WET}) can be used in place of equation (\ref{eq3:MTBn}) in equation (\ref{eq3:EMt}) to gather useful information for a }\DIFdelend \DIFaddbegin \DIFadd{for the }\DIFaddend given choice pattern and $\theta$ vector:
\begin{align}
	E( \Delta W_T | \theta) &= \int \Delta W_T(\beta) f(\beta | \theta) d \beta \DIFdelbegin %DIFDELCMD < \label{eq3:EWST}%%%
\DIFdelend \DIFaddbegin \label{eq:EDWT}\DIFaddend \\
	E( \% W_T | \theta) &= \int \% W_T(\beta) f(\beta | \theta) d \beta \DIFdelbegin %DIFDELCMD < \label{eq3:EWET}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \label{eq:EPWT}
\DIFaddend \end{align}

Given equation (\DIFdelbegin \DIFdel{\ref{eq3:NMB}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:NMB}}\DIFaddend ), we can denote the expected welfare surplus and the expected \DIFdelbegin \DIFdel{welfare efficiency }\DIFdelend \DIFaddbegin \DIFadd{proportion of welfare }\DIFaddend obtained by agents who have committed $e \in [0,T]$ errors by making choices $y_t \times T$ as follows:
\begin{align}
	E( \Delta W_T | \theta, e) &= \int \bigr( \Delta W_T(\beta) \times N[M(\beta),e] \bigr) f(\beta | \theta) d \beta \DIFdelbegin %DIFDELCMD < \label{eq3:EDWTe}%%%
\DIFdelend \DIFaddbegin \label{eq:EDWTe}\DIFaddend \\
	E( \% W_T | \theta, e) &= \int \bigl( \% W_T(\beta) \times N[M(\beta),e] \bigr) f(\beta | \theta) d \beta \DIFdelbegin %DIFDELCMD < \label{eq3:EPWTe}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \label{eq:EPWTe}
\DIFaddend \end{align}

The same limitation mentioned about MSL concerning a computer's inability to perform closed-form integration \DIFdelbegin \DIFdel{in general }\DIFdelend applies to equations (\DIFdelbegin \DIFdel{\ref{eq3:EMt}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EMt}}\DIFaddend ), (\DIFdelbegin \DIFdel{\ref{eq3:PE}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:PE}}\DIFaddend ), and (\DIFdelbegin \DIFdel{\ref{eq3:EWST}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EDWT}}\DIFaddend ) through (\DIFdelbegin \DIFdel{\ref{eq3:EWET}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWTe}}\DIFaddend ).
However, these equations can be approximated in the manner described for MSL in equation (\DIFdelbegin \DIFdel{\ref{eq3:SPnt}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:SPnt}}\DIFaddend ): the terms in these equations between the integrand and the density function will be evaluated with $\beta$ vectors randomly drawn $H$ times from the distribution governed by $\theta$, and then averaged.
As $H$ gets sufficiently large, the simulated statistics approach the true statistics.

\DIFdelbegin \subsection{\DIFdel{Sample Level Analysis with an EUT Population}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The simulation methods described here and for the remainder of this chapter characterize an individual agent as having a single $\beta_n$ vector representing her preferences, and making choices in an economic environment that satisfies the \textcite{Smith1982} precepts for valid economic experiments.
An individual agent $n$ generates a }\textit{\DIFdel{observed}} %DIFAUXCMD
\DIFdel{choice pattern $y_t \times T$ by resolving the stochastic process defined by her preferences.
In Chapter 2, we described normatively coherent stochastic models as those models that characterize agents as having non-random preferences, thus an agent's preferences do not change from choice to choice.}%DIFDELCMD < {\footnotemark}
%DIFDELCMD < %%%
\DIFdel{Individual $\beta_n$ parameter vectors are themselves drawn from a population of $\beta$ vectors.
This distribution of $\beta$ vectors in the population is characterized by the parameter vector $\theta$.
Throughout the following discussion, we will refer to a choice pattern's likelihood of being }\textit{\DIFdel{observed}}%DIFAUXCMD
\DIFdel{, by which we mean the choice pattern's simulated likelihood as calculated in equation (\ref{eq3:SLnT}).
This is the probability of a randomly drawn agent from the population defined by $\theta$ producing the choice pattern, thus the pattern's likelihood of being observed.
Likewise, when we discuss the expected welfare implications of a choice pattern for a given population, we are discussing the expected welfare implications for an agent from that population who generated that choice pattern}\DIFdelend \DIFaddbegin \DIFadd{We can construct similar notation for welfare metrics utilizing equations (17) and (18), as well as calculating variances for every expectation metric proposed above, but these steps will just complicate the meat of the welfare analysis unnecessarily.
For now, we can simply begin to analyze the HL-MPL given these metrics}\DIFaddend .

\DIFdelbegin \DIFdel{To make an explicit numerical example , }\DIFdelend \DIFaddbegin \subsubsection{\DIFadd{Sample Level Analysis with an EUT Population}}

\DIFadd{To make this numerical example explicit }\DIFaddend we will first define \DIFdelbegin \DIFdel{the models characterizing an individual agent's choice probabilities, and then the marginal distributions of the elements of $\beta$ which together define the populationcharacterized by $\theta$.
For the sake of simplicity }\DIFdelend \DIFaddbegin \DIFadd{a sample population.
For simplicity sake}\DIFaddend , we will first consider a \DIFdelbegin \DIFdel{population }\DIFdelend \DIFaddbegin \DIFadd{sample }\DIFaddend entirely composed of agents \DIFdelbegin \DIFdel{conforming to }\DIFdelend \DIFaddbegin \DIFadd{with }\DIFaddend an EUT utility \DIFdelbegin \DIFdel{model with a Contextual Utility (CU ) stochastic model}%DIFDELCMD < \parencite{Wilcox2008}%%%
\DIFdel{.
Thus, choice probabilities for an individual agent are defined as follows:
}%DIFDELCMD < 

%DIFDELCMD < \addtocounter{footnote}{-1}
%DIFDELCMD < \stepcounter{footnote}%%%
\footnotetext{\DIFdel{Not only are we assuming that agents do not have random preferences, we're also assuming that an agent's preferences are static across choices generally.
	We could, as \textcite{Hey2001} does, model some or all of the parameters in an agent's utility function as being partly determined by the number of choices that the agent has encountered.
	Because preferences modeled in this way change from choice to choice in a non-random manner, the welfare analysis discussed in this chapter could be extended in a normatively coherent manner to incorporate this }%DIFDELCMD < \enquote{learning}%%%
\DIFdel{, potentially with interesting implications.
	This would involve specifying additional marginal distributions which characterize the parameters defining the }%DIFDELCMD < \enquote{learning} %%%
\DIFdel{process.
}}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\begin{align*}
	\DIFdel{%DIFDELCMD < \label{eq3:RE}%%%
	\begin{split}
	P_{njt}(\beta_n) &= {\Prob}\left(  \epsilon_t \geq \frac{1}{D(\beta_n,X_t) \lambda_n} \left[ G(\beta_n,X_{kt}) - G(\beta_n,X_{jt}) \right] \right)\\
	&= 1 - F\left( \dfrac{G(\beta_n,X_{kt}) - G(\beta_n,X_{jt})}{D(\beta_n,X_t)\lambda_n }  \right)\\
	&= {\Prob}(y_t = j)
	\end{split}
}\end{align*}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \noindent %%%
\DIFdel{where $\epsilon_t$ defines the random error associated with the measurement of utility, the functional form the utility function, $G(\cdot)$, is the CRRA functionof the form $u(x) = \frac{x^{1-r}}{(1-r)}$, $F(\cdot)$ is the logistic cumulative distribution function (cdf), and the adjusting function $D(\cdot)$ is as follows:
}%DIFDELCMD < 

%DIFDELCMD < %%%
\begin{align*}
	\DIFdel{%DIFDELCMD < \label{eq3:CU}%%%
	\begin{split}
		&D(\beta_n,X_t) = \mathit{max}[u(x_{it})] - \mathit{min}[u(x_{it})]\\
		&\mathit{st.}\; w_i(x_{it}) \neq 0
	\end{split}
}\end{align*}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \noindent %%%
\DIFdelend \DIFaddbegin \DIFadd{structure and a CU stochastic model.
The functional form for EUT will be the CRRA function.
}\DIFaddend Thus the $\beta_n$ vector for each agent is said to consist of only two parameters, $r$ and $\lambda$.
\DIFdelbegin \DIFdel{The joint distribution of these two parameters characterizes the population of agents and is defined by the parameter vector $\theta$.
}\DIFdelend We will consider \DIFdelbegin \DIFdel{the marginal distributions of the $r$ and $\lambda$ }\DIFdelend \DIFaddbegin \DIFadd{these }\DIFaddend parameters to be \DIFdelbegin \DIFdel{independent and }\DIFdelend uncorrelated in the \DIFdelbegin \DIFdel{population.
}%DIFDELCMD < {\footnotemark}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{sample.
}\DIFaddend The $r$ parameter can conceivably take any value, but to make the bulk of the density lie in the \DIFdelbegin \DIFdel{familiar }\DIFdelend \DIFaddbegin \DIFadd{relevant }\DIFaddend range of the HL-MPL, we will consider it as distributed \DIFdelbegin \DIFdel{normal}\DIFdelend \DIFaddbegin \DIFadd{normally}\DIFaddend , with mean of $0.65$ and a standard deviation of $0.3$, thus $r \sim \mathcal{N}(0.65 , 0.3^2 )$.
The $\lambda$ parameter must be strictly positive, so it will be assumed to be distributed as gamma with a mean of $0.35$ and a standard deviation of $0.3$.
This is equivalent to a gamma distribution with a shape parameter of $k \approx 1.36$ and a scale parameter of $t\approx0.26$, thus $\lambda \sim \Gamma(1.36 , 0.26)$.
Together these 4 parameters make the joint distribution-shaping parameter $\theta=\{0.65 ,0.3^2, 1.36 , 0.26\}$.
\DIFdelbegin \DIFdel{Because the marginal distributions are uncorrelated, we don't need to define elements of a covariance matrix.
}\DIFdelend 

The metrics described in equations (\DIFdelbegin \DIFdel{\ref{eq3:SLnT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:SLnT}}\DIFaddend ) and (\DIFdelbegin \DIFdel{\ref{eq3:MTBn}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:MBn}}\DIFaddend ) through (\DIFdelbegin \DIFdel{\ref{eq3:EPWTe}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWTe}}\DIFaddend ) rely on a given choice pattern, $y_t \times T$.
In the HL-MPL there are a total of $2^{10}=1024$ choice patterns that can be described.
To begin the discussion of the welfare implications of stochastic choice models, we calculate the values for equations (\DIFdelbegin \DIFdel{\ref{eq3:SLnT}) , and (\ref{eq3:MTBn}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:SLnT}) and (\ref{eq:MBn}}\DIFaddend ) through (\DIFdelbegin \DIFdel{\ref{eq3:EPWTe}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWTe}}\DIFaddend ) for all $\mathit{TT} =1024$ choice patterns and all $e \in[0,T]$ for the given $\theta$, with $H=2.5 \times 10^6$.{\footnotemark} 
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < \addtocounter{footnote}{-3}
%DIFDELCMD < \stepcounter{footnote}%%%
\footnotetext{\DIFdel{This is done for convenience; adding correlation among the marginal distributions would require the definition of a covariance matrix.
	In samples of real populations, we might expect there to be correlation among these marginal distributions, and this analysis can be easily extended to accommodate it.
	This additional step is not difficult, but introduces more parameters to keep track of and doesn't significantly add to the narrative.
}}
%DIFAUXCMD
%DIFDELCMD < \stepcounter{footnote}%%%
\footnotetext{\DIFdel{The calculations were performed using the R statistical software. 
	The large number of calculations was facilitated by authoring a new package, }%DIFDELCMD < \enquote{ctools}%%%
\DIFdel{, which primarily acts as a wrapper for the }%DIFDELCMD < \enquote{parallel} %%%
\DIFdel{and }%DIFDELCMD < \enquote{Rhpc} %%%
\DIFdel{packages.
	The use of parallel computing is common in various statistical software, but R does parallel computation in a particularly useful way.
	All code used is available on request.
}}
%DIFAUXCMD
%DIFDELCMD < \stepcounter{footnote}%%%
\footnotetext{\DIFdel{As noted earlier, instead of a built-in random number generator, we transform Halton sequences into the required distributions. 
	The normal distribution was created by inverting a Halton sequence constructed with a prime base of $3$, and the gamma distribution was created by inverting a Halton sequence constructed with a prime base of $7$.
	The first 30 elements of each sequence were dropped and the next 2.5 million elements were used. 
}}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{To make clear how the result of these equations are arrived at, we can work through the calculations step by step.
First, we select a choice pattern from one of the $1024$ choice patterns possible with the HL-MPL, for example, the choice of option A for the first five rows and option B for rows $6$ through $10$.
Next, a $\beta_n$ vector is drawn from the joint distribution defined by }\DIFdelend \DIFaddbegin \DIFadd{While this large a value for $H$ is not necessary to gather sufficiently unbiased estimates for }\DIFaddend $\theta$ \DIFdelbegin \DIFdel{.
As an example, we will assume $\beta_n = \lbrace r = 0.65, \lambda = .35\rbrace$ was drawn; recall that we are also assuming EUT with a CU stochastic model.
Utilizing this $\beta_n$ and choice pattern, we can evaluate the various metrics proposed.
First, we evaluate equation (\ref{eq3:PnT}), the likelihood that agent $n$ would produce this choice pattern, utilizing equation (\ref{eq3:RE}) to calculate choice probabilities for the individual tasks:
}\begin{align*}
	\DIFdel{\label{eq3:example_PnT}
	\begin{split}
		P_{n,j,1}  = Pr(y_1 = A    \,|\, \beta_n)    &= 0.82 \\
		P_{n,j,2}  = Pr(y_2 = A    \,|\, \beta_n)    &= 0.78 \\
		P_{n,j,3}  = Pr(y_3 = A    \,|\, \beta_n)    &= 0.74 \\
		P_{n,j,4}  = Pr(y_4 = A    \,|\, \beta_n)    &= 0.68 \\
		P_{n,j,5}  = Pr(y_5 = A    \,|\, \beta_n)    &= 0.62 \\
		P_{n,j,6}  = Pr(y_6 = B    \,|\, \beta_n)    &= 0.44 \\
		P_{n,j,7}  = Pr(y_7 = B    \,|\, \beta_n)    &= 0.51 \\
		P_{n,j,8}  = Pr(y_8 = B    \,|\, \beta_n)    &= 0.57 \\
		P_{n,j,9}  = Pr(y_9 = B    \,|\, \beta_n)    &= 0.63 \\
		P_{n,j,10} = Pr(y_{10} = B \,|\, \beta_n)    &= 0.95 \\
		P_{nT}     = \prod_{t = 1}^{T = 10} P_{njt}(\beta_n)  &= 0.0154
	\end{split}
}\end{align*}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \noindent %%%
\DIFdel{Note that $P_{n,j,6} < 0.50$.
With the CU stochastic model, the option with the highest utility, expected or otherwise, will always have the highest probability of being chosen.
Since there are only two alternatives in task $6$, it must be the case that option $B$ in this task had a lower expected utility than option $A$, and therefore the choice of $B$ in task $6$ is a choice error.
Using the notation defined in Section \ref{ssec:Notation}, $y_6 = 2$, and for all $t \in \lbrace T \,\backslash\, 6 \rbrace$, $y_t = 1$.
This information allows us to evaluate equation (\ref{eq3:MTBn}), the frequency of choice errors in a given choice pattern, utilizing equation (\ref{eq3:Itb}):
}%DIFDELCMD < 

%DIFDELCMD < %%%
\begin{align*}
	\DIFdel{\label{eq3:example_MTBn}
	\begin{split}
		P_{n,j,1}  = Pr(y_1 = A    \,|\, \beta_n) = 0.82 ~ \Rightarrow ~ K_{1}(\beta_n)  &= 0 \\
		P_{n,j,2}  = Pr(y_2 = A    \,|\, \beta_n) = 0.78 ~ \Rightarrow ~ K_{2}(\beta_n)  &= 0 \\
		P_{n,j,3}  = Pr(y_3 = A    \,|\, \beta_n) = 0.74 ~ \Rightarrow ~ K_{3}(\beta_n)  &= 0 \\
		P_{n,j,4}  = Pr(y_4 = A    \,|\, \beta_n) = 0.68 ~ \Rightarrow ~ K_{4}(\beta_n)  &= 0 \\
		P_{n,j,5}  = Pr(y_5 = A    \,|\, \beta_n) = 0.62 ~ \Rightarrow ~ K_{5}(\beta_n)  &= 0 \\
		P_{n,j,6}  = Pr(y_6 = B    \,|\, \beta_n) = 0.44 ~ \Rightarrow ~ K_{6}(\beta_n)  &= 1 \\
		P_{n,j,7}  = Pr(y_7 = B    \,|\, \beta_n) = 0.51 ~ \Rightarrow ~ K_{7}(\beta_n)  &= 0 \\
		P_{n,j,8}  = Pr(y_8 = B    \,|\, \beta_n) = 0.57 ~ \Rightarrow ~ K_{8}(\beta_n)  &= 0 \\
		P_{n,j,9}  = Pr(y_9 = B    \,|\, \beta_n) = 0.63 ~ \Rightarrow ~ K_{9}(\beta_n)  &= 0 \\
		P_{n,j,10} = Pr(y_{10} = B \,|\, \beta_n) = 0.95 ~ \Rightarrow ~ K_{10}(\beta_n) &= 0 \\
		                                M(\beta_n) = \sum_{t = 1}^{T = 10}{K_t(\beta_n)} &= 1
	\end{split}
}\end{align*}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \noindent %%%
\DIFdel{Thus our subject $n$ has committed one choice error across the $T$ tasks.This result allows us to calculate equation (\ref{eq3:NMB}) for values of $e \in [ 0, T ]$ which indicates if there have been $e$ number of errors in this choice pattern:
}%DIFDELCMD < 

%DIFDELCMD < %%%
\begin{align*}
	\DIFdel{\label{eq3:example_NMB}
	\begin{split}
		N( M_T(\beta_n) = 1, e = 0 )  &= 0 \\
		N( M_T(\beta_n) = 1, e = 1 )  &= 1 \\
		N( M_T(\beta_n) = 1, e = 2 )  &= 0 \\
		N( M_T(\beta_n) = 1, e = 3 )  &= 0 \\
		N( M_T(\beta_n) = 1, e = 4 )  &= 0 \\
		N( M_T(\beta_n) = 1, e = 5 )  &= 0 \\
		N( M_T(\beta_n) = 1, e = 6 )  &= 0 \\
		N( M_T(\beta_n) = 1, e = 7 )  &= 0 \\
		N( M_T(\beta_n) = 1, e = 8 )  &= 0 \\
		N( M_T(\beta_n) = 1, e = 9 )  &= 0 \\
		N( M_T(\beta_n) = 1, e = 10 ) &= 0 \\
	\end{split}
}\end{align*}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \noindent %%%
\DIFdel{Next we can calculate the two welfare metrics from equations equations (\ref{eq3:WST}) and (\ref{eq3:WET}), indicating welfare surplus and welfare efficiency respectively.
First we calculate the }\DIFdelend \DIFaddbegin \DIFadd{utilizing MSL, it does allow us to make very accurate estimates at the tails of the distribution.}\DIFaddend {\DIFdelbegin %DIFDELCMD < \CE} %%%
\DIFdel{of option $A$ and option $B$ for all $T = 10$ tasks using equation (\ref{eq3:CEcalc}). 
We note the }%DIFDELCMD < {\CE} %%%
\DIFdel{of the chosen and unchosen options for the given choice pattern, the difference between the two, and the greatest }%DIFDELCMD < {\CE} %%%
\DIFdel{for each task:
}%DIFDELCMD < 

%DIFDELCMD < \begin{table}[ht]
%DIFDELCMD < 	\centering
%DIFDELCMD < 	\setlength{\tabcolsep}{1pt}
%DIFDELCMD < 	%%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{Example }%DIFDELCMD < {\CE}%%%
\DIFdelFL{'s of EUT Agent with HL-MPL}}
	%DIFAUXCMD
%DIFDELCMD < \label{tb:example_CE}
%DIFDELCMD < 	\begin{adjustbox}{width=1\textwidth}
%DIFDELCMD < 	\pgfplotstabletypeset[
%DIFDELCMD < 		col sep=comma,
%DIFDELCMD < 		every head row/.style={
%DIFDELCMD < 			after row=\hline
%DIFDELCMD < 		},
%DIFDELCMD < 		every last row/.style={
%DIFDELCMD < 			after row=\hline
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/0/.style={
%DIFDELCMD < 			column type={c},
%DIFDELCMD < 			column name = {Task}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/1/.style={
%DIFDELCMD < 			precision = 2,
%DIFDELCMD < 			zerofill,
%DIFDELCMD < 			column name = { \cnline{ {\CE} of A} }
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/2/.style={
%DIFDELCMD < 			precision = 2,
%DIFDELCMD < 			zerofill,
%DIFDELCMD < 			column name = { \cnline{ {\CE} of B } }
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/3/.style={
%DIFDELCMD < 			precision = 2,
%DIFDELCMD < 			zerofill,
%DIFDELCMD < 			column name = { \cnline{ {\CE} of Chosen\\Option } }
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/4/.style={
%DIFDELCMD < 			precision = 2,
%DIFDELCMD < 			zerofill,
%DIFDELCMD < 			column name = { \cnline{ {\CE} of Unchosen\\Option} }
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/5/.style={
%DIFDELCMD < 			precision = 2,
%DIFDELCMD < 			fixed,
%DIFDELCMD < 			zerofill,
%DIFDELCMD < 			column name = { \cnline{ {\CE} of Chosen -\\{\CE} of Unchosen} }
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/6/.style={
%DIFDELCMD < 			precision = 2,
%DIFDELCMD < 			zerofill,
%DIFDELCMD < 			column name = { \cnline{Greatest \\{\CE}}}
%DIFDELCMD < 		}
%DIFDELCMD < 	]{tables/Example_CE.csv} %%%
%DIF <  path/to/file
	%DIFDELCMD < \end{adjustbox}
%DIFDELCMD < \end{table}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{With the }%DIFDELCMD < {\CE}%%%
\DIFdel{'s calculated, we can substitute them in to equations (\ref{eq3:WST}) and (\ref{eq3:WET}).
For equation equation (\ref{eq3:WST}), we take the sum of column $6$ in Table \ref{tb:example_CE}:
}%DIFDELCMD < 

%DIFDELCMD < %%%
\begin{displaymath}
	\DIFdel{\label{eq3:example_WST}
	\Delta W_{nT} = \sum_{t=1}^T \left( }{\DIFdel{\CE}}\DIFdel{_{nyt} - }{\DIFdel{\CE}}\DIFdel{_{n1t}^Z \right) = 8.92
}\end{displaymath}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \noindent %%%
\DIFdel{and for equation (\ref{eq3:WET}), we take the sum of column $4$ and divide it by the sum of column $7$:
}%DIFDELCMD < 

%DIFDELCMD < %%%
\begin{displaymath}
	\DIFdel{\label{eq3:example_WET}
	\%W_{nT} = \frac{\displaystyle\sum_{t=1}^{T} {\CE}_{nyt} }{\displaystyle\sum_{t=1}^{T} {\CE}_{n1t}} = \frac{21.37}{21.74} = .983
}\end{displaymath}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \noindent %%%
\DIFdel{Finally, we multiply the welfare metrics derived in equations (\ref{eq3:example_WST}) and (\ref{eq3:example_WET}) by the indicator functions derived for $e \in [0,T]$ in equation (\ref{eq3:NMB}):
}%DIFDELCMD < 

%DIFDELCMD < %%%
\begin{align*}
	\DIFdel{\label{eq3:example_NMBWST}
	\begin{split}
		N( M_T(\beta_n) = 1, e = 1 )  \times \Delta W_{nT} &= 1 \times 8.92 = 8.92\\
		N( M_T(\beta_n) = 1, e \neq 1 )  \times \Delta W_{nT} &= 0 \times 8.92 = 0
	\end{split}
}\end{align*}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\begin{align*}
	\DIFdel{\label{eq3:example_NMBWET}
	\begin{split}
		N( M_T(\beta_n) = 1, e = 1 )  \times \%W_{nT} &= 1 \times .983 = .983\\
		N( M_T(\beta_n) = 1, e \neq 1 )  \times \%W_{nT} &= 0 \times .983 = 0
	\end{split}
}\end{align*}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \noindent %%%
\DIFdel{The indicator functions in equation (\ref{eq3:example_NMB}) are mutually exclusive, therefore the product of the indicator functions and the welfare metrics will be $0$ for all but one value of $e$, and equal to the metrics for the remaining $e$, in this example, for $e = 1$. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < Thus, the derivation of the results in equations (\ref{eq3:example_PnT}) through (\ref{eq3:example_WET}) constitutes the core of the computational exercise that results in population level expectations.
\DIFdel{Having derived the results of these equations for one given choice pattern, we iterate through the remaining $1023$ choice patterns for this particular agent, repeating the numerical exercise described above for each choice pattern.
With metrics defined for this particular agent across all $TT = 1024$ possible choice patterns, a new $\beta_n$ vector is drawn from $\theta$, and the entire process repeated.
For the calculations described below, we repeat the process of drawing a $\beta_n$ from $\theta$ and calculating the results of these metrics for all choice patterns $H = 2.5 \times 10^6$ times.}%DIFDELCMD < {%%%
\DIFdelend \footnotemark}
\DIFdelbegin \DIFdel{This process results in a $3$ dimensional array with $(\mathit{\#\ of\ metrics} \times \mathit{\#\ of\ choice\ patterns} \times H) = 33 \times 1024 \times (2.5 \times 10^6) = 8.448 \times 10^{10}$ elements.
}%DIFDELCMD < 

%DIFDELCMD < \addtocounter{footnote}{-1}
%DIFDELCMD < \stepcounter{footnote}%%%
\footnotetext{\DIFdel{Since each of these repetitions are effectively independent of each other, this kind of task is termed an }%DIFDELCMD < \enquote{embarrassingly parallel} %%%
\DIFdel{problem.
	The }%DIFDELCMD < \enquote{ctools} %%%
\DIFdel{package written for this project makes the calculation of these kinds of problems across multiple CPUs particularly easy.
}}
%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{To arrive at the population level metrics, we take the average of each metric defined in equations (\ref{eq3:example_PnT}) through (\ref{eq3:example_NMBWET}) across all $H$ simulated agents for each choice pattern.
Since each $\beta_n$ was drawn randomly from the distribution governed by $\theta$, only a simple average is needed.
This averaging leaves us with a dataset that has $33 \times 1024 = 33,792$ elements.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend resulting dataset, however, is too large to be displayed in full, so for now we restrict attention to the 10 choice patterns \DIFaddbegin \DIFadd{which are the }\DIFaddend most likely to be observed, and discuss the metrics calculated in equations (\DIFdelbegin \DIFdel{\ref{eq3:EMt}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EMt}}\DIFaddend ), (\DIFdelbegin \DIFdel{\ref{eq3:EWST}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EDWT}}\DIFaddend ), (\DIFdelbegin \DIFdel{\ref{eq3:EWET}) , and (\ref{eq3:PE}) with $e = (0,1)$}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWT}) and (\ref{eq:PE}) with $e \in (0,1)$}\DIFaddend .
The results of these equations for the 10 most likely choice patterns are as follows:

\DIFaddbegin \addtocounter{footnote}{-2}
\stepcounter{footnote}\footnotetext{
	\DIFadd{The calculations were performed using the R statistical software. 
	The large number of calculations was greatly sped up by using the }\enquote{parallel} \DIFadd{package provided with R which allowed the use of multiple processors for many of the computations. 
	All code used is available on request.
}}
\stepcounter{footnote}\footnotetext{
	\DIFadd{As noted earlier, instead of a built-in random number generator, one transforms Halton sequences into the required distributions. 
	The normal distribution was created by inverting a Halton sequence constructed with a prime base of $3$, and the gamma distribution was created by inverting a Halton sequence constructed with a prime base of $7$.
	The first 30 elements of each sequence were dropped and the next 2.5 million elements were used. 
	An added benefit of the Halton sequence is its reproducibility. 
	The results described can be replicated exactly.
}}

\DIFaddend \break

\begin{table}[ht]
	\DIFdelbeginFL %DIFDELCMD < \setlength{\tabcolsep}{2pt}
%DIFDELCMD < 	%%%
\DIFdelendFL \centering
	\caption{HL-MPL Welfare and Error Expectations for Top Ten \DIFdelbeginFL \DIFdelFL{Most Likely }\DIFdelendFL Choice Patterns, EUT}
	\label{tb:TopTenEUT}
	\begin{adjustbox}{width=1\textwidth}
	\DIFdelbeginFL %DIFDELCMD < \pgfplotstabletypeset[
%DIFDELCMD < 		col sep=comma,
%DIFDELCMD < 		every head row/.style={
%DIFDELCMD < 			before row={
%DIFDELCMD < 				\multicolumn{1}{c}{} &
%DIFDELCMD < 				\multicolumn{10}{c}{Choice in Row} &
%DIFDELCMD < 				\cnline{Simulated\\Likelihood} &
%DIFDELCMD < 				\cnline{Expected\\Errors} &
%DIFDELCMD < 				\cnline{Welfare\\Efficiency} &
%DIFDELCMD < 				\cnline{Welfare\\Surplus} &
%DIFDELCMD < 				$P_E(e=0)$ &
%DIFDELCMD < 				$P_E(e=1)$\\
%DIFDELCMD < 			},
%DIFDELCMD < 			after row=\hline
%DIFDELCMD < 		},
%DIFDELCMD < 		every last row/.style={
%DIFDELCMD < 			after row=\hline
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/0/.style={
%DIFDELCMD < 			column name = {Rank},
%DIFDELCMD < 			column type={c}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/1/.style={
%DIFDELCMD < 			column name = {1},
%DIFDELCMD < 			column type={|p{.3cm}}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/2/.style={
%DIFDELCMD < 			column name = {2},
%DIFDELCMD < 			column type={p{.3cm}}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/3/.style={
%DIFDELCMD < 			column name = {3},
%DIFDELCMD < 			column type={p{.3cm}}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/4/.style={
%DIFDELCMD < 			column name = {4},
%DIFDELCMD < 			column type={p{.3cm}}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/5/.style={
%DIFDELCMD < 			column name = {5},
%DIFDELCMD < 			column type={p{.3cm}}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/6/.style={
%DIFDELCMD < 			column name = {6},
%DIFDELCMD < 			column type={p{.3cm}}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/7/.style={
%DIFDELCMD < 			column name = {7},
%DIFDELCMD < 			column type={p{.3cm}}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/8/.style={
%DIFDELCMD < 			column name = {8},
%DIFDELCMD < 			column type={p{.3cm}}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/9/.style={
%DIFDELCMD < 			column name = {9},
%DIFDELCMD < 			column type={p{.3cm}}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/10/.style={
%DIFDELCMD < 			column name = {10},
%DIFDELCMD < 			column type={p{.4cm}|}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/11/.style={
%DIFDELCMD < 			precision = 4,
%DIFDELCMD < 			fixed,
%DIFDELCMD < 			%sci precision = 2,
%DIFDELCMD < 			zerofill,
%DIFDELCMD < 			column name = {}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/12/.style={
%DIFDELCMD < 			precision = 3,
%DIFDELCMD < 			zerofill,
%DIFDELCMD < 			sci precision = 3,
%DIFDELCMD < 			column name = {}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/13/.style={
%DIFDELCMD < 			precision = 4,
%DIFDELCMD < 			sci precision = 3,
%DIFDELCMD < 			zerofill,
%DIFDELCMD < 			column name = {}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/14/.style={
%DIFDELCMD < 			precision = 2,
%DIFDELCMD < 			sci precision = 3,
%DIFDELCMD < 			zerofill,
%DIFDELCMD < 			column name = {}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/15/.style={
%DIFDELCMD < 			precision = 3,
%DIFDELCMD < 			sci precision = 3,
%DIFDELCMD < 			column name = {}
%DIFDELCMD < 		},
%DIFDELCMD < 		display columns/16/.style={
%DIFDELCMD < 			precision = 3,
%DIFDELCMD < 			sci precision = 3,
%DIFDELCMD < 			zerofill,
%DIFDELCMD < 			column name = {}
%DIFDELCMD < 		}
%DIFDELCMD < 	]{tables/TopTenEUT.csv} %%%
\DIFdelendFL \DIFaddbeginFL \pgfplotstabletypeset[
		col sep=comma,
		every head row/.style={
			before row={
				\multicolumn{10}{c}{Choice in Row} &
				\cnline{Simulated\\Likelihood} &
				\cnline{Expected\\Errors} &
				\cnline{Welfare\\Proportion} &
				\cnline{Welfare\\Surplus} &
				$P_E(e=0)$ &
				$P_E(e=1)$\\
			},
			after row=\hline
		},
		every last row/.style={
			after row=\hline
		},
		display columns/0/.style={
			column name = {1},
			column type={p{.2cm}}
		},
		display columns/9/.style={
			column name = {10},
			column type={p{.3cm}|}
		},
		display columns/10/.style={
			precision = 2,
			sci precision = 3,
			column name = {}
		},
		display columns/11/.style={
			precision = 3,
			sci precision = 3,
			column name = {}
		},
		display columns/12/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/13/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/14/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/15/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		}
	]{tables/TopTenEUT.csv} \DIFaddendFL % path/to/file
	\end{adjustbox}
\end{table}

For the \enquote{Choice in Row} column in Table (\ref{tb:TopTenEUT}), $0$ indicates a choice of A for the row, and $1$ indicates a choice of B.
Note that the choice pattern that is mostly likely to be observed from a sample drawn from the specified population \DIFdelbegin \DIFdel{, shown in the first row where }\textit{\DIFdel{Rank}} %DIFAUXCMD
\DIFdel{is $1$, is }\DIFdelend \DIFaddbegin \DIFadd{is }\DIFaddend the choice pattern we would observe from an agent described by a deterministic choice process with preferences at the mean of the distribution of $r$.
The next two most likely choice patterns \DIFdelbegin \DIFdel{, where }\textit{\DIFdel{Rank}} %DIFAUXCMD
\DIFdel{is $2$ and $3$, }\DIFdelend correspond to the choice pattern we would observe from agents described by a deterministic choice process with preferences one standard deviation either side of the mean of the distribution of $r$.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Interestingly, for each of \DIFdelbegin \DIFdel{the three most likely }\DIFdelend \DIFaddbegin \DIFadd{these three }\DIFaddend choice patterns, it is far more likely than not that an agent displaying these choice patterns made at least one choice error, and thus did not obtain maximal welfare from her choices.
\DIFdelbegin \DIFdel{This is shown by the values in column $P_E(e=0)$, which reference equation (\ref{eq3:PE}), all being less than $0.50$.
}\DIFdelend Note that only \DIFdelbegin \DIFdel{$32.2\%$ }\DIFdelend \DIFaddbegin \DIFadd{$32.15\%$ }\DIFaddend of agents who display the most likely choice pattern \DIFdelbegin \DIFdel{in row $1$ }\DIFdelend are expected to \DIFdelbegin \DIFdel{have not made any choice errors and }\DIFdelend obtain maximal welfare.
This is despite the fact that any of these choice patterns can be rationalized by some set of preferences\DIFdelbegin \DIFdel{for our assumed model}\DIFdelend .
These patterns do, however, produce \DIFdelbegin \DIFdel{relatively high expected welfare efficiency and surplus}\DIFdelend \DIFaddbegin \DIFadd{a relatively high proportion of expected welfare}\DIFaddend .
The welfare surplus metric is less informative in this comparison\DIFdelbegin \DIFdel{: }\DIFdelend \DIFaddbegin \DIFadd{, }\DIFaddend it is more useful in making absolute rather than relative statements about welfare.

The \DIFdelbegin \DIFdel{relatively large values of $ 1 - P_E(e = 0)$, which imply that most choice patterns contain at least $1$ choice error, is mainly due }\DIFdelend \DIFaddbegin \DIFadd{large value of $P_E(e >0)$ is due largely }\DIFaddend to the shape and location of the distribution of $r$.
The mean of $0.65$ lies just next to the indifference boundary between rows 6 and 7 of the HL-MPL, as indicated in \DIFdelbegin \DIFdel{the column }%DIFDELCMD < \enquote{CRRA for Indifference} %%%
\DIFdel{of Table (\ref{tb:HL-MPL}}\DIFdelend \DIFaddbegin \DIFadd{Table (1}\DIFaddend ).
That means that the bulk of the $r$ values drawn from this distribution define utility values that indicate near indifference between the A and B lotteries in row 7 of the HL-MPL.
All RE models increase the probability of a choice error the closer an agent is to being indifferent between 2 options, so it should not be a surprise that with this particular choice of distribution for $r$ we have a large proportion of choice errors.

The fourth and fifth most likely choice patterns\DIFdelbegin \DIFdel{in Table (\ref{tb:TopTenEUT}), where }\textit{\DIFdel{Rank}} %DIFAUXCMD
\DIFdel{is $4$ and $5$}\DIFdelend , \DIFaddbegin \DIFadd{however, }\DIFaddend are not consistent with any deterministic EUT preferences.
These patterns display what we will call \enquote{Light MSB}: not including the choice made in row 10, the agent has \enquote{switched} between choosing A and B three times.{\footnotemark}
Because MSB is not consistent with any deterministic EUT preferences, $P_E(e=0)=0$ for these patterns.
In fact, the only choice patterns in which $P_E(e=0)>0$ will be those which are \enquote{Consistent}: displaying a choice pattern that can be rationalized by some EUT \DIFdelbegin \DIFdel{preferences.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Despite }\DIFdelend \DIFaddbegin \DIFadd{function.
What is interesting about }\DIFaddend the patterns in rows 4 and 5 of Table (\ref{tb:TopTenEUT}) \DIFdelbegin \DIFdel{being }\DIFdelend \DIFaddbegin \DIFadd{is that, despite the fact that they are }\DIFaddend obviously inconsistent with a deterministic EUT process, they both are more likely to be observed \DIFdelbegin \DIFdel{from agents drawn from a population defined by $\theta$, and obtain greater welfare surplus }\DIFdelend \DIFaddbegin \DIFadd{and obtain a greater proportion of welfare }\DIFaddend than the sixth most likely choice pattern which is \enquote{Consistent.}
\DIFdelbegin \DIFdel{The likelihood of the }%DIFDELCMD < \enquote{Light MSB} %%%
\DIFdel{choice patterns in rows 4 and 5, displayed in the }%DIFDELCMD < \enquote{Simulated Likelihood} %%%
\DIFdel{column, are greater than the likelihood of the choice pattern in row 6, which is consistent.
The welfare efficiency metric for row 5, displayed in the }%DIFDELCMD < \enquote{Welfare Efficiency} %%%
\DIFdel{column, is greater than that of row 6, and the welfare surplus metrics for both rows 4 and 5 are greater than for row 6.
Since metrics for all $TT = 1024$ choice patterns were calculated, we will see in the immediate discussion that }\DIFdelend \DIFaddbegin \DIFadd{In fact, }\DIFaddend these two Light MSB patterns are both more likely to be observed and be less costly \DIFdelbegin \DIFdel{in terms of welfare surplus than 6 }\DIFdelend \DIFaddbegin \DIFadd{than 7 }\DIFaddend out of 10 \enquote{Consistent} patterns.

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	The reason that row 10 is not included in this definition is because we are making a distinction between patterns which do and do not include a choice of A in row 10 later.
}

Another interesting aspect of this analysis is the correlation of welfare and the likelihood of observing a choice pattern.
The correlation between \DIFdelbegin \DIFdel{the simulated likelihood of the choice patterns and their expected welfare efficiency is $0.62$ }\DIFdelend \DIFaddbegin \DIFadd{likelihood and the ratio of obtained to maximal welfare is $0.55$ }\DIFaddend across the whole dataset, \DIFdelbegin \DIFdel{while the simulated likelihood and expect welfare surplus has a correlation of $0.68$.
These are }\DIFdelend \DIFaddbegin \DIFadd{which is }\DIFaddend positive but far from 1.
That is, as the \DIFdelbegin \DIFdel{likelihood }\DIFdelend \DIFaddbegin \DIFadd{probability }\DIFaddend of observing a \DIFdelbegin \DIFdel{choice }\DIFdelend pattern increases, the \DIFdelbegin \DIFdel{expected welfare efficiency and surplus of the choice pattern }\DIFdelend \DIFaddbegin \DIFadd{ratio of obtained welfare to maximal welfare }\DIFaddend generally increases as well, but not always.
This is apparent in rows 8 and 9 of Table \DIFdelbegin \DIFdel{\ref{tb:TopTenEUT}}\DIFdelend \DIFaddbegin \DIFadd{(\ref{tb:TopTenEUT})}\DIFaddend .
The choice pattern described in row 8 is more likely to be observed than the pattern in row 9, but the pattern in row 9 has a higher expected welfare \DIFdelbegin \DIFdel{efficiency }\DIFdelend \DIFaddbegin \DIFadd{ratio }\DIFaddend than row 8.
The very large $H$ employed in these calculations rules out the possibility that this is a statistical fluke \DIFdelbegin \DIFdel{caused by }\DIFdelend \DIFaddbegin \DIFadd{from }\DIFaddend the random way these statistics were calculated.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{This }\DIFdelend \DIFaddbegin \DIFadd{Instead, this }\DIFaddend example illustrates how stochastic models are not \enquote{welfare ranking} models but instead incorporate aspects of the choice process that are robust to the incentivized environment that the agents exist in.
The example of row 8 and 9 only depicts the most common occurrence where the expected welfare \DIFdelbegin \DIFdel{efficiency }\DIFdelend of a pattern and its \DIFdelbegin \DIFdel{likelihood }\DIFdelend \DIFaddbegin \DIFadd{probability }\DIFaddend diverge in this hypothetical population.
The most drastic divergence occurs between the patterns which have violated FOSD by selecting option A in row 10, and those that have not.

To make this distinction clear, Figure \ref{fig:ConFOSD} plots the log of the SL (SLL) against the expected welfare \DIFdelbegin \DIFdel{efficiency }\DIFdelend \DIFaddbegin \DIFadd{proportion }\DIFaddend of the choice patterns that \DIFaddbegin \DIFadd{are}\DIFaddend :
\begin{itemize}
 \setlength\itemsep{-.5em}
	\item \DIFdelbegin \DIFdel{are Consistent with determinisitic EUT}\DIFdelend \DIFaddbegin \DIFadd{consistent}\DIFaddend ,
	\item are \DIFdelbegin \DIFdel{Consistent }\DIFdelend \DIFaddbegin \DIFadd{consistent }\DIFaddend other than the choice of A in row 10 (FOSD Only),
	\item display Light MSB \DIFdelbegin \DIFdel{, the agent has }%DIFDELCMD < \enquote{switched} %%%
\DIFdel{between choosing A and B three times, }\DIFdelend with a choice of B in row 10 \DIFaddbegin \DIFadd{(Light MSB)}\DIFaddend ,
	\item \DIFaddbegin \DIFadd{and }\DIFaddend display Light MSB with a choice of A in row 10 (Light MSB + FOSD)
\end{itemize}

\begin{figure}[h!]
	\caption{Consistent and Light MSB, With and Without Row 10 Error}
	\includegraphics[width=\linewidth]{figures/SamPlots/EUT-ConFOSD.jpg}
	\label{fig:ConFOSD}
\end{figure}

In Figure \ref{fig:ConFOSD}, each point represents a unique choice pattern.
For any given point plotted, any other point to the Southeast of that point indicates a pattern that is both more likely to be \DIFdelbegin \DIFdel{produced by an agent drawn randomly from this population and provides lower expected welfare efficiency}\DIFdelend \DIFaddbegin \DIFadd{observed and is expected to provides less welfare}\DIFaddend .
For instance, any point in the shaded region of Figure \ref{fig:ConFOSD} represents a choice pattern that is both more likely to be observed and have a lower expected welfare \DIFdelbegin \DIFdel{efficiency }\DIFdelend \DIFaddbegin \DIFadd{ratio }\DIFaddend than pattern Y.

Figure \ref{fig:ConFOSD} shows that the choice of A in row 10 greatly decreases the SLL of the pattern, but barely decreases the expected ratio of obtained welfare to maximal welfare, all else equal.
For example, the most likely consistent choice pattern is the top right-most red dot in Figure \ref{fig:ConFOSD}, labeled \enquote{X}, which corresponds to row 1 of Table 2 and has a welfare \DIFdelbegin \DIFdel{efficiency }\DIFdelend \DIFaddbegin \DIFadd{ratio }\DIFaddend of 0.986 and a SL of 0.036.
The most likely choice pattern with a choice of A in row 10 is the top right-most green dot, labeled \enquote{Y}.
This pattern is identical to the \enquote{X} pattern other than the selection of A in row 10 and has a welfare \DIFdelbegin \DIFdel{efficiency }\DIFdelend \DIFaddbegin \DIFadd{proportion }\DIFaddend of 0.938 and a SL of 0.00246.
The ratio of welfare obtained to maximum differs only by 0.0483, but pattern X is about 14.65 times more likely to be observed than pattern Y.
The seventh most likely consistent pattern, not displayed in Table 1, but can be seen as the red dot in Figure \ref{fig:ConFOSD} labeled \enquote{Z}, is about 1.55 times more likely to be observed than pattern Y, and has an expected welfare \DIFdelbegin \DIFdel{efficiency }\DIFdelend \DIFaddbegin \DIFadd{ratio }\DIFaddend that is about 0.065 \textit{lower} than pattern Y.

The general result of this exercise is to make clear that stochastic models do not perfectly link the likelihood of a choice pattern with its realized welfare\DIFdelbegin \DIFdel{as consumer surplus or efficiency}\DIFdelend .
This is due to the way in which heteroscedastic RE models disproportionately \enquote{punish} FOSD by assigning occurrences of it a very low likelihood.
The choice of A in row 10 is punished even more by the fact that there is no risk involved.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Empirically, experimental economists rarely observe behavior such as the choice of A in row 10 because the agents they study are in environments that incentivize them to reject dominated offers.
There is, by definition, no extra benefit to actively choosing a dominated offer.
But just because there is no extra benefit to be had, it shouldn't be inferred that the agent doesn't value the dominated option positively.
Any agent who selected option A in row 10 still receives a \$2 benefit from having had the choice problem presented to her if that choice is selected for payment.
It makes a great deal of sense to model choice in this fashion: that there is somewhat of a disconnect between greater realized welfare and greater likelihood of choice helps to illustrate the complex nature of economic agency.

\DIFdelbegin \subsection{\DIFdel{Sample Level Analysis with a Mixed EUT-RDU Population}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \subsubsection{\DIFadd{Sample Level Analysis with a Mixed EUT-RDU Population}}
\DIFaddend 

The above discussion focuses on a population that is entirely composed of EUT conforming agents.
Individual level estimates from \textcite{Hey1994} and the mixture model estimates from \textcite{Harrison2008a} show that many populations are likely not composed entirely of EUT agents.
We can extend the example above, defining the population as being made of some mixture of EUT agents and RDU agents.
By mixture, we mean that there will be two subpopulations of a grand population, but agents from these subpopulations carry no observable characteristics to distinguish themselves as belonging to one subpopulation or another.

Before beginning the analysis of this mixture population, we can extend the metrics utilized in equations (\DIFdelbegin \DIFdel{\ref{eq3:SLnT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:SLnT}}\DIFaddend ) and (\DIFdelbegin \DIFdel{\ref{eq3:MTBn}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:MBn}}\DIFaddend ) through (\DIFdelbegin \DIFdel{\ref{eq3:EPWTe}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWTe}}\DIFaddend ) to be defined for mixed populations.
This is \DIFdelbegin \DIFdel{implemented }\DIFdelend \DIFaddbegin \DIFadd{done }\DIFaddend much the same way as mixture models were defined in equation (\DIFdelbegin \DIFdel{\ref{eq3:PT_Mix}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:PT_Mix}}\DIFaddend ); each metric, $Q_m$, for subpopulation $m$ is weighted by the proportion of the subpopulation in the grand population, $M$.
\begin{align}
	\DIFdelbegin %DIFDELCMD < \label{eq3:Metric_Mix}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:Metric_Mix}
	\DIFaddend \begin{split}
		\bm{\mathit{Q^M}} = \sum_m^M \pi_m \times Q^m \\ 
		\mathit{st.} \sum_m^M \pi_m = 1
	\end{split}
\end{align}

\noindent where $\pi_m$ is the proportion of subpopulation $m$ in the grand population.
For example, the probability of observing any given choice pattern $y \times T$ for a grand population made of M subpopulations is:
\begin{align}
	\DIFdelbegin %DIFDELCMD < \label{eq3:LnT_Mix}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:LnT_Mix}
	\DIFaddend \begin{split}
		\bm{L_{nT}^M} = \sum_m^M \pi_m \times L_{nT}^m(\theta^m) \\ 
		\mathit{st.} \sum_m^M \pi_m = 1
	\end{split}
\end{align}

\noindent where $L_{nT}^m$ is as described in equation (\DIFdelbegin \DIFdel{\ref{eq3:LnT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:LnT}}\DIFaddend ) for some subpopulation $m$ defined by $\theta^m$.

A final metric before we begin the example of the mixed population is the probability that any given choice pattern was produced by population $m$.
Utilizing equation (\DIFdelbegin \DIFdel{\ref{eq3:LnT_Mix}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:LnT_Mix}}\DIFaddend ) we define the probability that a pattern was produced by population $m$ as the ratio of the weighted simulated likelihood of observing the pattern from subpopulation $m$ to the likelihood of observing the pattern in the grand population:

\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:Propm}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:Propm}
	\DIFaddend \mathit{Prop^m_{T}} = \frac{\pi_m \times L_{nT}^m(\theta^m) }{\bm{L_{nT}^M}}
\end{equation}

With this mixing framework in mind, we can define our grand population.
We assume that $70\%$ of agents in the grand population conform to EUT, while the remaining $30\%$ conform to RDU.
Given that the previous example thoroughly examined an EUT population, rather than duplicate analysis, we assume that the EUT subpopulation is the same as the previous EUT-only example.
Thus, the EUT subpopulation is defined as using a CU stochastic model and CRRA function with the $r$ parameter normally distributed $r \sim \mathcal{N}(0.65 , 0.3^2 )$, and the $\lambda$ parameter following a gamma distribution $\lambda \sim \Gamma(1.36 , 0.26)$.
This results in $\theta^{EUT} = \lbrace 0.65 ,0.3^2, 1.36 , 0.26\rbrace$.

For the RDU \DIFdelbegin \DIFdel{subpopulation, we employ the the }\DIFdelend \DIFaddbegin \DIFadd{population, we will again use a CU stochastic model and a CRRA utility function, and additionally use the }\DIFaddend flexible 2 parameter decision weighting function defined by \textcite{Prelec1998}\DIFdelbegin \DIFdel{as the probability weighting funciton to be substiuted into equation (\ref{eq3:dweight}):
}\begin{displaymath}
	\DIFdel{%DIFDELCMD < \label{eq3:pw:pre}%%%
	\omega(p_i)=\exp(-\beta(-\ln(p_i))^\alpha)
}\end{displaymath}
%DIFAUXCMD
%DIFDELCMD < \noindent %%%
\DIFdel{where $\alpha > 0$ and $\beta > 0$.
We continue to use the CRRA utility function and CU stochastic model for the RDU subpopulation.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{.
}\DIFaddend The $r$ parameter is be distributed identically to the $r$ parameter in the EUT population $r \sim \mathcal{N}(0.65 , 0.3^2 )$, and the $\lambda$ parameter still uses the gamma distribution, but is distributed $\lambda \sim \Gamma(0.563 , 0.26)$, which results in the mean of the $\lambda$ distribution at $0.15$ and a standard deviation of $0.2$.{\footnotemark}
Both the $\alpha$ and $\beta$ parameters for the decision weight function must be greater than $0$, so they will also be distributed with a gamma distribution, $\alpha \sim \Gamma(169 , 7.69 \times 10^{-3})$ and $\beta \sim \Gamma(144 , 8.33 \times 10^{-3})$.
Thus the mean of $\alpha$ is $\approx 1.3$ and its standard deviation is $\approx 0.1$, and the mean of $\beta$ is $\approx 1.2$ and its standard deviation is $\approx 0.1$.
This results in $\theta^{RDU} = \lbrace  0.65 ,0.3^2,  0.563 , 0.26 , 169 , 7.69 \times 10^{-3} , 144 , 8.33 \times 10^{-3} \rbrace$.

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	With the mass of the $\lambda$ distribution closer to $0$, \textit{a priori} we should expect fewer choice errors among the RDU population than the EUT population.
}

Once again, we will employ an $H = 2.5 \times 10^6$ and calculate the values for equations (\DIFdelbegin \DIFdel{\ref{eq3:SLnT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:SLnT}}\DIFaddend ) and (\DIFdelbegin \DIFdel{\ref{eq3:MTBn}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:MBn}}\DIFaddend ) through (\DIFdelbegin \DIFdel{\ref{eq3:EPWTe}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWTe}}\DIFaddend ) for all $\mathit{TT} =1024$ choice patterns and all $e \in[0,T]$ for the RDU subpopulation.
With the results of the calculations for the EUT subpopulation calculated previously, and the results of the same calculations for the RDU population, we can mix each of these metrics as described in equation (\DIFdelbegin \DIFdel{\ref{eq3:Metric_Mix}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:Metric_Mix}}\DIFaddend ) with $\pi_{EUT} = 0.7$ and $\pi_{RDU} = 0.3$.
Again, it is impractical to display the results of all metrics for all $1024$ choice patterns, so first, we recreate Table (\ref{tb:TopTenEUT}) with the results of the RDU metrics.

\begin{table}[ht]
	\centering
	\caption{HL-MPL Welfare and Error Expectations for\\Top Ten Choice Patterns, RDU}
	\label{tb:TopTenRDU}
	\begin{adjustbox}{width=1\textwidth}
	\pgfplotstabletypeset[
		col sep=comma,
		every head row/.style={
			before row={
				\multicolumn{10}{c}{Choice in Row} &
				\cnline{Simulated\\Likelihood} &
				\cnline{Expected\\Errors} &
				\cnline{Welfare\\Proportion} &
				\cnline{Welfare\\Surplus} &
				$P_E(e=0)$ &
				$P_E(e=1)$\\
			},
			after row=\hline
		},
		every last row/.style={
			after row=\hline
		},
		display columns/0/.style={
			column name = {1},
			column type={p{.2cm}}
		},
		display columns/9/.style={
			column name = {10},
			column type={p{.3cm}|}
		},
		display columns/10/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/11/.style={
			precision = 3,
			sci precision = 3,
			column name = {}
		},
		display columns/12/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/13/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/14/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/15/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		}
	]{tables/TopTenRDU.csv} % path/to/file
	\end{adjustbox}
\end{table}

There is a great deal of similarity between Table \DIFdelbegin \DIFdel{\ref{tb:TopTenRDU} and Table \ref{tb:TopTenEUT}}\DIFdelend \DIFaddbegin \DIFadd{(\ref{tb:TopTenRDU}) and Table (\ref{tb:TopTenEUT})}\DIFaddend .
In particular, the two subpopulations share the same $3$ most likely choice patterns, though with different simulated likelihood, welfare, and error metrics.
Again we note that the choice patterns which display Light MSB have $0$ probability of containing $0$ choice errors, and that several Light MSB choice patterns are expected to contain fewer choice errors and be less costly in \DIFdelbegin \DIFdel{terms }\DIFdelend \DIFaddbegin \DIFadd{tems }\DIFaddend of welfare than some Consistent choice patterns.
There appears to be less of a disconnect between welfare and likelihood in the RDU subpopulation than in the EUT subpopulation. 
In Table \DIFdelbegin \DIFdel{\ref{tb:TopTenRDU} }\DIFdelend \DIFaddbegin \DIFadd{(\ref{tb:TopTenRDU}) }\DIFaddend we observe that going from row 6 to 7 the Simulated Likelihood decreases, but the Welfare Proportion metric increases.
However, the Welfare Surplus metric decreases with the Simulated Likelihood of the choice pattern for all patterns in Table (\ref{tb:TopTenRDU}).

A major difference between the two subpopulations is that the RDU subpopulation's most likely choice pattern has a much greater likelihood than the EUT subpopulation's most likely choice pattern.
Much of this is due to the greater mass of the $\lambda$ \DIFdelbegin \DIFdel{distribution }\DIFdelend \DIFaddbegin \DIFadd{distrubtion }\DIFaddend close to $0$ in the RDU subpopulation compared to the EUT subpopulation, but it is also because the distributions chosen for the decision weighting parameters imply greater risk aversion.
This means that although the CRRA coefficients lie near the \DIFdelbegin \DIFdel{boundary }\DIFdelend \DIFaddbegin \DIFadd{boundry }\DIFaddend of row 6 and 7 of the HL-MPL, the way the RDU subpopulation weights probabilities makes them more risk averse, and therefore more likely to switch at row 7 than if they did not weight probabilities.
These differences are important when we look at the grand population metrics.

\begin{table}[ht]
	\centering
	\caption{HL-MPL Welfare and Error Expectations for\\Top Ten Choice Patterns, EUT-RDU Mixture}
	\label{tb:TopTenMIX}
	\begin{adjustbox}{width=1\textwidth}
	\pgfplotstabletypeset[
		col sep=comma,
		every head row/.style={
			before row={
				\multicolumn{10}{c}{Choice in Row}&
				\cnline{Proportion\\EUT}&
				\cnline{Simulated\\Likelihood}&
				\cnline{Expected\\Errors}&
				\cnline{Welfare\\Proportion}&
				\cnline{Welfare\\Surplus}&
				$P_E(e=0)$\\
			},
			after row=\hline
		},
		every last row/.style={
			after row=\hline
		},
		display columns/0/.style={
			column name = {1},
			column type={p{.2cm}}
		},
		display columns/9/.style={
			column name = {10},
			column type={p{.3cm}|}
		},
		display columns/10/.style={
			precision = 2,
			sci precision = 3,
			column name = {}
		},
		display columns/11/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/12/.style={
			precision = 3,
			sci precision = 3,
			column name = {}
		},
		display columns/13/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/14/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/15/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		}
	]{tables/TopTenMIX.csv} % path/to/file
	\end{adjustbox}
\end{table}

The grand population metrics displayed in Table (\ref{tb:TopTenMIX}) are barely noteworthy by themselves.
They easily could have been generated by a population composed entirely of EUT agents with a distribution of $\lambda$ somewhat closer to $0$ than the EUT subpopulation that actually composes $70\%$ of the agents in this population.
Many of the same features of the two subpopulations are apparent in the mixed grand population;
choice patterns displaying any form of MSB have $0$ likelihood of $0$ choice errors, and there are some disconnects between simulated likelihood and welfare as observed in rows $5$-$6$, and $7$-$8$ for the welfare \DIFdelbegin \DIFdel{efficiency }\DIFdelend \DIFaddbegin \DIFadd{proportion }\DIFaddend metric and rows $9$-$10$ for the welfare surplus metric.

Of greater interest is the \enquote{Proportion EUT} metric, defined in equation (\DIFdelbegin \DIFdel{\ref{eq3:Propm}).
This meteric calculates the unconditional likelihood that a subject displaying a particular choice pattern belongs to the EUT subpopulation we defined.
}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:Propm}).
}\DIFaddend For every choice pattern in the top ten most likely to be observed choice patterns, we expect that the proportion of the agents belonging to the EUT subpopulation that generated the choice pattern is smaller than the proportion of EUT agents in the grand population.
For the top 3 choice patterns, the difference between the proportion of EUT agents in the total population and the proportion of EUT agents that generated the choice pattern is greater than $20\%$.
In fact, it is more likely than not that these choice patterns are generaed by the RDU subpopulation.
This is despite the fact that the EUT subpopulation makes up $70\%$ of the grand population, and that the top three most likely to be observed choice patterns in the grand popualtion all correspond to the same top three choice patterns in the EUT subpopulation.

The above discussion of the three populations helps to illuminate the concept that knowledge of the distribution of preferences in a sample allows an analyst to have a better interpretation of observed patterns of choice.
Table 2 demonstrates that for many choice patterns which can be described as \enquote{consistent} with EUT, the EUT function which \DIFdelbegin \DIFdel{rationalizes }\DIFdelend \DIFaddbegin \DIFadd{would best fit the }\DIFaddend individual agent's data actually misrepresents the agent's \enquote{true} preferences.
Similar conclusions can be drawn for a population comprised entirely of RDU agents.
The consequence of this misrepresentation is that an agent is assumed to have made choices that maximize her subjective welfare when this is not the case.
In our particular choice of example EUT and RDU population parameters, for every agent who displays a \enquote{consistent} choice pattern it is more likely than not that their \enquote{true} underlying preferences are best characterized by a utility function which does not rationalize their observed choice behavior.
This general result is just as applicable to choice patterns described by RDU as it is to patterns described by EUT as well as for other coherent stochastic models, not just CU.

It should also be clear from Table 2 and Figure \ref{fig:ConFOSD} that not all choice patterns that are consistent with EUT should be judged as superior to choice patterns which are apparently inconsistent with EUT from the perspective of welfare realization\DIFdelbegin \DIFdel{as consumer surplus or welfare efficiency}\DIFdelend .
Figure \ref{fig:ConFOSD} demonstrates that stochastic models can make normative sense of occurrences of FOSD, while making the descriptively accurate claim that violations of dominance are unlikely to occur, a claim backed by the empirical literature described in the previous chapter.

%DIF < \section{Individual vs. Sample Level Identification}
%DIF < 
%DIF < Having made the case that sample level assessment of welfare provides information that is lacking in individual level assessment, it is important to ask the question \enquote{How much more information?} or \enquote{Why/When should we (economists) care?} 
%DIF < The welfare metric described in equation (\ref{eq3:EWET}) provides a useful benchmark to describe the difference between the welfare evaluation done on the individual level versus welfare evaluation done on the sample level.
%DIF < This distinction can be made clearer by examining \enquote{consistent} choice patterns, that is, patterns which can be described as conforming to deterministic utility theory.
%DIF < These patterns, along with the same associated metrics calculated in Table (\ref{tb:TopTenEUT}) for the EUT population are presented below, ranked in order of their likelihood of being observed:
%DIF < 
%DIF < \begin{table}[ht]
%DIF < 	\centering
%DIF < 	\caption{Consistent Choice Patterns and Associated Metrics\\EUT Population}
%DIF < 	\label{tb:ConPat}
%DIF < 	\begin{adjustbox}{width=1\textwidth}
%DIF < 	\pgfplotstabletypeset[
%DIF < 		col sep=tab,
%DIF < 		every head row/.style={
%DIF < 		% as in the previous example, this patches the first row:
%DIF < 			before row={
%DIF < 				\multicolumn{10}{c}{Choice in Row} &
%DIF < 				\cnline{Simulated\\Likelihood} & 
%DIF < 				\cnline{Expected\\Errors} & 
%DIF < 				\cnline{Welfare\\Proportion} & 
%DIF < 				\cnline{Welfare\\Surplus} &
%DIF < 				$P_E(e=0)$ & 
%DIF < 				\cnline{CRRA\\Boundary}\\
%DIF < 			},
%DIF < 			after row=\hline,
%DIF < 		},
%DIF < 		every last row/.style={
%DIF < 		after row=\hline},
%DIF < 		display columns/0/.style={
%DIF < 			column name = {1},
%DIF < 			column type={p{.2cm}}
%DIF < 		},
%DIF < 		display columns/9/.style={
%DIF < 			column name = {10},
%DIF < 			column type={p{.3cm}}
%DIF < 		},
%DIF < 		display columns/10/.style={
%DIF < 			precision = 2,
%DIF < 			sci precision = 3,
%DIF < 			column name = {}
%DIF < 		},
%DIF < 		display columns/11/.style={
%DIF < 			precision = 3,
%DIF < 			sci precision = 3,
%DIF < 			column name = {}
%DIF < 		},
%DIF < 		display columns/12/.style={
%DIF < 			precision = 4,
%DIF < 			sci precision = 3,
%DIF < 			column name = {}
%DIF < 		},
%DIF < 		display columns/13/.style={
%DIF < 			precision = 4,
%DIF < 			sci precision = 3,
%DIF < 			column name = {}
%DIF < 		},
%DIF < 		display columns/14/.style={
%DIF < 			precision = 4,
%DIF < 			sci precision = 2,
%DIF < 			column name = {}
%DIF < 		},
%DIF < 		display columns/15/.style={
%DIF < 			string type,
%DIF < 			column name = {}
%DIF < 		}
%DIF < 	]{tables/ConsistentEUT.csv} % path/to/file
%DIF < 	\end{adjustbox}
%DIF < \end{table}
%DIF < 
%DIF < 
%DIF < We begin this discussion by noting that because each of the patterns listed in Table (\ref{tb:ConPat}) are \enquote{consistent}, should an estimation routine be applied to any of them individually, the preference parameter returned will lie somewhere in the indifference band described under the \enquote{CRRA Boundary} column.{\footnotemark} 
%DIF < Also, because each of these patterns is \enquote{consistent}, the welfare efficiency will be 1 for each of these estimates.
%DIF < That is to say, because these patterns can be perfectly explained by a CRRA utility function with a parameter value within the interval described in the \enquote{CRRA Boundary} column, agents who have generated these choice patterns have made 0 apparent choice errors, thus, they have not apparently forgone any welfare.
%DIF < 
%DIF < \addtocounter{footnote}{-1}
%DIF < \stepcounter{footnote}\footnotetext{
%DIF < 	Note that for the choice pattern in row 10 of Table (\ref{tb:ConPat}) estimation is impossible because of the lack of variation in the data. 
%DIF < 	For the remaining 9 choice patterns, there is little room to estimate both the CRRA parameter and a stochastic error parameter. 
%DIF < 	The CRRA boundary, however, still describes the range of parameter values that explain this choice pattern given a deterministic EUT choice process.
%DIF < }
%DIF < 
%DIF < What is clear from the sample level analyses however is that this is not the case in reality.
%DIF < In this particular population, every observed \enquote{consistent} choice pattern is more likely to be the result of one or more choice errors made by the agent than to be an accurate representation of the agent's \enquote{true} preferences.
%DIF < For choice patterns that have a low likelihood of being observed given the population, there is effectively a $0\%$ probability that the choice pattern accurately represents the \enquote{true} preference of the agent that generated it.
%DIF < 
%DIF < While Table (\ref{tb:ConPat}) is constructed based on an EUT-only population, a similar table could be constructed for the EUT/RDU Mixture population described in Table (\ref{tb:TopTenMIX}).
%DIF < As before, there would not be much difference in the presented metrics, but the presence of an RDU subpopulation presents further potential problems for individual level estimation.
%DIF < The top three choice patterns described in Tables (\ref{tb:ConPat}) and (\ref{tb:TopTenMIX}) can all be perfectly explained by an EUT model. 
%DIF < But, as is seen by the \enquote{EUT Proportion} metric in Table (\ref{tb:TopTenMIX}), the majority of agents displaying these patterns in our example mixed population actually conform to RDU, not EUT.
%DIF < Thus, unlike the potential for misidentification of a population composed entirely of EUT agents as displayed in Table (\ref{tb:ConPat}), the most likely choice patterns to be observed are also among the most likely to lead to misidentification.
%DIF < 
%DIF < In general, however, it appears that the less likely it is to observe a choice pattern from a given population, the more likely it is that individual level estimation applied to these choice patterns will misidentify the preferences of the agent that generated it.
%DIF < In addition, the cost of this misidentification, in terms of a mischaracterization of the agent's welfare, also increases as the likelihood of observing the choice pattern decreases.
%DIF < We can readily see this by observing how far the welfare efficiency in Table (\ref{tb:ConPat}) are from 1.
%DIF < While this difference is low for the most common choice patterns, it is not insignificant.
%DIF < For instance, the choice pattern in row 5, which is only about a third as likely to be observed as the most likely choice pattern, is more than 0.07 away from 1.
%DIF < 
%DIF < To end this discussion, it should be made clear that we recognize that, to an extent, the comparison of individual level estimation against sample level estimation using the HL instrument defined is somewhat of a straw-man argument.
%DIF < Estimation at the individual level is often done using instruments that have greater than 40 choice problems presented to subjects, and with lottery pairs constructed specifically to aid in the correct identification of RDU agents.
%DIF < These considerations likely lead to greater statistical power than the 10-choice HL-MPL in the example given.
%DIF < Larger numbers of choice problems presented to subjects leads to lower likelihoods of observing the kind of choice error patterns that we discuss.
%DIF < In addition, it is not possible to retrieve estimates from most choice patterns in the HL-MPL because of the low number of choice problems.
%DIF < Thus we are not able to directly compare estimates of welfare at the individual level with estimates done on the sample level for the majority of choice patterns using the HL-MPL.
%DIF < 
%DIF < However, having a larger number of choice problems does not rule out the potential for serious mischaracterization of preferences, and by extension the characterization of welfare.
%DIF < The potential for such problems depends in large part due to the idiosyncratic aspects of the experimental instrument along with the particular distributions of parameters in the population the sample was drawn from.
%DIF < In particular, it seems that individual level estimation will lead to mischaracterization of the tails of the distribution of preferences in a sample.
%DIF < We believe that this exercise demonstrates some potential pitfalls of conducting individual level estimation.
\DIFaddbegin \subsection{\DIFadd{Individual vs. Sample Level Identification}}
\DIFaddend 

\DIFdelbegin \section{\DIFdel{Population Level Analysis of Welfare: Preferences, Noise, and the Instrument}}
%DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \DIFadd{Having made the case that sample level assessment of welfare provides information that is lacking in individual level assessment, it is important to ask the question }\enquote{How much more information?} \DIFadd{or }\enquote{Why/When should we (economists) care?} 
\DIFadd{The welfare metric described in equation (\ref{eq:EPWT}) provides a useful benchmark to describe the difference between the welfare evaluation done on the individual level versus welfare evaluation done on the sample level.
This distinction can be made clearer by examining }\enquote{consistent} \DIFadd{choice patterns, that is, patterns which can be described as conforming to deterministic utility theory.
These patterns, along with the same associated metrics calculated in Table (\ref{tb:TopTenEUT}) for the EUT population are presented below, ranked in order of their likelihood of being observed:
}\DIFaddend 

\DIFaddbegin \begin{table}[ht]
	\centering
	\caption{\DIFaddFL{Consistent Choice Patterns and Associated Metrics}\\\DIFaddFL{EUT Population}}
	\label{tb:ConPat}
	\begin{adjustbox}{width=1\textwidth}
	\pgfplotstabletypeset[
		col sep=tab,
		every head row/.style={
		% as in the previous example, this patches the first row:
			before row={
				\multicolumn{10}{c}{Choice in Row} &
				\cnline{Simulated\\Likelihood} & 
				\cnline{Expected\\Errors} & 
				\cnline{Welfare\\Proportion} & 
				\cnline{Welfare\\Surplus} &
				$P_E(e=0)$ & 
				\cnline{CRRA\\Boundary}\\
			},
			after row=\hline,
		},
		every last row/.style={
		after row=\hline},
		display columns/0/.style={
			column name = {1},
			column type={p{.2cm}}
		},
		display columns/9/.style={
			column name = {10},
			column type={p{.3cm}}
		},
		display columns/10/.style={
			precision = 2,
			sci precision = 3,
			column name = {}
		},
		display columns/11/.style={
			precision = 3,
			sci precision = 3,
			column name = {}
		},
		display columns/12/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/13/.style={
			precision = 4,
			sci precision = 3,
			column name = {}
		},
		display columns/14/.style={
			precision = 4,
			sci precision = 2,
			column name = {}
		},
		display columns/15/.style={
			string type,
			column name = {}
		}
	]{tables/ConsistentEUT.csv} %DIF >  path/to/file
	\end{adjustbox}
\end{table}


\DIFadd{We begin this discussion by noting that because each of the patterns listed in Table (\ref{tb:ConPat}) are }\enquote{consistent}\DIFadd{, should an estimation routine be applied to any of them individually, the preference parameter returned will lie somewhere in the indifference band described under the }\enquote{CRRA Boundary} \DIFadd{column.}{\footnotemark} 
\DIFadd{Also, because each of these patterns is }\enquote{consistent}\DIFadd{, the welfare ratio will be 1 for each of these estimates.
That is to say, because these patterns can be perfectly explained by a CRRA utility function with a parameter value within the interval described in the }\enquote{CRRA Boundary} \DIFadd{column, agents who have generated these choice patterns have made 0 apparent choice errors, thus, they have not apparently forgone any welfare.
}

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	\DIFadd{Note that for the choice pattern in row 10 of Table (\ref{tb:ConPat}) estimation is impossible because of the lack of variation in the data. 
	For the remaining 9 choice patterns, there is little room to estimate both the CRRA parameter and a stochastic error parameter. 
	The CRRA boundary, however, still describes the range of parameter values that explain this choice pattern given a deterministic EUT choice process.
}}

\DIFadd{What is clear from the sample level analyses however is that this is not the case in reality.
In this particular population, every observed }\enquote{consistent} \DIFadd{choice pattern is more likely to be the result of one or more choice errors made by the agent than to be an accurate representation of the agent's }\enquote{true} \DIFadd{preferences.
For choice patterns that have a low likelihood of being observed given the population, there is effectively a $0\%$ probability that the choice pattern accurately represents the }\enquote{true} \DIFadd{preference of the agent that generated it.
}

\DIFadd{While Table (\ref{tb:ConPat}) is constructed based on an EUT-only population, a similar table could be constructed for the EUT/RDU Mixture population described in Table (\ref{tb:TopTenMIX}).
As before, there would not be much difference in the presented metrics, but the presence of an RDU subpopulation presents further potential problems for individual level estimation.
The top three choice patterns described in Tables (\ref{tb:ConPat}) and (\ref{tb:TopTenMIX}) can all be perfectly explained by an EUT model. 
But, as is seen by the }\enquote{EUT Proportion} \DIFadd{metric in Table (\ref{tb:TopTenMIX}), the majority of agents displaying these patterns in our example mixed population actually conform to RDU, not EUT.
Thus, unlike the potential for misidentification of a population composed entirely of EUT agents as displayed in Table (\ref{tb:ConPat}), the most likely choice patterns to be observed are also among the most likely to lead to misidentification.
}

\DIFadd{In general, however, it appears that the less likely it is to observe a choice pattern from a given population, the more likely it is that individual level estimation applied to these choice patterns will misidentify the preferences of the agent that generated it.
In addition, the cost of this misidentification, in terms of a mischaracterization of the agent's welfare, also increases as the likelihood of observing the choice pattern decreases.
We can readily see this by observing how far the welfare ratios in Table (\ref{tb:ConPat}) are from 1.
While this difference is low for the most common choice patterns, it is not insignificant.
For instance, the choice pattern in row 5, which is only about a third as likely to be observed as the most likely choice pattern, is more than 0.07 away from 1.
}

\DIFadd{To end this discussion, it should be made clear that we recognize that, to an extent, the comparison of individual level estimation against sample level estimation using the HL instrument defined is somewhat of a straw-man argument.
Estimation at the individual level is often done using instruments that have greater than 40 choice problems presented to subjects, and with lottery pairs constructed specifically to aid in the correct identification of RDU agents.
These considerations likely lead to greater statistical power than the 10-choice HL-MPL in the example given.
Larger numbers of choice problems presented to subjects leads to lower likelihoods of observing the kind of choice error patterns that we discuss.
In addition, it is not possible to retrieve estimates from most choice patterns in the HL-MPL because of the low number of choice problems.
Thus we are not able to directly compare estimates of welfare at the individual level with estimates done on the sample level for the majority of choice patterns using the HL-MPL.
}

\DIFadd{However, having a larger number of choice problems does not rule out the potential for serious mischaracterization of preferences, and by extension the characterization of welfare.
The potential for such problems depends in large part due to the idiosyncratic aspects of the experimental instrument along with the particular distributions of parameters in the population the sample was drawn from.
In particular, it seems that individual level estimation will lead to mischaracterization of the tails of the distribution of preferences in a sample.
We believe that this exercise demonstrates some potential pitfalls of conducting individual level estimation.
}


\subsection{\DIFadd{Population Level Analysis of Welfare: Preferences, Noise, and the Instrument}}

\DIFaddend The proposed characterizations of the welfare of a sample, including the degree to which certain consistent choice patterns are expected to be more costly than inconsistent choice patterns, are ultimately determined by the distribution of preferences and stochastic parameters in the sample.
To analyze how the welfare characterizations change as the distribution of preferences change in the sample, we could repeat the computational exercise that \DIFdelbegin \DIFdel{led }\DIFdelend \DIFaddbegin \DIFadd{lead }\DIFaddend to Table (\ref{tb:TopTenEUT}) for a few different distributions and discuss implications pattern by pattern.
This\DIFdelbegin \DIFdel{exercise}\DIFdelend , however, \DIFdelbegin \DIFdel{will produce data only for the populations chosen, and will be less informative about how expectations of welfare change as the population changes}\DIFdelend \DIFaddbegin \DIFadd{would be tedious}\DIFaddend .
Instead, it will be useful to define a few \DIFdelbegin \DIFdel{population-level metrics that look at the data at the aggregate level}\DIFdelend \DIFaddbegin \DIFadd{sample-level metrics instead}\DIFaddend .
For instance, for each $y \times T$ choice pattern, we can weigh the expected welfare \DIFdelbegin \DIFdel{efficiency }\DIFdelend \DIFaddbegin \DIFadd{proportion }\DIFaddend resulting from equation (\DIFdelbegin \DIFdel{\ref{eq3:EWET}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWT}}\DIFaddend ) by the simulated likelihood resulting from equation (\DIFdelbegin \DIFdel{\ref{eq3:SLnT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:SLnT}}\DIFaddend ) and then sum across all TT choice patterns to retrieve the sample expected welfare \DIFdelbegin \DIFdel{efficiency}\DIFdelend \DIFaddbegin \DIFadd{proportion}\DIFaddend :

\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:EPWTT}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:EPWTT}
	\DIFaddend \E(\%W_T(\theta)) = \sum_{tt=1}^{TT} \mathit{SL}_{Ntt}(\theta) \times \E(\%W_{tt} | \theta)
\end{equation}

Similar expectations can be derived for any of the per-choice pattern statistics defined previously, but we will be paying particular interest to the statistics derived from equations (\DIFdelbegin \DIFdel{\ref{eq3:EMt}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EMt}}\DIFaddend ), and (\DIFdelbegin \DIFdel{\ref{eq3:PE}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:PE}}\DIFaddend ) where $e=0$.
We are not limited to looking at expectations however, we can utilize equation (\DIFdelbegin \DIFdel{\ref{eq3:EPWTT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWTT}}\DIFaddend ) to derive higher moments of these statistics, such as the variance:

\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:VPWTT}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:VPWTT}
	\DIFaddend \Var(\%W_T(\theta)) = \sum_{tt=1}^{TT} \mathit{SL}_{Ntt}(\theta) \times \left[ \E(\%W_{tt} | \theta) - \E(\%W_T | \theta) \right]^2
\end{equation}

Having the means and variances of the statistics described allows us to make high-level inferences about the welfare implications of an instrument like the HL-MPL on different populations for a given stochastic model.
That is, we can contribute to the answer of our primary question of \enquote{what are the welfare implications of stochastic models} by solving equations (\DIFdelbegin \DIFdel{\ref{eq3:EPWTT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWTT}}\DIFaddend ) and (\DIFdelbegin \DIFdel{\ref{eq3:VPWTT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:VPWTT}}\DIFaddend ) for various values of $\theta$ and relating the elements of $\theta$ to these results.
We can substitute any  $y \times T$ statistic from derived from equations (\DIFdelbegin \DIFdel{\ref{eq3:EMt}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EMt}}\DIFaddend ), (\DIFdelbegin \DIFdel{\ref{eq3:PE}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:PE}}\DIFaddend ), (\DIFdelbegin \DIFdel{\ref{eq3:EWET}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWT}}\DIFaddend ), and (\DIFdelbegin \DIFdel{\ref{eq3:EPWTe}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWTe}}\DIFaddend ) in place of $\%W_T$ in equations (\DIFdelbegin \DIFdel{\ref{eq3:EPWTT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWTT}}\DIFaddend ) and (\DIFdelbegin \DIFdel{\ref{eq3:VPWTT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:VPWTT}}\DIFaddend ) to describe these statistics on a population by population basis.

While equations (\DIFdelbegin \DIFdel{\ref{eq3:EPWTT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWTT}}\DIFaddend ) and (\DIFdelbegin \DIFdel{\ref{eq3:VPWTT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:VPWTT}}\DIFaddend ) may in fact have analytical solutions to determine these relationships, meaning we could attempt to solve for the partial derivative of equations (\DIFdelbegin \DIFdel{\ref{eq3:EPWTT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWTT}}\DIFaddend ) and (\DIFdelbegin \DIFdel{\ref{eq3:VPWTT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:VPWTT}}\DIFaddend ) with respect to each element of $\theta$, any analytical solution will be unique with respect to so many idiosyncratic factors that this becomes unfeasible, and potentially uninformative.
These factors include:
\begin{itemize}
 \setlength\itemsep{-.25em}
	\item The stochastic model
	\item The utility model
	\item The location, dispersion and shape of the joint distribution governing the complete stochastic specification
	\item The number of $H$ draws used to simulate the probabilities
	\item The base prime number used for the Halton sequences
	\item The specific tasks faced by the sample population
\end{itemize}

Given these limitations, instead we will examine the relationship of the parameters making up the stochastic specification \DIFdelbegin \DIFdel{, i.e. the elements of $\theta$, }\DIFdelend with the associated results of equations (\DIFdelbegin \DIFdel{\ref{eq3:EPWTT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWTT}}\DIFaddend ) and (\DIFdelbegin \DIFdel{\ref{eq3:VPWTT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:VPWTT}}\DIFaddend ) visually and with the use of locally weighted polynomial regression (LOESS) as originally developed by Cleveland, Grosse, Shyu, Chambers \& Hastie (1992).
We will examine two types of populations, both entirely composed of EUT agents.
The first type has preference parameters that are Normally distributed, while the second type will have Logit-Normal distributed preferences.
For each population type, we generate 500,000 unique population parameter sets, $\theta_i$, the elements of which are assumed to be uncorrelated, and solve equations (\DIFdelbegin \DIFdel{\ref{eq3:EPWTT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWTT}}\DIFaddend ) and (\DIFdelbegin \DIFdel{\ref{eq3:VPWTT}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:VPWTT}}\DIFaddend ) for the statistics derived in equations (\DIFdelbegin \DIFdel{\ref{eq3:EMt}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EMt}}\DIFaddend ), (\DIFdelbegin \DIFdel{\ref{eq3:PE}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:PE}}\DIFaddend ) and (\DIFdelbegin \DIFdel{\ref{eq3:EWET}}\DIFdelend \DIFaddbegin \DIFadd{\ref{eq:EPWT}}\DIFaddend ) with $e=0$ and with each equation solved with $H=10,000$.

\DIFdelbegin \subsection{\DIFdel{EUT Populations with Normally Distributed Preferences}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \subsubsection{\DIFadd{EUT Populations with Normally Distributed Preferences}}
\DIFaddend 

We begin our discussion with an example utilizing normally distributed preferences and gamma distributed stochastic errors.
Thus, each $\theta_i$ is comprised of 4 elements: the mean of the CRRA parameter and the Lambda term, $\mu_r$ and $\mu_\lambda$, and standard deviation of the CRRA parameter and the Lambda term, $\sigma_r$ and $\sigma_\lambda$. Each of the candidate $\theta_i$ vectors was randomly drawn from a joint uniform distribution of these elements.
The bounds of the marginal distributions of these elements are as follows: $\mu_r \in [-1.9 , 1.55 ]$ , $\sigma_r \in [0 , 1]$ , $\mu_\lambda \in [.05 , 2.25]$ , $\sigma_\lambda \in [.01 , .75]$. 
These bounds are almost arbitrary; the bounds for $\mu_r$ were chosen to be just outside the indifference bounds of the HL instrument, but the remaining marginal distributions were chosen to be broad enough to provide some interesting patterns.

This exercise results in 8 statistics for each $\theta_i$: the means and variances of the expected proportion of welfare to the maximum attainable welfare, the expected welfare surplus, the expected number of choice errors, and the expected proportion of agents who make no errors.
Each statistic will be plotted against the 4 elements of $\theta_i$.
The result is 32 plots of the raw data and 32 charts of the LOESS lines associated with the raw data plots.
All LOESS lines are plotted along with 95\% confidence intervals in shaded gray.

Each plot and chart also attempts to give information about another parameter not plotted on the $x$ or $y$ axes by color coding the plotted data with respect to different values of this \enquote{z} parameter.
For $\mu_r$ this \enquote{z} parameter is $\sigma_r$, for $\sigma_r$ it is $\mu_r$, for $\mu_\lambda$ it is $\sigma_\lambda$ and for $\sigma_\lambda$ it is $\mu_\lambda$.
For all of the charts, the \enquote{z} parameter is split into quartiles and for the LOESS line charts, LOESS lines are calculated for the \enquote{x} and \enquote{y} parameter values that belong to each quartile.
Additionally, in the raw data plots, each point has been given a large degree of transparency.
This means that the density of points in the plot is represented by the density of color in the plot.

We will examine the LOESS line charts of these data with the raw data plots included in Appendix A.
First we will discuss the effect on welfare expectations of the parameters governing the stochastic model, and then discuss the parameters governing the utility model.
Thus, we will first be looking at Figures (\ref{fig:S-Wel-um}), (\ref{fig:S-Err-um}), (\ref{fig:S-Wel-us}) and (\ref{fig:S-Err-us}).
Figures (\ref{fig:S-Wel-um}) and (\ref{fig:S-Err-um}) demonstrate the effect of the mean of the distribution of the lambda term on welfare and the error frequencies, while Figures (\ref{fig:S-Wel-us}) and (\ref{fig:S-Err-us}) demonstrate the effect of the standard deviation of the lambda term on the same statistics.

The results of the plots of stochastic model parameters are mostly intuitive and unsurprising.
Looking at Figures (\ref{fig:S-Wel-um}) and (\ref{fig:S-Err-um}), as the mean of the distribution increases, the expected welfare and expected proportion of 0 error choice patterns monotonically decreases, while the expected number of choice errors monotonically increases.
Because lambda is distributed with a gamma distribution, for any given mean, a higher standard deviation implies that the mass of the distribution shifts closer towards $0$.
Thus, it is unsurprising that those populations with high standard deviations of lambda tend to exhibit choice patterns with lower expected choice errors and greater expected proportions of no error choice patterns.
This is because for any given choice problem, a lower value of lambda implies a lower probability of committing a choice error.{\footnotemark}
This directly translates into greater expected welfare than those populations with lower standard deviations holding the mean constant.

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	Since $\lambda$ is in the denominator of each exponential transformation, as $\lambda \to 0$, ${\Prob}(y_t = j) \to 1$ for $j = 1$ and ${\Prob}(y_t = j) \to 0$ for $j\neq1$ regardless of the other parameters.
}

Looking at Figures (\ref{fig:S-Wel-us}) and (\ref{fig:S-Err-us}), we can see that $\sigma_\lambda$ is far less influential than the effect of $\mu_\lambda$.
In the (A) and (C) charts of Figure (\ref{fig:S-Wel-us}), the slopes of the LOESS lines are slightly positive, but mostly flat other than the line for the lowest quartile of $\mu_\lambda$.
In the (A) and (C) charts of Figure (\ref{fig:S-Err-us}), we see much of the same, mostly flat lines indicating very little variation across the parameter space.
Again the exception is the line for the lowest quartile of $\mu_\lambda$.
This should not be surprising given that the populations were generated with a CU stochastic model.
The third quartile of $\mu_\lambda$ begins at $1.15$, which means that majority of the mass of the distribution of lambda in any population will lie above 1 for any value of $\sigma_\lambda$ in the range explored.
At these high levels of lambda, most choice probabilities will converge to something close to $\Pr( y_t = j) \to 0.5$.
This leaves little room for variation in expected welfare or expected choice errors.

In contrast to the monotonic relations of the lambda distribution, the effect of the CRRA parameters on the expected welfare and expected error statistics displays influences of the idiosyncratic aspects of the HL instrument.
This is most apparent in the plots of $\mu_r$.
In interpreting these plots, it is important to keep in mind that the CRRA parameters used in each population are Normally distributed.
Thus, the mean of the distribution always represents the point of the distribution with the greatest density, with smaller standard deviations leading to greater concentration of the mass of the distribution around the mean and larger standard deviations leading to the reverse.

In Figures (\ref{fig:S-Wel-rm}) and (\ref{fig:S-Err-rm}), each tick mark on the x-axis represent the values of the CRRA parameter at which an agent would be indifferent between lotteries for some row of the HL instrument.
From left to right, the first tick mark corresponds to the value of the CRRA parameter that would make an agent indifferent between the lotteries in the first row of the instrument, the second tick mark corresponds the second row of the instrument, and so on.
There are only 9 ticks because the there does not exist any CRRA parameter which would set an agent to be indifferent between the lotteries in row 10 of the instrument.

We will begin by first discussing the effect of $\mu_r$ on choice errors as displayed in Figure (\ref{fig:S-Err-rm}).
Something that is immediately apparent is that the orange LOESS line, depicting populations with low standard deviations of CRRA parameters, is much more volatile than the other quartile lines.
Interestingly, the orange line dips downward in plots (B),(C) and (D) and peaks upward in plot (A) at the values of $\mu_r$ that correspond to the indifference values described previously.
From plots (A) and (C), we draw the conclusion that as the mass of the distribution of preferences grows around parameter values which correspond to values which imply indifference in a choice scenario, we will see an increase in the number of choice errors in the population.

In the case of the quartile described by the orange line, the idiosyncratic relationship between $\mu_r$ and the points that represent indifference also holds for the variance of expected choice errors, as depicted in plots (B) and (D) of Figure (\ref{fig:S-Err-rm}).
That is, the increase in the average number of expected errors at these points is largely driven by a sharp reduction in the probability of observing a choice pattern with few expected errors relative to the probability of observing a choice pattern with a large number of errors.{\footnotemark}

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	Because $\Pr(y_t=j) = \Pr(y_t=k) \forall j,k$ as $\lambda \to \infty$, the maximum expected number of errors that can ever be observed in a population is $\sum_{t=1}^T \frac{J_t -1}{J_t}$. 
	That is, since every option is given equal probability in the limit, and only one option is not an error, the sum of ratio of choice errors to options across all tasks is the maximum expected number of choice errors in the limit.
	The maximum in the case of the HL instrument where $J_t = 2 \ \forall t$ and $T=10$ is therefore 5.
}

The remainder of the quartiles however do not follow this general pattern of heightened influence around the indifference points.
Instead, for plots (B),(C) and (D) of Figure (\ref{fig:S-Err-rm}), the lines generally decrease until $\mu_r = 0.15$ and plot (A) increases until just about the same point.
This less volatile pattern is because the 3 highest quartiles all indicate populations with high standard deviations.
Consider the 3 upper quartile lines around $\mu_r = 0.15$.
The distances between this point and the two closest indifference points are $0.26$ and $0.29$.
The second lowest quartile's lower bound of $\sigma_r$ is $0.26$, which means that the density of the preference relation distribution at these points of indifference is much larger than for the lowest quartile, relatively.
It should be apparent from observing the lowest quartile line that as the density of the preference distribution increases around these points of indifference, the frequency of errors will increase.
We can attempt to see this more formally by creating a metric that characterizes how much the distribution of preferences \enquote{sits} on these points of indifference:
\begin{equation}
	\DIFdelbegin %DIFDELCMD < \label{eq3:Dstat}
%DIFDELCMD < 	%%%
\DIFdelend \DIFaddbegin \label{eq:Dstat}
	\DIFaddend D_j = \sum_r^R \frac{f_j(r)}{\max f_j(x)}
\end{equation}

\noindent where $f_j(r)$ is the density of the distribution of CRRA parameters for population $j$ at point $r$ and $R$ is the set of values for the CRRA parameters at which an economic agent would be indifferent between the two options in each choice problem.
The denominator of the ratio is the maximum density of the distribution $f(\cdot)$ for population $j$.
Since the CRRA parameters were distributed Normally, this value is always equivalent to the density at the mean, $\mu_r$.
The set of $R$ in for the HL instrument is:
\begin{equation}
	R \equiv \{-1.71, -0.95, -0.49, -0.14, 0.15, 0.41, 0.68, 0.97, 1.37\}
\end{equation}

We evaluate the metric from equation (73) against the 8 statistics utilized in Figures (\ref{fig:S-Wel-rm}) through (\ref{fig:S-Err-us}).
The raw plots of this data are provided in the Appendix, and the LOESS Figures will be discussed presently.
Since it can be seen in Figures (\ref{fig:S-Wel-um}) and (\ref{fig:S-Err-um}) that the effect of the $\mu_\lambda$ term asymptotes rapidly as $\mu_\lambda > 1$, we restrict our plots to populations for which $\mu_\lambda < 1$.
This leaves us with about 150k observations.
These 150k observations are first split into deciles of $\mu_\lambda$ and then the LOESS lines are calculated for each decile.
This splitting of the data helps to make clear the large effect of the stochastic elements on the statistics explored and also the large amount of heterogeneity in the effect of preference parameters caused by the stochastic parameters.

The metric developed in equation (73) is not perfect, we should expect to see clumping of data points around 0 and 1 where populations will be wholly sitting on one point or wholly between points, but it does provide a generally good description of the phenomenon we are concerned with.
Looking at Figure (\ref{fig:D-Wel-smooth}), plots (A) and (C), we can confirm what was suspected to be driving the shape of the plots in Figure (\ref{fig:S-Err-rm}).
As $D$ increases, and more of the density of the CRRA distribution is shifted onto the points describing indifference, the greater the expected number of errors we should observe.

This effect is remarkably monotonic across every decile of $\mu_\lambda$, though the effect is strongest for lower deciles.
What should be no surprise is that the highest 3 deciles of $\mu_\lambda$ effectively expect $0\%$ of the populations considered to produce choice patterns with no choice errors, as can be seen in plot (C).
The variance statistics in plots (B) and (D) are generally monotonic, but not universally so.
In general, the variance in the number of expected errors across populations tends to decrease as $D$ increases.
This is in line with the populations becoming increasingly error prone.

In Figure (\ref{fig:D-Err-smooth}) we see the story of Figure (\ref{fig:D-Wel-smooth}) interpreted into welfare, but with an interesting and important difference: the expected welfare metrics in plots (A) and (C) are effectively equal around $D=0$ and $D=1$.
There doesn't exist an equality in the error metrics around these values of $D$ in Figure (\ref{fig:D-Wel-smooth}), nor should there be.
$D=0$ corresponds to populations which have a $\mu_r$ and $\sigma_r$ such that the entire population sits between the indifference points in $R$.
$D=1$ will generally{\footnotemark} represent the opposite; such a population will have a $\mu_r$ and $\sigma_r$ such that the entire population sits on top of one of the indifference points in $R$.
If the entire population sits far from an indifference point, holding the stochastic element constant, we expect there to be fewer errors compared to a population that sits on top of an indifference point because the average agent will not be close to indifference for the lottery pair in question.
But, this is also precisely why the welfare metrics are close to equivalent: if a population sits on an indifference point, it means that agents are mostly indifferent between the options in the lottery pair, and therefore any errors made for this lottery pair will be relatively un-costly in terms of welfare.

\addtocounter{footnote}{-1}
\stepcounter{footnote}\footnotetext{
	Generally because there are multiple ways to get $D=1$. A population with $\mu_r$ close to one and a $\sigma_r$ such that there is some density on $r_j \in R \ \mathit{s.t.} \ i \neq j$ can potentially make $D \to 1$.
	However, $\mu_r \to r_j \in R$ and $\sigma_r \to 0$ is the most frequent scenario.
}

Other than the particular case where $D=0$ and $D=1$, in Figure (\ref{fig:D-Err-smooth}) we see the general trend that we might expect from looking at Figure (\ref{fig:D-Wel-smooth}): as the $D$ metric increases and the relative density of the CRRA distribution increases around points of indifference, expected welfare decreases monotonically.
This is because other than the case of $D=1$, where errors should be relatively frequent but not costly, an increasing $D$ not only means that a greater proportion of agents lie on the indifference points, but also around it.
It is this greater proportion of agents lying sufficiently near an indifference point to make an error relatively likely, but sufficiently far to make it relatively costly which drives down expected welfare.
Similar to what was seen in Figure (\ref{fig:D-Wel-smooth}), in Figure (\ref{fig:D-Err-smooth}) we see that the effect of $D$ is stronger with populations with $\mu_\lambda$ in the lower deciles and weakest with populations with $\mu_\lambda$ in the higher deciles.

What is also clear from Figures (\ref{fig:D-Wel-smooth}) and (\ref{fig:D-Err-smooth}) is that the preference aspect of the utility model, represented by $D$, contributes far less to expected choice errors and, more importantly, to expected welfare than is contributed by the stochastic aspects of the model.
Looking at the lowest decile lines in Figures (\ref{fig:D-Wel-smooth}) and (\ref{fig:D-Err-smooth}), we can see that a relatively large increase in $D$ is needed to cause the same effect as moving to the next lowest decile.
Comparing the lowest decile with the highest decile reveals tremendous changes in expected errors and expected welfare while holding $D$ constant for the populations analyzed.

%DIF < \subsection{EUT Populations with Logit-Normal Distributed Preferences}
%DIF < 
%DIF < In the previous subsection, we discussed the expected welfare outcomes of populations with normally distributed preferences and gamma distributed stochastic errors faced with the HL-MPL.
%DIF < We now turn our attention to populations with Logit-Normal distributed preferences.
%DIF < However, rather than redo Figures (\ref{fig:S-Wel-rm}) through (\ref{fig:S-Err-us}) with the new populations, we will restrict this analysis to re-examining the D statistic of equation (\ref{eq3:Dstat}) and Figures (\ref{fig:D-Wel-smooth}) and (\ref{fig:D-Err-smooth}).
%DIF < 
%DIF < The Logit-Normal distribution is defined as a Logit transformation of a Normal distribution.
%DIF < \begin{align}
%DIF < 	\begin{split}
%DIF < 		X \sim \mathcal{N}(\mu,\sigma)\\
%DIF < 		Y = \frac{\exp(X)}{1 + \exp(X)}
%DIF < 	\end{split}
%DIF < \end{align}
%DIF < 
%DIF < \noindent where $Y$ is distributed Logit-Normally.
%DIF < One of the benefits of this distribution is no extra parameters are needed in comparison to a standard normal distribution.
%DIF < A minor drawback of this distribution is that it is bounded between $[0,1]$, but this is easily rectified by \enquote{scaling} and \enquote{shifting} the distribution.
%DIF < For our analysis, we will scale every distribution by a fixed constant of $3.45$ and every the distribution by a fixed constant of $-1.9$.
%DIF < This will result in every distribution being bounded between $[-1.9,1.55]$, just outside of the lower and upper indifference boundaries of the HL-MPL.
%DIF < A somewhat 
\DIFaddbegin \subsubsection{\DIFadd{EUT Populations with Logit-Normal Distributed Preferences}}
\DIFaddend 

\DIFaddbegin \DIFadd{In the previous subsection, we discussed the expected welfare outcomes of populations with normally distributed preferences and gamma distributed stochastic errors faced with the HL-MPL.
We now turn our attention to populations with Logit-Normal distributed preferences.
However, rather than redo Figures (\ref{fig:S-Wel-rm}) through (\ref{fig:S-Err-us}) with the new populations, we will restrict this analysis to re-examining the D statistic of equation (\ref{eq:Dstat}) and Figures (\ref{fig:D-Wel-smooth}) and (\ref{fig:D-Err-smooth}).
}

\DIFadd{The Logit-Normal distribution is defined as a Logit transformation of a Normal distribution.
}\begin{align}
	\DIFadd{\begin{split}
		X \sim \mathcal{N}(\mu,\sigma)\\
		Y = \frac{\exp(X)}{1 + \exp(X)}
	\end{split}
}\end{align}

\noindent \DIFadd{where $Y$ is distributed Logit-Normally.
One of the benefits of this distribution is no extra parameters are needed in comparison to a standard normal distribution.
A minor drawback of this distribution is that it is bounded between $[0,1]$, but this is easily rectified by }\enquote{scaling} \DIFadd{and }\enquote{shifting} \DIFadd{the distribution.
For our analysis, we will scale every distribution by a fixed constant of $3.45$ and every the distribution by a fixed constant of $-1.9$.
This will result in every distribution being bounded between $[-1.9,1.55]$, just outside of the lower and upper indifference boundaries of the HL-MPL.
A somewhat 
}




\DIFaddend The general analysis of the population level data reveals several somewhat expected results, and several somewhat unexpected results.
Firstly it is clear, and unsurprising, that the means of both the CRRA and Lambda distributions individually drive a great deal of the variation in the number of expected choice errors and the expected welfare of a population.
Specifically that the effect of the mean of Lambda on the expected number of choice errors was large should have been obvious \textit{a priori}.
The \DIFdelbegin \DIFdel{$\lambda$ }\DIFdelend \DIFaddbegin \DIFadd{Lambda }\DIFaddend parameter directly influences choice probabilities regardless of the underlying instrument.
Similarly, that populations with CRRA parameters tightly distributed around a point of indifference would have greater expected number of choice errors was intuitive.
That larger numbers of expected choice errors generally lead to lower welfare was already clear from previous analyses.

Somewhat more surprising is just how dominant the stochastic elements of utility functions are over the preference aspects when deriving expectations around welfare.
Figures (\ref{fig:D-Wel-smooth}) and (\ref{fig:D-Err-smooth}) make clear, despite the potential flaws with the $D$ metric, that the way the preference parameters interact with the idiosyncratic aspects of the instrument matter a great deal, but the stochastic parameters unambiguously matter more.
This result should be important to economists and policy makers concerned with estimating the potential welfare implications of new policy instruments.


\DIFdelbegin \section{\DIFdel{Summary of Analyses}}
%DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \subsection{\DIFadd{Summary of Analyses}}
\DIFaddend 

We describe the current econometric toolkit used to estimate parameters of utility models given choice data and, by extension of a normatively coherent stochastic model, the welfare implications of these parameters.
The only methods considered involved the structural estimation of a utility function through maximum likelihood estimation.
This toolkit progressed from utilizing pooled data across an entire sample to estimate parameters for a single representative agent, to controlling for observable heterogeneity within the sample, and, with the use of mixture models, a limited amount of unobservable heterogeneity.
These methods, however, still pool data and produce estimates that are effectively estimates of means of the distributions of parameters that characterize a sample of agents, and provide little to no information about the shapes of these distributions.
By extension, little could be said about the welfare of any given agent from within the sample.

To improve the potential for making accurate individual level welfare assessments, economists estimated the same stochastic models on \enquote{large} amounts of data collected from individual agents.
As was shown in the SMP thought experiment in the previous chapter, there is nothing normatively incorrect about estimating models at the individual level, and with a sufficient number of observations per subject, such estimation can be statistically very powerful.
\DIFdelbegin \DIFdel{\textcite{Harrison2016} uses individual-level estimation to estimate the welfare surplus of a decision to purchase an insurance product.
}\DIFdelend \DIFaddbegin \DIFadd{Individual level estimates of utility functions however have rarely been utilized to conduct welfare evaluation.
}\DIFaddend 

%DIF < This method is not without drawbacks.
%DIF < Often, economists who estimate individual level parameters compile the individual estimates to describe the sample distribution of parameters.
%DIF < Each of these individual estimates, however, has a standard error associated with it which makes it difficult to aggregate these estimates into distributional data.
%DIF < A more practical concern is that occasionally estimation of a utility model on individual level data is made impossible by the particular choices made by the subjects.
%DIF < Choice patterns that deviate significantly from deterministic theories of utility become cause maximum likelihood routines to fail to converge on parameter sets.
\DIFaddbegin \DIFadd{This method is not without drawbacks.
Often, economists who estimate individual level parameters compile the individual estimates to describe the sample distribution of parameters.
Each of these individual estimates, however, has a standard error associated with it which makes it difficult to aggregate these estimates into distributional data.
A more practical concern is that occasionally estimation of a utility model on individual level data is made impossible by the particular choices made by the subjects.
Choice patterns that deviate significantly from deterministic theories of utility become cause maximum likelihood routines to fail to converge on parameter sets.
}\DIFaddend 

A concern we raise about this method is the drawback of discarding sample level information.
This concern is heightened by the stochastic component of choice.
We show that certain choices made by subjects are better characterized as \enquote{choice errors} which describe the subject as failing to accumulate a certain amount of welfare.
Every stochastic model puts a positive probability on such choice errors.
The risk this poses to individual level estimation is that one or more choice errors will be made by an individual in such a way as to cause a misidentification of the utility model.
This poses a potential problem for economists and policy makers hoping to understand the welfare implications of institutional instruments.

%DIF < To address this concern, we propose estimation of utility functions through maximum simulated likelihood methods (MSL) to recover estimates of the distribution of preferences across the whole sample, as opposed to recovering just the mean with other pooled estimation techniques and discarding sample level information with individual estimation.
To address this concern, we propose \DIFdelbegin \DIFdel{that }\DIFdelend \DIFaddbegin \DIFadd{estimation of utility functions through maximum simulated likelihood methods (MSL) to recover }\DIFaddend estimates of the distribution of preferences across the whole sample\DIFdelbegin \DIFdel{should be recovered, potentially with maximum simulated likelihood (MSL), and utilized in welfare surplus calculations}\DIFdelend \DIFaddbegin \DIFadd{, as opposed to recovering just the mean with other pooled estimation techniques and discarding sample level information with individual estimation}\DIFaddend .
With the distributional characteristics of the utility model in hand, \DIFdelbegin \DIFdel{unconditional }\DIFdelend \DIFaddbegin \DIFadd{the }\DIFaddend welfare implications of any given choice pattern \DIFaddbegin \DIFadd{made by an individual }\DIFaddend can be analyzed.

We test this methodology through simulation methods by first specifying a hypothetical population of agents characterized by a joint distribution of utility and stochastic parameters, and then simulating choice data for this population utilizing the instrument used in the popular Holt \& Laury (2002) (HL) experiment.
\DIFdelbegin \DIFdel{Our methodology allows us to calculate the unconditional likelihood of any given choice pattern, how many choice errors we expect to exist in the pattern, and }\DIFdelend \DIFaddbegin \DIFadd{For the particular population explored, }\DIFaddend the \DIFdelbegin \DIFdel{welfare consequences thereof}\DIFdelend \DIFaddbegin \DIFadd{results of this analysis confirmed concerns about individual level estimation.
For particular populations, there is a great risk of misidentifying individual subject's utility functions, and thus mis-characterizing the welfare implications of those subject's choices}\DIFaddend .
We see \DIFdelbegin \DIFdel{in Table \ref{tb:TopTenEUT} and Figure \ref{fig:ConFOSD} that many choices that are consistent with EUT provide less welfare surplus than apparently inconsistent choice patterns .
In these cases, knowledge of the distribution of preferences in a population a subject is sampled from provides us with insight about the subject's possible welfare surplus that may have been lost if the }\DIFdelend \DIFaddbegin \DIFadd{this demonstrated in Table (\ref{tb:TopTenEUT}) and Figure (\ref{fig:ConFOSD}).
The choice patterns with lower expected likelihood of being observed are the most likely to misidentify the preferences of the subjects that produced them, and this misidentification leads to a large mischaracterization of the welfare implications of these }\DIFaddend subject's choices\DIFdelbegin \DIFdel{were viewed in isolation.
}\DIFdelend \DIFaddbegin \DIFadd{.
The most common misidentification however will occur from choice patterns that are most likely to be observed, but the welfare implications of this misidentification are relatively small.
}\DIFaddend 

%DIF < For the particular population explored, the results of this analysis confirmed concerns about individual level estimation.
%DIF < For particular populations, there is a great risk of misidentifying individual subject's utility functions, and thus mis-characterizing the welfare implications of those subject's choices.
%DIF < The choice patterns with lower expected likelihood of being observed are the most likely to misidentify the preferences of the subjects that produced them, and this misidentification leads to a large mischaracterization of the welfare implications of these subject's choices.
%DIF < The most common misidentification however will occur from choice patterns that are most likely to be observed, but the welfare implications of this misidentification are relatively small.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend The analysis of the individual population we chose left open the question of how much do population level characteristics matter when it comes to describing the welfare of the population, given an instrument.
To explore this question 350k populations were simulated and several metrics describing welfare and choice errors were calculated for each population.
Each parameter that defined the populations was then plotted against these metrics and a visual analyses of the data was conducted.
We discovered, unsurprisingly, that the means of the marginal distributions that made up each population's joint distribution were of greatest importance in describing the welfare of populations.
Also, having controlled for the entire marginal distribution of the preference parameter and the idiosyncratic aspects of the instrument, we discovered that the stochastic aspect of the utility model is the dominant driver of expected welfare.
This result was somewhat surprising, though not entirely unintuitive.


%DIF < \section{Concluding Remarks}
\DIFaddbegin \subsection{\DIFadd{Concluding Remarks}}
\DIFaddend 

%DIF < The analyses conducted in this chapter provide several useful results for econometricians, experimenters, and policy makers.
%DIF < We demonstrate a method for calculating the unconditional welfare consequences of discrete economic choices made by individual agents.
%DIF < We show that these unconditional metrics lead to the somewhat unintuitive conclusion that choice patterns that are apparently inconsistent with either EUT or RDU can in fact result in greater welfare surplus for an individual than apparently inconsistent choice patterns.
\DIFaddbegin \DIFadd{The analyses conducted in this chapter provide several useful results for econometricians, experimenters, and policy makers.
Firstly, there exist stochastic models of choice which arguably describe choice probabilities well, but which are unable to provide useful statements about the accumulation of economic welfare, and thus should not be considered when conducting analyses of experimental choice data.
Secondly, experimental instruments intended to produce choice data for estimating individual level preference may provide data that leads to the misidentification of these preference on the individual level by not adequately incorporating sample level information.
}\DIFaddend 

%DIF < Finally, given the strong effect of the stochastic elements of choice on the probability and magnitude of misidentifying utility structures and by extension welfare characterizations, this chapter reinforces the statement made more than 20 years ago by Hey \& Orme (1994 p.1322):
%DIF < \enquote{Perhaps we should now spend some time on thinking about the noise, rather than about even more alternatives to EU?}
\DIFaddbegin \DIFadd{It is almost certainty the case that these same data can be analyzed at the sample level using the MSL methodology described in this chapter to produce potentially more accurate descriptions of the economic welfare of the subjects.
Thus, the large existing body of experimental data can likely be reexamined using the MSL method discussed in this chapter.
What appears likely, however, is that there may need to be a shift in economic experiments away from presenting subjects with ever more choice problems in the hope of gaining more accurate individual level estimates of utility parameters, and towards presenting subjects with smaller instruments and collecting data from larger samples.
What constitutes a sufficiently powerful instrument and what is the smallest sufficiently large sample to make accurate characterizations of expected individual welfare are open questions at this point.
It does however seem plausible that these questions can in large part be answered through simulating choice data based on prior expectations of the distributions of utility function parameters and conducting power analyses on these data.
Power analyses of this kind are ever more feasible with modern computing power and statistical software, and yet are rarely performed either }\textit{\DIFadd{a priori}} \DIFadd{or }\textit{\DIFadd{ex ante}}\DIFadd{.
}\DIFaddend 

\DIFaddbegin \DIFadd{Finally, given the strong effect of the stochastic elements of choice on the probability and magnitude of misidentifying utility structures and by extension welfare characterizations, this chapter reinforces the statement made more than 20 years ago by Hey \& Orme (1994 p.1322):
}\enquote{Perhaps we should now spend some time on thinking about the noise, rather than about even more alternatives to EU?}

\DIFaddend \newpage

\DIFdelbegin \section{\DIFdel{Figures}}
%DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \subsection{\DIFadd{Figures}}
\DIFaddend 

\begin{figure}[hp!]
	\center
	\caption{Mean of CRRA against Welfare}
	\includegraphics[height=.28\paperheight]{figures/AggPlots/S-Wel-rm.jpg}
	\label{fig:S-Wel-rm}
\end{figure}

\begin{figure}[hp!]
	\center
	\caption{Mean of CRRA against Errors}
	\includegraphics[height=.29\paperheight]{figures/AggPlots/S-Err-rm.jpg}
	\label{fig:S-Err-rm}
\end{figure}

\begin{figure}[hp!]
	\center
	\caption{Standard Deviation of CRRA against Welfare}
	\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Wel-rs.jpg}
	\label{fig:S-Wel-rs}
\end{figure}

\begin{figure}[hp!]
	\center
	\caption{Standard Deviation of CRRA against Errors}
	\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Err-rs.jpg}
	\label{fig:S-Err-rs}
\end{figure}

\begin{figure}[hp!]
	\center
	\caption{Mean of Lambda against Welfare}
	\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Wel-um.jpg}
	\label{fig:S-Wel-um}
\end{figure}

\begin{figure}[hp!]
	\center
	\caption{Mean of Lambda against Errors}
	\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Err-um.jpg}
	\label{fig:S-Err-um}
\end{figure}

\begin{figure}[hp!]
	\center
	\caption{Standard Deviation of Lambda against Welfare}
	\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Wel-us.jpg}
	\label{fig:S-Wel-us}
\end{figure}

\begin{figure}[hp!]
	\center
	\caption{Standard Deviation of Lambda against Errors}
	\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Err-us.jpg}
	\label{fig:S-Err-us}
\end{figure}

\begin{figure}[hp!]
	\center
	\caption{D Statistic against Welfare}
	\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Wel-D.jpg}
	\label{fig:D-Wel-smooth}
\end{figure}

\begin{figure}[hp!]
	\center
	\caption{D Statistic against Errors}
	\includegraphics[height=.3\paperheight]{figures/AggPlots/S-Err-D.jpg}
	\label{fig:D-Err-smooth}
\end{figure}

\newpage

\DIFdelbegin %DIFDELCMD < \onlyinsubfile{
%DIFDELCMD < \newpage
%DIFDELCMD < \printbibliography[segment=3, heading=subbibliography]
%DIFDELCMD < }
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \printbibliography
\DIFaddend 

\end{document}
